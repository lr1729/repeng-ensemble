{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9346733668341709, "accuracy_n": 995, "auc": 0.9346733668341709}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9325955734406438, "accuracy_n": 994, "auc": 0.9325955734406438}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.851985559566787, "accuracy_n": 277, "auc": 0.851985559566787}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.829465186680121, "accuracy_n": 991, "auc": 0.829465186680121}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.48697394789579157, "accuracy_n": 499, "auc": 0.48697394789579157}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.688, "accuracy_n": 500, "auc": 0.688}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5852842809364549, "accuracy_n": 299, "auc": 0.5852842809364549}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5664160401002506, "accuracy_n": 399, "auc": 0.5664160401002506}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9752085264133457, "accuracy_n": 322, "auc": 0.9752085264133457}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8154605263157895, "accuracy_n": 292, "auc": 0.8154605263157895}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8078534147204529, "accuracy_n": 1902, "auc": 0.8078534147204529}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7704071169692258, "accuracy_n": 2000, "auc": 0.7704071169692258}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9425770308123249, "accuracy_n": 59, "auc": 0.9425770308123249}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.942713567839196, "accuracy_n": 995, "auc": 0.942713567839196}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9396378269617707, "accuracy_n": 994, "auc": 0.9396378269617707}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8447653429602888, "accuracy_n": 277, "auc": 0.8447653429602888}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8355196770938446, "accuracy_n": 991, "auc": 0.8355196770938446}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5070140280561122, "accuracy_n": 499, "auc": 0.5070140280561122}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.692, "accuracy_n": 500, "auc": 0.692}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5914786967418546, "accuracy_n": 399, "auc": 0.5914786967418546}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9648208217485326, "accuracy_n": 322, "auc": 0.9648208217485326}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8253759398496241, "accuracy_n": 292, "auc": 0.8253759398496241}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8283848637650388, "accuracy_n": 1902, "auc": 0.8283848637650388}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7859667339909706, "accuracy_n": 2000, "auc": 0.7859667339909706}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9187675070028011, "accuracy_n": 59, "auc": 0.9187675070028011}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9153846153846155, "accuracy_n": 23, "auc": 0.9153846153846155}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.942713567839196, "accuracy_n": 995, "auc": 0.942713567839196}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9275653923541247, "accuracy_n": 994, "auc": 0.9275653923541247}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8736462093862816, "accuracy_n": 277, "auc": 0.8736462093862816}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7951564076690212, "accuracy_n": 991, "auc": 0.7951564076690212}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3466933867735471, "accuracy_n": 499, "auc": 0.3466933867735471}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 500, "auc": 0.5}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9797265987025023, "accuracy_n": 322, "auc": 0.9797265987025023}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8284774436090224, "accuracy_n": 292, "auc": 0.8284774436090224}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8720607749469214, "accuracy_n": 1902, "auc": 0.8720607749469214}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7366899450701704, "accuracy_n": 2000, "auc": 0.7366899450701704}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8641456582633054, "accuracy_n": 59, "auc": 0.8641456582633054}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8692307692307693, "accuracy_n": 23, "auc": 0.8692307692307693}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.935678391959799, "accuracy_n": 995, "auc": 0.935678391959799}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9295774647887324, "accuracy_n": 994, "auc": 0.9295774647887324}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8086642599277978, "accuracy_n": 277, "auc": 0.8086642599277978}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.834510595358224, "accuracy_n": 991, "auc": 0.834510595358224}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.56312625250501, "accuracy_n": 499, "auc": 0.56312625250501}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.724, "accuracy_n": 500, "auc": 0.724}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.745819397993311, "accuracy_n": 299, "auc": 0.745819397993311}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7744360902255639, "accuracy_n": 399, "auc": 0.7744360902255639}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9449721964782206, "accuracy_n": 322, "auc": 0.9449721964782206}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8609492481203007, "accuracy_n": 292, "auc": 0.8609492481203007}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7482672062986555, "accuracy_n": 1902, "auc": 0.7482672062986555}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.743633451676055, "accuracy_n": 2000, "auc": 0.743633451676055}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.707282913165266, "accuracy_n": 59, "auc": 0.707282913165266}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9336683417085427, "accuracy_n": 995, "auc": 0.9336683417085427}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9175050301810865, "accuracy_n": 994, "auc": 0.9175050301810865}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6967509025270758, "accuracy_n": 277, "auc": 0.6967509025270758}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7194752774974773, "accuracy_n": 991, "auc": 0.7194752774974773}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.46292585170340683, "accuracy_n": 499, "auc": 0.46292585170340683}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 500, "auc": 0.65}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.42474916387959866, "accuracy_n": 299, "auc": 0.42474916387959866}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5213032581453634, "accuracy_n": 399, "auc": 0.5213032581453634}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9271523178807947, "accuracy_n": 302, "auc": 0.9271523178807947}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9682576459684894, "accuracy_n": 322, "auc": 0.9682576459684894}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8174342105263158, "accuracy_n": 292, "auc": 0.8174342105263158}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8546377388535031, "accuracy_n": 1902, "auc": 0.8546377388535031}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7350543546220186, "accuracy_n": 2000, "auc": 0.7350543546220186}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9439775910364145, "accuracy_n": 59, "auc": 0.9439775910364145}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8923076923076924, "accuracy_n": 23, "auc": 0.8923076923076924}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9085427135678392, "accuracy_n": 995, "auc": 0.9085427135678392}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7173038229376257, "accuracy_n": 994, "auc": 0.7173038229376257}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7245206861755802, "accuracy_n": 991, "auc": 0.7245206861755802}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7454909819639278, "accuracy_n": 499, "auc": 0.7454909819639278}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.754, "accuracy_n": 500, "auc": 0.754}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8260869565217391, "accuracy_n": 299, "auc": 0.8260869565217391}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8170426065162907, "accuracy_n": 399, "auc": 0.8170426065162907}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9602649006622517, "accuracy_n": 302, "auc": 0.9602649006622517}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9396431881371641, "accuracy_n": 322, "auc": 0.9396431881371641}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.912593984962406, "accuracy_n": 292, "auc": 0.912593984962406}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6101833421797593, "accuracy_n": 1902, "auc": 0.6101833421797593}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7164686451809104, "accuracy_n": 2000, "auc": 0.7164686451809104}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8711484593837534, "accuracy_n": 59, "auc": 0.8711484593837534}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9396984924623115, "accuracy_n": 995, "auc": 0.9396984924623115}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9305835010060363, "accuracy_n": 994, "auc": 0.9305835010060363}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7256317689530686, "accuracy_n": 277, "auc": 0.7256317689530686}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8304742684157417, "accuracy_n": 991, "auc": 0.8304742684157417}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7414829659318637, "accuracy_n": 499, "auc": 0.7414829659318637}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.762, "accuracy_n": 500, "auc": 0.762}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8260869565217391, "accuracy_n": 299, "auc": 0.8260869565217391}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8295739348370927, "accuracy_n": 399, "auc": 0.8295739348370927}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9337748344370861, "accuracy_n": 302, "auc": 0.9337748344370861}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9807692307692307, "accuracy_n": 322, "auc": 0.9807692307692307}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9463815789473684, "accuracy_n": 292, "auc": 0.9463815789473684}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5076466295116773, "accuracy_n": 1902, "auc": 0.5076466295116773}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7259775779056239, "accuracy_n": 2000, "auc": 0.7259775779056239}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.850140056022409, "accuracy_n": 59, "auc": 0.850140056022409}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9266331658291457, "accuracy_n": 995, "auc": 0.9266331658291457}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9356136820925554, "accuracy_n": 994, "auc": 0.9356136820925554}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6498194945848376, "accuracy_n": 277, "auc": 0.6498194945848376}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7184661957618567, "accuracy_n": 991, "auc": 0.7184661957618567}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7394789579158316, "accuracy_n": 499, "auc": 0.7394789579158316}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.756, "accuracy_n": 500, "auc": 0.756}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8160535117056856, "accuracy_n": 299, "auc": 0.8160535117056856}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8195488721804511, "accuracy_n": 399, "auc": 0.8195488721804511}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.956953642384106, "accuracy_n": 302, "auc": 0.956953642384106}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9725826382452888, "accuracy_n": 322, "auc": 0.9725826382452888}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9357142857142857, "accuracy_n": 292, "auc": 0.9357142857142857}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975669099756691, "accuracy_n": 411, "auc": 0.9975669099756691}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5345994780608634, "accuracy_n": 1902, "auc": 0.5345994780608634}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.704854452457337, "accuracy_n": 2000, "auc": 0.704854452457337}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8711484593837535, "accuracy_n": 59, "auc": 0.8711484593837535}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9185929648241206, "accuracy_n": 995, "auc": 0.9185929648241206}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8058350100603622, "accuracy_n": 994, "auc": 0.8058350100603622}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7245206861755802, "accuracy_n": 991, "auc": 0.7245206861755802}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.717434869739479, "accuracy_n": 499, "auc": 0.717434869739479}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 500, "auc": 0.74}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8260869565217391, "accuracy_n": 299, "auc": 0.8260869565217391}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8045112781954887, "accuracy_n": 399, "auc": 0.8045112781954887}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9105960264900662, "accuracy_n": 302, "auc": 0.9105960264900662}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8591288229842446, "accuracy_n": 322, "auc": 0.8591288229842446}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9674812030075187, "accuracy_n": 292, "auc": 0.9674812030075187}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9902676399026764, "accuracy_n": 411, "auc": 0.9902676399026764}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6094137031139419, "accuracy_n": 1902, "auc": 0.6094137031139419}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7003448244816379, "accuracy_n": 2000, "auc": 0.7003448244816379}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7549019607843137, "accuracy_n": 59, "auc": 0.7549019607843137}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9407035175879397, "accuracy_n": 995, "auc": 0.9407035175879397}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9265593561368209, "accuracy_n": 994, "auc": 0.9265593561368209}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.855595667870036, "accuracy_n": 277, "auc": 0.855595667870036}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.858728557013118, "accuracy_n": 991, "auc": 0.858728557013118}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5871743486973948, "accuracy_n": 499, "auc": 0.5871743486973948}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.642, "accuracy_n": 500, "auc": 0.642}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 299, "auc": 0.6153846153846154}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.568922305764411, "accuracy_n": 399, "auc": 0.568922305764411}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9894964473277726, "accuracy_n": 322, "auc": 0.9894964473277726}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7644266917293232, "accuracy_n": 292, "auc": 0.7644266917293232}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6202704794762915, "accuracy_n": 1902, "auc": 0.6202704794762915}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7861578029668711, "accuracy_n": 2000, "auc": 0.7861578029668711}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9831932773109243, "accuracy_n": 59, "auc": 0.9831932773109243}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 23, "auc": 0.8461538461538461}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9306532663316583, "accuracy_n": 995, "auc": 0.9306532663316583}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9426559356136821, "accuracy_n": 994, "auc": 0.9426559356136821}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.858728557013118, "accuracy_n": 991, "auc": 0.858728557013118}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5190380761523046, "accuracy_n": 499, "auc": 0.5190380761523046}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.592, "accuracy_n": 500, "auc": 0.592}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.47491638795986624, "accuracy_n": 299, "auc": 0.47491638795986624}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3684210526315789, "accuracy_n": 399, "auc": 0.3684210526315789}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 302, "auc": 1.0}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9977988878591288, "accuracy_n": 322, "auc": 0.9977988878591288}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7634868421052632, "accuracy_n": 292, "auc": 0.7634868421052632}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.693531050955414, "accuracy_n": 1902, "auc": 0.693531050955414}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7747726929421521, "accuracy_n": 2000, "auc": 0.7747726929421521}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.964985994397759, "accuracy_n": 59, "auc": 0.964985994397759}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9346733668341709, "accuracy_n": 995, "auc": 0.9346733668341709}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8592057761732852, "accuracy_n": 277, "auc": 0.8592057761732852}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8577194752774975, "accuracy_n": 991, "auc": 0.8577194752774975}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.561122244488978, "accuracy_n": 499, "auc": 0.561122244488978}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.652, "accuracy_n": 500, "auc": 0.652}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6889632107023411, "accuracy_n": 299, "auc": 0.6889632107023411}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.41353383458646614, "accuracy_n": 399, "auc": 0.41353383458646614}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9919292554834723, "accuracy_n": 322, "auc": 0.9919292554834723}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7731672932330828, "accuracy_n": 292, "auc": 0.7731672932330828}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7434658085633404, "accuracy_n": 1902, "auc": 0.7434658085633404}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7936965244453249, "accuracy_n": 2000, "auc": 0.7936965244453249}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9915966386554622, "accuracy_n": 59, "auc": 0.9915966386554622}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 23, "auc": 0.8461538461538461}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9417085427135679, "accuracy_n": 995, "auc": 0.9417085427135679}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9154929577464789, "accuracy_n": 994, "auc": 0.9154929577464789}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8050541516245487, "accuracy_n": 277, "auc": 0.8050541516245487}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8365287588294652, "accuracy_n": 991, "auc": 0.8365287588294652}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.685370741482966, "accuracy_n": 499, "auc": 0.685370741482966}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.688, "accuracy_n": 500, "auc": 0.688}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7792642140468228, "accuracy_n": 299, "auc": 0.7792642140468228}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7969924812030075, "accuracy_n": 399, "auc": 0.7969924812030075}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8841059602649006, "accuracy_n": 302, "auc": 0.8841059602649006}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8720651838121718, "accuracy_n": 322, "auc": 0.8720651838121718}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9515977443609023, "accuracy_n": 292, "auc": 0.9515977443609023}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6304869957537156, "accuracy_n": 1902, "auc": 0.6304869957537156}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7781799229521857, "accuracy_n": 2000, "auc": 0.7781799229521857}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7436974789915967, "accuracy_n": 59, "auc": 0.7436974789915967}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9457286432160804, "accuracy_n": 995, "auc": 0.9457286432160804}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9215291750503019, "accuracy_n": 994, "auc": 0.9215291750503019}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.856710393541877, "accuracy_n": 991, "auc": 0.856710393541877}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.674, "accuracy_n": 500, "auc": 0.674}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7023411371237458, "accuracy_n": 299, "auc": 0.7023411371237458}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6892230576441103, "accuracy_n": 399, "auc": 0.6892230576441103}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9892647513129441, "accuracy_n": 322, "auc": 0.9892647513129441}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7949718045112784, "accuracy_n": 292, "auc": 0.7949718045112784}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7304659854918611, "accuracy_n": 1902, "auc": 0.7304659854918611}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7801006163224924, "accuracy_n": 2000, "auc": 0.7801006163224924}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9803921568627451, "accuracy_n": 59, "auc": 0.9803921568627451}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9276381909547738, "accuracy_n": 995, "auc": 0.9276381909547738}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9114688128772636, "accuracy_n": 994, "auc": 0.9114688128772636}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7870036101083032, "accuracy_n": 277, "auc": 0.7870036101083032}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8193743693239153, "accuracy_n": 991, "auc": 0.8193743693239153}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.34468937875751504, "accuracy_n": 499, "auc": 0.34468937875751504}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.276, "accuracy_n": 500, "auc": 0.276}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3411371237458194, "accuracy_n": 299, "auc": 0.3411371237458194}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.24060150375939848, "accuracy_n": 399, "auc": 0.24060150375939848}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9072847682119205, "accuracy_n": 302, "auc": 0.9072847682119205}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8792091442693852, "accuracy_n": 322, "auc": 0.8792091442693852}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6161654135338346, "accuracy_n": 292, "auc": 0.6161654135338346}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9221411192214112, "accuracy_n": 411, "auc": 0.9221411192214112}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8860160120311393, "accuracy_n": 1902, "auc": 0.8860160120311393}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5499410287113649, "accuracy_n": 2000, "auc": 0.5499410287113649}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7801120448179272, "accuracy_n": 59, "auc": 0.7801120448179272}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6307692307692309, "accuracy_n": 23, "auc": 0.6307692307692309}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9306532663316583, "accuracy_n": 995, "auc": 0.9306532663316583}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8700361010830325, "accuracy_n": 277, "auc": 0.8700361010830325}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.851664984863774, "accuracy_n": 991, "auc": 0.851664984863774}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 499, "auc": 0.6172344689378757}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.664, "accuracy_n": 500, "auc": 0.664}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6488294314381271, "accuracy_n": 299, "auc": 0.6488294314381271}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7117794486215538, "accuracy_n": 399, "auc": 0.7117794486215538}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9881062712388013, "accuracy_n": 322, "auc": 0.9881062712388013}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7825657894736842, "accuracy_n": 292, "auc": 0.7825657894736842}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6318935774946921, "accuracy_n": 1902, "auc": 0.6318935774946921}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8339260473030765, "accuracy_n": 2000, "auc": 0.8339260473030765}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.984593837535014, "accuracy_n": 59, "auc": 0.984593837535014}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9396984924623115, "accuracy_n": 995, "auc": 0.9396984924623115}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9406438631790744, "accuracy_n": 994, "auc": 0.9406438631790744}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8592057761732852, "accuracy_n": 277, "auc": 0.8592057761732852}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.863773965691221, "accuracy_n": 991, "auc": 0.863773965691221}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5430861723446894, "accuracy_n": 499, "auc": 0.5430861723446894}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.602, "accuracy_n": 500, "auc": 0.602}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5538847117794486, "accuracy_n": 399, "auc": 0.5538847117794486}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.98779734321903, "accuracy_n": 322, "auc": 0.98779734321903}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7298872180451128, "accuracy_n": 292, "auc": 0.7298872180451128}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7404292728237792, "accuracy_n": 1902, "auc": 0.7404292728237792}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7462443942263157, "accuracy_n": 2000, "auc": 0.7462443942263157}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9887955182072828, "accuracy_n": 59, "auc": 0.9887955182072828}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9376884422110553, "accuracy_n": 995, "auc": 0.9376884422110553}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8953722334004024, "accuracy_n": 994, "auc": 0.8953722334004024}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6570397111913358, "accuracy_n": 277, "auc": 0.6570397111913358}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6286579212916246, "accuracy_n": 991, "auc": 0.6286579212916246}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5571142284569138, "accuracy_n": 499, "auc": 0.5571142284569138}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.408, "accuracy_n": 500, "auc": 0.408}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.36454849498327757, "accuracy_n": 299, "auc": 0.36454849498327757}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9881448872412728, "accuracy_n": 322, "auc": 0.9881448872412728}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6552631578947368, "accuracy_n": 292, "auc": 0.6552631578947368}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9829683698296837, "accuracy_n": 411, "auc": 0.9829683698296837}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.645718329794763, "accuracy_n": 1902, "auc": 0.645718329794763}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7305757378413608, "accuracy_n": 2000, "auc": 0.7305757378413608}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9257703081232492, "accuracy_n": 59, "auc": 0.9257703081232492}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 23, "auc": 1.0}}
