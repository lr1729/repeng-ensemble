{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7746478873239436, "accuracy_n": 994, "auc": 0.7746478873239436}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.262, "accuracy_n": 500, "auc": 0.262}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3010033444816054, "accuracy_n": 299, "auc": 0.3010033444816054}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.543046357615894, "accuracy_n": 302, "auc": 0.543046357615894}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.507105344454742, "accuracy_n": 322, "auc": 0.507105344454742}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5521616541353384, "accuracy_n": 292, "auc": 0.5521616541353384}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5498783454987834, "accuracy_n": 411, "auc": 0.5498783454987834}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7910993895966031, "accuracy_n": 1902, "auc": 0.7910993895966031}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5447736632924485, "accuracy_n": 2000, "auc": 0.5447736632924485}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5644257703081232, "accuracy_n": 59, "auc": 0.5644257703081232}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.549748743718593, "accuracy_n": 995, "auc": 0.549748743718593}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7525150905432596, "accuracy_n": 994, "auc": 0.7525150905432596}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5923309788092835, "accuracy_n": 991, "auc": 0.5923309788092835}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2709030100334448, "accuracy_n": 299, "auc": 0.2709030100334448}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5662251655629139, "accuracy_n": 302, "auc": 0.5662251655629139}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6171995675007723, "accuracy_n": 322, "auc": 0.6171995675007723}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5148496240601503, "accuracy_n": 292, "auc": 0.5148496240601503}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5669099756690997, "accuracy_n": 411, "auc": 0.5669099756690997}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8385107041755131, "accuracy_n": 1902, "auc": 0.8385107041755131}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5374450176513721, "accuracy_n": 2000, "auc": 0.5374450176513721}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.506036217303823, "accuracy_n": 994, "auc": 0.506036217303823}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6044399596367306, "accuracy_n": 991, "auc": 0.6044399596367306}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2605210420841683, "accuracy_n": 499, "auc": 0.2605210420841683}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.312, "accuracy_n": 500, "auc": 0.312}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22556390977443608, "accuracy_n": 399, "auc": 0.22556390977443608}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6721854304635762, "accuracy_n": 302, "auc": 0.6721854304635762}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7203815261044177, "accuracy_n": 322, "auc": 0.7203815261044177}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.661889097744361, "accuracy_n": 292, "auc": 0.661889097744361}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5231143552311436, "accuracy_n": 411, "auc": 0.5231143552311436}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7244802724699222, "accuracy_n": 1902, "auc": 0.7244802724699222}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5264600520788005, "accuracy_n": 2000, "auc": 0.5264600520788005}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5546218487394958, "accuracy_n": 59, "auc": 0.5546218487394958}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5226130653266332, "accuracy_n": 995, "auc": 0.5226130653266332}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5181086519114688, "accuracy_n": 994, "auc": 0.5181086519114688}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.30434782608695654, "accuracy_n": 299, "auc": 0.30434782608695654}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5326305220883535, "accuracy_n": 322, "auc": 0.5326305220883535}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5224154135338346, "accuracy_n": 292, "auc": 0.5224154135338346}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5182481751824818, "accuracy_n": 411, "auc": 0.5182481751824818}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6504190994338287, "accuracy_n": 1902, "auc": 0.6504190994338287}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5125680370613791, "accuracy_n": 2000, "auc": 0.5125680370613791}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5224089635854341, "accuracy_n": 59, "auc": 0.5224089635854341}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5221327967806841, "accuracy_n": 994, "auc": 0.5221327967806841}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2965931863727455, "accuracy_n": 499, "auc": 0.2965931863727455}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.246, "accuracy_n": 500, "auc": 0.246}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.18045112781954886, "accuracy_n": 399, "auc": 0.18045112781954886}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5164118010503553, "accuracy_n": 322, "auc": 0.5164118010503553}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5908364661654135, "accuracy_n": 292, "auc": 0.5908364661654135}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5304136253041363, "accuracy_n": 411, "auc": 0.5304136253041363}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5779867745930645, "accuracy_n": 1902, "auc": 0.5779867745930645}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5127861157877994, "accuracy_n": 2000, "auc": 0.5127861157877994}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6050420168067226, "accuracy_n": 59, "auc": 0.6050420168067226}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5688442211055277, "accuracy_n": 995, "auc": 0.5688442211055277}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5583501006036218, "accuracy_n": 994, "auc": 0.5583501006036218}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.45090180360721444, "accuracy_n": 499, "auc": 0.45090180360721444}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2756892230576441, "accuracy_n": 399, "auc": 0.2756892230576441}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.609271523178808, "accuracy_n": 302, "auc": 0.609271523178808}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5811708371949336, "accuracy_n": 322, "auc": 0.5811708371949336}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5670582706766918, "accuracy_n": 292, "auc": 0.5670582706766918}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5012165450121655, "accuracy_n": 411, "auc": 0.5012165450121655}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7578301928520879, "accuracy_n": 1902, "auc": 0.7578301928520879}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5120678564961951, "accuracy_n": 2000, "auc": 0.5120678564961951}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5210084033613445, "accuracy_n": 59, "auc": 0.5210084033613445}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5352112676056338, "accuracy_n": 994, "auc": 0.5352112676056338}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.38276553106212424, "accuracy_n": 499, "auc": 0.38276553106212424}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.394, "accuracy_n": 500, "auc": 0.394}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3311036789297659, "accuracy_n": 299, "auc": 0.3311036789297659}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2606516290726817, "accuracy_n": 399, "auc": 0.2606516290726817}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6475903614457832, "accuracy_n": 322, "auc": 0.6475903614457832}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5990131578947369, "accuracy_n": 292, "auc": 0.5990131578947369}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5158150851581509, "accuracy_n": 411, "auc": 0.5158150851581509}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.597613123673036, "accuracy_n": 1902, "auc": 0.597613123673036}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5233249202962269, "accuracy_n": 2000, "auc": 0.5233249202962269}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5010060362173038, "accuracy_n": 994, "auc": 0.5010060362173038}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5983854692230071, "accuracy_n": 991, "auc": 0.5983854692230071}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.42084168336673344, "accuracy_n": 499, "auc": 0.42084168336673344}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 500, "auc": 0.28}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.32441471571906355, "accuracy_n": 299, "auc": 0.32441471571906355}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2756892230576441, "accuracy_n": 399, "auc": 0.2756892230576441}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5993377483443708, "accuracy_n": 302, "auc": 0.5993377483443708}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6095149830089589, "accuracy_n": 322, "auc": 0.6095149830089589}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6640037593984962, "accuracy_n": 292, "auc": 0.6640037593984962}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.610705596107056, "accuracy_n": 411, "auc": 0.610705596107056}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6489063605803256, "accuracy_n": 1902, "auc": 0.6489063605803256}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.508770666210502, "accuracy_n": 2000, "auc": 0.508770666210502}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364145658263305, "accuracy_n": 59, "auc": 0.5364145658263305}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346733668341709, "accuracy_n": 995, "auc": 0.5346733668341709}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5643863179074446, "accuracy_n": 994, "auc": 0.5643863179074446}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.32064128256513025, "accuracy_n": 499, "auc": 0.32064128256513025}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.39348370927318294, "accuracy_n": 399, "auc": 0.39348370927318294}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6291390728476821, "accuracy_n": 302, "auc": 0.6291390728476821}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5020466481309854, "accuracy_n": 322, "auc": 0.5020466481309854}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6026315789473684, "accuracy_n": 292, "auc": 0.6026315789473684}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6277372262773723, "accuracy_n": 411, "auc": 0.6277372262773723}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6119681086341118, "accuracy_n": 1902, "auc": 0.6119681086341118}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5113475964823301, "accuracy_n": 2000, "auc": 0.5113475964823301}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6008403361344538, "accuracy_n": 59, "auc": 0.6008403361344538}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5734406438631791, "accuracy_n": 994, "auc": 0.5734406438631791}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3066132264529058, "accuracy_n": 499, "auc": 0.3066132264529058}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20300751879699247, "accuracy_n": 399, "auc": 0.20300751879699247}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6688741721854304, "accuracy_n": 302, "auc": 0.6688741721854304}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5285372258263825, "accuracy_n": 322, "auc": 0.5285372258263825}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5610432330827069, "accuracy_n": 292, "auc": 0.5610432330827069}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5450121654501217, "accuracy_n": 411, "auc": 0.5450121654501217}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.532192144373673, "accuracy_n": 1902, "auc": 0.532192144373673}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6030046846911735, "accuracy_n": 2000, "auc": 0.6030046846911735}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6652661064425771, "accuracy_n": 59, "auc": 0.6652661064425771}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5613682092555332, "accuracy_n": 994, "auc": 0.5613682092555332}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25851703406813625, "accuracy_n": 499, "auc": 0.25851703406813625}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.304, "accuracy_n": 500, "auc": 0.304}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.19799498746867167, "accuracy_n": 399, "auc": 0.19799498746867167}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6157707754093297, "accuracy_n": 322, "auc": 0.6157707754093297}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5322368421052631, "accuracy_n": 292, "auc": 0.5322368421052631}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5036496350364964, "accuracy_n": 411, "auc": 0.5036496350364964}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6341980714791224, "accuracy_n": 1902, "auc": 0.6341980714791224}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5296211932507635, "accuracy_n": 2000, "auc": 0.5296211932507635}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5812324929971989, "accuracy_n": 59, "auc": 0.5812324929971989}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5185929648241207, "accuracy_n": 995, "auc": 0.5185929648241207}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5010060362173038, "accuracy_n": 994, "auc": 0.5010060362173038}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6115035317860746, "accuracy_n": 991, "auc": 0.6115035317860746}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27054108216432865, "accuracy_n": 499, "auc": 0.27054108216432865}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3210702341137124, "accuracy_n": 299, "auc": 0.3210702341137124}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2631578947368421, "accuracy_n": 399, "auc": 0.2631578947368421}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7890407784986099, "accuracy_n": 322, "auc": 0.7890407784986099}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7769266917293233, "accuracy_n": 292, "auc": 0.7769266917293233}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5401459854014599, "accuracy_n": 411, "auc": 0.5401459854014599}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7256745399858456, "accuracy_n": 1902, "auc": 0.7256745399858456}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.623363034055294, "accuracy_n": 2000, "auc": 0.623363034055294}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5980392156862745, "accuracy_n": 59, "auc": 0.5980392156862745}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5382293762575453, "accuracy_n": 994, "auc": 0.5382293762575453}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28256513026052105, "accuracy_n": 499, "auc": 0.28256513026052105}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.268, "accuracy_n": 500, "auc": 0.268}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7448640716713005, "accuracy_n": 322, "auc": 0.7448640716713005}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7678101503759398, "accuracy_n": 292, "auc": 0.7678101503759398}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.583941605839416, "accuracy_n": 411, "auc": 0.583941605839416}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.549274593064402, "accuracy_n": 1902, "auc": 0.549274593064402}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.606514951897635, "accuracy_n": 2000, "auc": 0.606514951897635}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6246498599439776, "accuracy_n": 59, "auc": 0.6246498599439776}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.312625250501002, "accuracy_n": 499, "auc": 0.312625250501002}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.252, "accuracy_n": 500, "auc": 0.252}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24749163879598662, "accuracy_n": 299, "auc": 0.24749163879598662}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5066225165562914, "accuracy_n": 302, "auc": 0.5066225165562914}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145196169292555, "accuracy_n": 322, "auc": 0.5145196169292555}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5331766917293234, "accuracy_n": 292, "auc": 0.5331766917293234}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6326034063260341, "accuracy_n": 411, "auc": 0.6326034063260341}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6722819355980185, "accuracy_n": 1902, "auc": 0.6722819355980185}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172567296794142, "accuracy_n": 2000, "auc": 0.5172567296794142}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140056022408964, "accuracy_n": 59, "auc": 0.5140056022408964}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.549748743718593, "accuracy_n": 995, "auc": 0.549748743718593}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6539235412474849, "accuracy_n": 994, "auc": 0.6539235412474849}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2665330661322645, "accuracy_n": 499, "auc": 0.2665330661322645}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2976588628762542, "accuracy_n": 299, "auc": 0.2976588628762542}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6324142724745134, "accuracy_n": 322, "auc": 0.6324142724745134}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5335526315789473, "accuracy_n": 292, "auc": 0.5335526315789473}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5450121654501217, "accuracy_n": 411, "auc": 0.5450121654501217}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8860613499646143, "accuracy_n": 1902, "auc": 0.8860613499646143}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5001095395437752, "accuracy_n": 2000, "auc": 0.5001095395437752}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5364145658263305, "accuracy_n": 59, "auc": 0.5364145658263305}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.685427135678392, "accuracy_n": 995, "auc": 0.685427135678392}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5674044265593562, "accuracy_n": 994, "auc": 0.5674044265593562}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.30861723446893785, "accuracy_n": 499, "auc": 0.30861723446893785}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 500, "auc": 0.3}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2608695652173913, "accuracy_n": 299, "auc": 0.2608695652173913}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5132450331125827, "accuracy_n": 302, "auc": 0.5132450331125827}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7448640716713006, "accuracy_n": 322, "auc": 0.7448640716713006}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7503289473684212, "accuracy_n": 292, "auc": 0.7503289473684212}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5644768856447688, "accuracy_n": 411, "auc": 0.5644768856447688}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5635483014861995, "accuracy_n": 1902, "auc": 0.5635483014861995}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6826764461970771, "accuracy_n": 2000, "auc": 0.6826764461970771}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6106442577030812, "accuracy_n": 59, "auc": 0.6106442577030812}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 994, "auc": 0.5553319919517102}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.583758109360519, "accuracy_n": 322, "auc": 0.583758109360519}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5526785714285715, "accuracy_n": 292, "auc": 0.5526785714285715}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5255474452554745, "accuracy_n": 411, "auc": 0.5255474452554745}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7533339968152866, "accuracy_n": 1902, "auc": 0.7533339968152866}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.538132265747935, "accuracy_n": 2000, "auc": 0.538132265747935}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6148459383753502, "accuracy_n": 59, "auc": 0.6148459383753502}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5321931589537223, "accuracy_n": 994, "auc": 0.5321931589537223}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.276, "accuracy_n": 500, "auc": 0.276}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7483443708609272, "accuracy_n": 302, "auc": 0.7483443708609272}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5361831943157245, "accuracy_n": 322, "auc": 0.5361831943157245}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.524812030075188, "accuracy_n": 292, "auc": 0.524812030075188}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6228710462287105, "accuracy_n": 411, "auc": 0.6228710462287105}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7654215322009909, "accuracy_n": 1902, "auc": 0.7654215322009909}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5147648301036674, "accuracy_n": 2000, "auc": 0.5147648301036674}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6008403361344538, "accuracy_n": 59, "auc": 0.6008403361344538}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8461538461538463, "accuracy_n": 23, "auc": 0.8461538461538463}}
