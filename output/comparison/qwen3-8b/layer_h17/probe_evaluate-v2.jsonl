{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9175879396984925, "accuracy_n": 995, "auc": 0.9175879396984925}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9185110663983903, "accuracy_n": 994, "auc": 0.9185110663983903}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.776173285198556, "accuracy_n": 277, "auc": 0.776173285198556}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6791120080726539, "accuracy_n": 991, "auc": 0.6791120080726539}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2865731462925852, "accuracy_n": 499, "auc": 0.2865731462925852}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9481773246833488, "accuracy_n": 322, "auc": 0.9481773246833488}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7188439849624061, "accuracy_n": 292, "auc": 0.7188439849624061}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8186980272469921, "accuracy_n": 1902, "auc": 0.8186980272469921}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8188856177079926, "accuracy_n": 2000, "auc": 0.8188856177079926}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8935574229691876, "accuracy_n": 59, "auc": 0.8935574229691876}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.935678391959799, "accuracy_n": 995, "auc": 0.935678391959799}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9426559356136821, "accuracy_n": 994, "auc": 0.9426559356136821}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.779783393501805, "accuracy_n": 277, "auc": 0.779783393501805}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6841574167507568, "accuracy_n": 991, "auc": 0.6841574167507568}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.312625250501002, "accuracy_n": 499, "auc": 0.312625250501002}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3082706766917293, "accuracy_n": 399, "auc": 0.3082706766917293}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9384847080630214, "accuracy_n": 322, "auc": 0.9384847080630214}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7572838345864663, "accuracy_n": 292, "auc": 0.7572838345864663}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8490998761500353, "accuracy_n": 1902, "auc": 0.8490998761500353}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8216035988992026, "accuracy_n": 2000, "auc": 0.8216035988992026}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8557422969187676, "accuracy_n": 59, "auc": 0.8557422969187676}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9095477386934674, "accuracy_n": 995, "auc": 0.9095477386934674}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7906137184115524, "accuracy_n": 277, "auc": 0.7906137184115524}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6892028254288597, "accuracy_n": 991, "auc": 0.6892028254288597}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.274, "accuracy_n": 500, "auc": 0.274}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3007518796992481, "accuracy_n": 399, "auc": 0.3007518796992481}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9548192771084336, "accuracy_n": 322, "auc": 0.9548192771084336}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7794172932330827, "accuracy_n": 292, "auc": 0.7794172932330827}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.785645567940552, "accuracy_n": 1902, "auc": 0.785645567940552}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8253039347204342, "accuracy_n": 2000, "auc": 0.8253039347204342}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8823529411764706, "accuracy_n": 59, "auc": 0.8823529411764706}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9115577889447236, "accuracy_n": 995, "auc": 0.9115577889447236}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.93158953722334, "accuracy_n": 994, "auc": 0.93158953722334}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6823104693140795, "accuracy_n": 277, "auc": 0.6823104693140795}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5529767911200807, "accuracy_n": 991, "auc": 0.5529767911200807}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3967935871743487, "accuracy_n": 499, "auc": 0.3967935871743487}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.372, "accuracy_n": 500, "auc": 0.372}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.36454849498327757, "accuracy_n": 299, "auc": 0.36454849498327757}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3308270676691729, "accuracy_n": 399, "auc": 0.3308270676691729}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9834437086092715, "accuracy_n": 302, "auc": 0.9834437086092715}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8934970651838121, "accuracy_n": 322, "auc": 0.8934970651838121}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7229793233082706, "accuracy_n": 292, "auc": 0.7229793233082706}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5831508757961783, "accuracy_n": 1902, "auc": 0.5831508757961783}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.79088450930786, "accuracy_n": 2000, "auc": 0.79088450930786}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8067226890756302, "accuracy_n": 59, "auc": 0.8067226890756302}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8307692307692308, "accuracy_n": 23, "auc": 0.8307692307692308}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6609657947686117, "accuracy_n": 994, "auc": 0.6609657947686117}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2865731462925852, "accuracy_n": 499, "auc": 0.2865731462925852}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.38, "accuracy_n": 500, "auc": 0.38}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.33444816053511706, "accuracy_n": 299, "auc": 0.33444816053511706}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2882205513784461, "accuracy_n": 399, "auc": 0.2882205513784461}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8795180722891567, "accuracy_n": 322, "auc": 0.8795180722891567}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.668609022556391, "accuracy_n": 292, "auc": 0.668609022556391}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5537376150035386, "accuracy_n": 1902, "auc": 0.5537376150035386}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6535359264694555, "accuracy_n": 2000, "auc": 0.6535359264694555}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7170868347338937, "accuracy_n": 59, "auc": 0.7170868347338937}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5115577889447236, "accuracy_n": 995, "auc": 0.5115577889447236}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5181086519114688, "accuracy_n": 994, "auc": 0.5181086519114688}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5811623246492986, "accuracy_n": 499, "auc": 0.5811623246492986}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.498, "accuracy_n": 500, "auc": 0.498}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4882943143812709, "accuracy_n": 299, "auc": 0.4882943143812709}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5764411027568922, "accuracy_n": 399, "auc": 0.5764411027568922}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8013245033112583, "accuracy_n": 302, "auc": 0.8013245033112583}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9546261970960765, "accuracy_n": 322, "auc": 0.9546261970960765}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8144266917293234, "accuracy_n": 292, "auc": 0.8144266917293234}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951338199513382, "accuracy_n": 411, "auc": 0.9951338199513382}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6294331652512385, "accuracy_n": 1902, "auc": 0.6294331652512385}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6297523405949548, "accuracy_n": 2000, "auc": 0.6297523405949548}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7184873949579832, "accuracy_n": 59, "auc": 0.7184873949579832}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7846153846153847, "accuracy_n": 23, "auc": 0.7846153846153847}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5316582914572864, "accuracy_n": 995, "auc": 0.5316582914572864}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5663983903420523, "accuracy_n": 994, "auc": 0.5663983903420523}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5090180360721442, "accuracy_n": 499, "auc": 0.5090180360721442}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.612, "accuracy_n": 500, "auc": 0.612}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5852842809364549, "accuracy_n": 299, "auc": 0.5852842809364549}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6892230576441103, "accuracy_n": 399, "auc": 0.6892230576441103}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6291390728476821, "accuracy_n": 302, "auc": 0.6291390728476821}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9162418906394811, "accuracy_n": 322, "auc": 0.9162418906394811}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8228383458646616, "accuracy_n": 292, "auc": 0.8228383458646616}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9708029197080292, "accuracy_n": 411, "auc": 0.9708029197080292}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5745720541401274, "accuracy_n": 1902, "auc": 0.5745720541401274}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5922968191517138, "accuracy_n": 2000, "auc": 0.5922968191517138}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6106442577030813, "accuracy_n": 59, "auc": 0.6106442577030813}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7923076923076924, "accuracy_n": 23, "auc": 0.7923076923076924}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6458752515090543, "accuracy_n": 994, "auc": 0.6458752515090543}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5230460921843687, "accuracy_n": 499, "auc": 0.5230460921843687}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.612, "accuracy_n": 500, "auc": 0.612}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6917293233082706, "accuracy_n": 399, "auc": 0.6917293233082706}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7450331125827815, "accuracy_n": 302, "auc": 0.7450331125827815}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9483704046957059, "accuracy_n": 322, "auc": 0.9483704046957059}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8287124060150376, "accuracy_n": 292, "auc": 0.8287124060150376}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951338199513382, "accuracy_n": 411, "auc": 0.9951338199513382}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5689700548478415, "accuracy_n": 1902, "auc": 0.5689700548478415}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7265292770690219, "accuracy_n": 2000, "auc": 0.7265292770690219}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6946778711484594, "accuracy_n": 59, "auc": 0.6946778711484594}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.614070351758794, "accuracy_n": 995, "auc": 0.614070351758794}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6559356136820925, "accuracy_n": 994, "auc": 0.6559356136820925}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4909819639278557, "accuracy_n": 499, "auc": 0.4909819639278557}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.604, "accuracy_n": 500, "auc": 0.604}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.681704260651629, "accuracy_n": 399, "auc": 0.681704260651629}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9519616929255483, "accuracy_n": 322, "auc": 0.9519616929255483}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.856437969924812, "accuracy_n": 292, "auc": 0.856437969924812}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9927007299270073, "accuracy_n": 411, "auc": 0.9927007299270073}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5157024062278839, "accuracy_n": 1902, "auc": 0.5157024062278839}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6412975084005326, "accuracy_n": 2000, "auc": 0.6412975084005326}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6666666666666667, "accuracy_n": 59, "auc": 0.6666666666666667}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9396984924623115, "accuracy_n": 995, "auc": 0.9396984924623115}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9225352112676056, "accuracy_n": 994, "auc": 0.9225352112676056}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6859205776173285, "accuracy_n": 277, "auc": 0.6859205776173285}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7436932391523713, "accuracy_n": 991, "auc": 0.7436932391523713}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3527054108216433, "accuracy_n": 499, "auc": 0.3527054108216433}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.268, "accuracy_n": 500, "auc": 0.268}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.32441471571906355, "accuracy_n": 299, "auc": 0.32441471571906355}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9682190299660178, "accuracy_n": 322, "auc": 0.9682190299660178}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7839755639097745, "accuracy_n": 292, "auc": 0.7839755639097745}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6707083775654636, "accuracy_n": 1902, "auc": 0.6707083775654636}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8127183913392734, "accuracy_n": 2000, "auc": 0.8127183913392734}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9635854341736695, "accuracy_n": 59, "auc": 0.9635854341736695}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9386934673366835, "accuracy_n": 995, "auc": 0.9386934673366835}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9496981891348089, "accuracy_n": 994, "auc": 0.9496981891348089}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6236125126135217, "accuracy_n": 991, "auc": 0.6236125126135217}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2608695652173913, "accuracy_n": 299, "auc": 0.2608695652173913}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9759422304603028, "accuracy_n": 322, "auc": 0.9759422304603028}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7901785714285714, "accuracy_n": 292, "auc": 0.7901785714285714}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6454252919320594, "accuracy_n": 1902, "auc": 0.6454252919320594}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.786684993282575, "accuracy_n": 2000, "auc": 0.786684993282575}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9047619047619047, "accuracy_n": 59, "auc": 0.9047619047619047}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8020100502512563, "accuracy_n": 995, "auc": 0.8020100502512563}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9285714285714286, "accuracy_n": 994, "auc": 0.9285714285714286}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6209386281588448, "accuracy_n": 277, "auc": 0.6209386281588448}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.727547931382442, "accuracy_n": 991, "auc": 0.727547931382442}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.45090180360721444, "accuracy_n": 499, "auc": 0.45090180360721444}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.49498327759197325, "accuracy_n": 299, "auc": 0.49498327759197325}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.43609022556390975, "accuracy_n": 399, "auc": 0.43609022556390975}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9786067346308311, "accuracy_n": 322, "auc": 0.9786067346308311}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.793515037593985, "accuracy_n": 292, "auc": 0.793515037593985}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6041688782731776, "accuracy_n": 1902, "auc": 0.6041688782731776}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8285691134499554, "accuracy_n": 2000, "auc": 0.8285691134499554}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9327731092436975, "accuracy_n": 59, "auc": 0.9327731092436975}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.671356783919598, "accuracy_n": 995, "auc": 0.671356783919598}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9486921529175051, "accuracy_n": 994, "auc": 0.9486921529175051}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7328519855595668, "accuracy_n": 277, "auc": 0.7328519855595668}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6912209889001009, "accuracy_n": 991, "auc": 0.6912209889001009}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4729458917835671, "accuracy_n": 499, "auc": 0.4729458917835671}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.448, "accuracy_n": 500, "auc": 0.448}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5284280936454849, "accuracy_n": 299, "auc": 0.5284280936454849}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5012531328320802, "accuracy_n": 399, "auc": 0.5012531328320802}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.971424158171146, "accuracy_n": 322, "auc": 0.971424158171146}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8644736842105263, "accuracy_n": 292, "auc": 0.8644736842105263}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5618752211606511, "accuracy_n": 1902, "auc": 0.5618752211606511}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8380335301043677, "accuracy_n": 2000, "auc": 0.8380335301043677}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8529411764705882, "accuracy_n": 59, "auc": 0.8529411764705882}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7577889447236181, "accuracy_n": 995, "auc": 0.7577889447236181}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9456740442655935, "accuracy_n": 994, "auc": 0.9456740442655935}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6508577194752775, "accuracy_n": 991, "auc": 0.6508577194752775}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.36072144288577157, "accuracy_n": 499, "auc": 0.36072144288577157}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.304, "accuracy_n": 500, "auc": 0.304}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3712374581939799, "accuracy_n": 299, "auc": 0.3712374581939799}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3358395989974937, "accuracy_n": 399, "auc": 0.3358395989974937}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9604572134692616, "accuracy_n": 322, "auc": 0.9604572134692616}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7781015037593986, "accuracy_n": 292, "auc": 0.7781015037593986}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.626382254069356, "accuracy_n": 1902, "auc": 0.626382254069356}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.819440818135347, "accuracy_n": 2000, "auc": 0.819440818135347}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9551820728291317, "accuracy_n": 59, "auc": 0.9551820728291317}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9055276381909547, "accuracy_n": 995, "auc": 0.9055276381909547}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8440643863179075, "accuracy_n": 994, "auc": 0.8440643863179075}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.326, "accuracy_n": 500, "auc": 0.326}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30434782608695654, "accuracy_n": 299, "auc": 0.30434782608695654}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7052980132450332, "accuracy_n": 302, "auc": 0.7052980132450332}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6387859128822984, "accuracy_n": 322, "auc": 0.6387859128822984}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5069548872180452, "accuracy_n": 292, "auc": 0.5069548872180452}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6082725060827251, "accuracy_n": 411, "auc": 0.6082725060827251}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.891530652866242, "accuracy_n": 1902, "auc": 0.891530652866242}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5089302238107957, "accuracy_n": 2000, "auc": 0.5089302238107957}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6344537815126051, "accuracy_n": 59, "auc": 0.6344537815126051}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.523076923076923, "accuracy_n": 23, "auc": 0.523076923076923}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9216080402010051, "accuracy_n": 995, "auc": 0.9216080402010051}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7364620938628159, "accuracy_n": 277, "auc": 0.7364620938628159}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7598385469223007, "accuracy_n": 991, "auc": 0.7598385469223007}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3967935871743487, "accuracy_n": 499, "auc": 0.3967935871743487}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4180602006688963, "accuracy_n": 299, "auc": 0.4180602006688963}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3383458646616541, "accuracy_n": 399, "auc": 0.3383458646616541}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9614998455359901, "accuracy_n": 322, "auc": 0.9614998455359901}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7680921052631579, "accuracy_n": 292, "auc": 0.7680921052631579}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6924241418966738, "accuracy_n": 1902, "auc": 0.6924241418966738}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8714445914975306, "accuracy_n": 2000, "auc": 0.8714445914975306}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9467787114845938, "accuracy_n": 59, "auc": 0.9467787114845938}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9246231155778895, "accuracy_n": 995, "auc": 0.9246231155778895}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9356136820925554, "accuracy_n": 994, "auc": 0.9356136820925554}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7527749747729566, "accuracy_n": 991, "auc": 0.7527749747729566}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.38276553106212424, "accuracy_n": 499, "auc": 0.38276553106212424}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.391304347826087, "accuracy_n": 299, "auc": 0.391304347826087}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3208020050125313, "accuracy_n": 399, "auc": 0.3208020050125313}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9677942539388322, "accuracy_n": 322, "auc": 0.9677942539388322}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.762218045112782, "accuracy_n": 292, "auc": 0.762218045112782}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7314777954706299, "accuracy_n": 1902, "auc": 0.7314777954706299}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7773646286309357, "accuracy_n": 2000, "auc": 0.7773646286309357}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9047619047619048, "accuracy_n": 59, "auc": 0.9047619047619048}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7728643216080402, "accuracy_n": 995, "auc": 0.7728643216080402}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 994, "auc": 0.545271629778672}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.615539858728557, "accuracy_n": 991, "auc": 0.615539858728557}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3667334669338677, "accuracy_n": 499, "auc": 0.3667334669338677}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.262, "accuracy_n": 500, "auc": 0.262}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6790237874575225, "accuracy_n": 322, "auc": 0.6790237874575225}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5404135338345865, "accuracy_n": 292, "auc": 0.5404135338345865}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9294403892944039, "accuracy_n": 411, "auc": 0.9294403892944039}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6449044585987261, "accuracy_n": 1902, "auc": 0.6449044585987261}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6142097297124262, "accuracy_n": 2000, "auc": 0.6142097297124262}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6904761904761906, "accuracy_n": 59, "auc": 0.6904761904761906}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9461538461538461, "accuracy_n": 23, "auc": 0.9461538461538461}}
