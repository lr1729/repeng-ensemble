{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9368104312938816, "accuracy_n": 997, "auc": 0.9368104312938816}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9568273092369478, "accuracy_n": 996, "auc": 0.9568273092369478}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9909274193548387, "accuracy_n": 992, "auc": 0.9909274193548387}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8050541516245487, "accuracy_n": 277, "auc": 0.8050541516245487}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7248995983935743, "accuracy_n": 996, "auc": 0.7248995983935743}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7074148296593187, "accuracy_n": 499, "auc": 0.7074148296593187}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 500, "auc": 0.77}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7993311036789298, "accuracy_n": 299, "auc": 0.7993311036789298}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9398797595190381, "accuracy_n": 499, "auc": 0.9398797595190381}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.795, "accuracy_n": 400, "auc": 0.795}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9854417670682731, "accuracy_n": 322, "auc": 0.9854417670682731}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9156954887218046, "accuracy_n": 292, "auc": 0.9156954887218046}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9378134403209629, "accuracy_n": 997, "auc": 0.9378134403209629}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9588353413654619, "accuracy_n": 996, "auc": 0.9588353413654619}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9899193548387096, "accuracy_n": 992, "auc": 0.9899193548387096}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.851985559566787, "accuracy_n": 277, "auc": 0.851985559566787}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7600401606425703, "accuracy_n": 996, "auc": 0.7600401606425703}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6513026052104208, "accuracy_n": 499, "auc": 0.6513026052104208}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7892976588628763, "accuracy_n": 299, "auc": 0.7892976588628763}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9218436873747495, "accuracy_n": 499, "auc": 0.9218436873747495}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7375, "accuracy_n": 400, "auc": 0.7375}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9735099337748344, "accuracy_n": 302, "auc": 0.9735099337748344}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9813870868087736, "accuracy_n": 322, "auc": 0.9813870868087736}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9015507518796992, "accuracy_n": 292, "auc": 0.9015507518796992}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "amazon_polarity", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9358074222668004, "accuracy_n": 997, "auc": 0.9358074222668004}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9578313253012049, "accuracy_n": 996, "auc": 0.9578313253012049}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9437751004016064, "accuracy_n": 996, "auc": 0.9437751004016064}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9909274193548387, "accuracy_n": 992, "auc": 0.9909274193548387}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8086642599277978, "accuracy_n": 277, "auc": 0.8086642599277978}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7168674698795181, "accuracy_n": 996, "auc": 0.7168674698795181}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6513026052104208, "accuracy_n": 499, "auc": 0.6513026052104208}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.694, "accuracy_n": 500, "auc": 0.694}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6989966555183946, "accuracy_n": 299, "auc": 0.6989966555183946}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8496993987975952, "accuracy_n": 499, "auc": 0.8496993987975952}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.655, "accuracy_n": 400, "auc": 0.655}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.956953642384106, "accuracy_n": 302, "auc": 0.956953642384106}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9824683348779735, "accuracy_n": 322, "auc": 0.9824683348779735}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9234022556390978, "accuracy_n": 292, "auc": 0.9234022556390978}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9368104312938816, "accuracy_n": 997, "auc": 0.9368104312938816}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9578313253012049, "accuracy_n": 996, "auc": 0.9578313253012049}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9939516129032258, "accuracy_n": 992, "auc": 0.9939516129032258}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7118473895582329, "accuracy_n": 996, "auc": 0.7118473895582329}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6432865731462926, "accuracy_n": 499, "auc": 0.6432865731462926}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.716, "accuracy_n": 500, "auc": 0.716}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 299, "auc": 0.782608695652174}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9038076152304609, "accuracy_n": 499, "auc": 0.9038076152304609}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7175, "accuracy_n": 400, "auc": 0.7175}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9768211920529801, "accuracy_n": 302, "auc": 0.9768211920529801}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9859051590979302, "accuracy_n": 322, "auc": 0.9859051590979302}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.924577067669173, "accuracy_n": 292, "auc": 0.924577067669173}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "dbpedia_14", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9368104312938816, "accuracy_n": 997, "auc": 0.9368104312938816}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9548192771084337, "accuracy_n": 996, "auc": 0.9548192771084337}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9879032258064516, "accuracy_n": 992, "auc": 0.9879032258064516}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.631768953068592, "accuracy_n": 277, "auc": 0.631768953068592}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6937751004016064, "accuracy_n": 996, "auc": 0.6937751004016064}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5531062124248497, "accuracy_n": 499, "auc": 0.5531062124248497}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.476, "accuracy_n": 500, "auc": 0.476}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4414715719063545, "accuracy_n": 299, "auc": 0.4414715719063545}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5671342685370742, "accuracy_n": 499, "auc": 0.5671342685370742}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7649006622516556, "accuracy_n": 302, "auc": 0.7649006622516556}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9685279579857893, "accuracy_n": 322, "auc": 0.9685279579857893}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9177631578947368, "accuracy_n": 292, "auc": 0.9177631578947368}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9358074222668004, "accuracy_n": 997, "auc": 0.9358074222668004}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9548192771084337, "accuracy_n": 996, "auc": 0.9548192771084337}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9307228915662651, "accuracy_n": 996, "auc": 0.9307228915662651}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9848790322580645, "accuracy_n": 992, "auc": 0.9848790322580645}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8736462093862816, "accuracy_n": 277, "auc": 0.8736462093862816}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8363453815261044, "accuracy_n": 996, "auc": 0.8363453815261044}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270541082164328, "accuracy_n": 499, "auc": 0.5270541082164328}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 500, "auc": 0.68}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 299, "auc": 0.5384615384615384}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7074148296593187, "accuracy_n": 499, "auc": 0.7074148296593187}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 400, "auc": 0.72}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6986754966887417, "accuracy_n": 302, "auc": 0.6986754966887417}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6913036762434354, "accuracy_n": 322, "auc": 0.6913036762434354}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7140507518796994, "accuracy_n": 292, "auc": 0.7140507518796994}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9200968523002422, "accuracy_n": 413, "auc": 0.9200968523002422}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9267803410230692, "accuracy_n": 997, "auc": 0.9267803410230692}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9497991967871486, "accuracy_n": 996, "auc": 0.9497991967871486}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9909274193548387, "accuracy_n": 992, "auc": 0.9909274193548387}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8628158844765343, "accuracy_n": 277, "auc": 0.8628158844765343}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8744979919678715, "accuracy_n": 996, "auc": 0.8744979919678715}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6312625250501002, "accuracy_n": 499, "auc": 0.6312625250501002}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.682, "accuracy_n": 500, "auc": 0.682}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7635270541082164, "accuracy_n": 499, "auc": 0.7635270541082164}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7325, "accuracy_n": 400, "auc": 0.7325}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9893419833178869, "accuracy_n": 322, "auc": 0.9893419833178869}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8984492481203008, "accuracy_n": 292, "auc": 0.8984492481203008}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9107321965897693, "accuracy_n": 997, "auc": 0.9107321965897693}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9246987951807228, "accuracy_n": 996, "auc": 0.9246987951807228}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7299196787148594, "accuracy_n": 996, "auc": 0.7299196787148594}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9828629032258065, "accuracy_n": 992, "auc": 0.9828629032258065}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7653429602888087, "accuracy_n": 277, "auc": 0.7653429602888087}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6606425702811245, "accuracy_n": 996, "auc": 0.6606425702811245}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.782, "accuracy_n": 500, "auc": 0.782}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8528428093645485, "accuracy_n": 299, "auc": 0.8528428093645485}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9659318637274549, "accuracy_n": 499, "auc": 0.9659318637274549}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8225, "accuracy_n": 400, "auc": 0.8225}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8940397350993378, "accuracy_n": 302, "auc": 0.8940397350993378}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9905004633920297, "accuracy_n": 322, "auc": 0.9905004633920297}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9553101503759399, "accuracy_n": 292, "auc": 0.9553101503759399}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.917753259779338, "accuracy_n": 997, "auc": 0.917753259779338}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8975903614457831, "accuracy_n": 996, "auc": 0.8975903614457831}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7309236947791165, "accuracy_n": 996, "auc": 0.7309236947791165}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9858870967741935, "accuracy_n": 992, "auc": 0.9858870967741935}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7018072289156626, "accuracy_n": 996, "auc": 0.7018072289156626}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.786, "accuracy_n": 500, "auc": 0.786}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.842809364548495, "accuracy_n": 299, "auc": 0.842809364548495}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9679358717434869, "accuracy_n": 499, "auc": 0.9679358717434869}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8175, "accuracy_n": 400, "auc": 0.8175}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9503311258278145, "accuracy_n": 302, "auc": 0.9503311258278145}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9889172072907013, "accuracy_n": 322, "auc": 0.9889172072907013}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.962640977443609, "accuracy_n": 292, "auc": 0.962640977443609}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8776328986960883, "accuracy_n": 997, "auc": 0.8776328986960883}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8443775100401606, "accuracy_n": 996, "auc": 0.8443775100401606}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8062248995983936, "accuracy_n": 996, "auc": 0.8062248995983936}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9798387096774194, "accuracy_n": 992, "auc": 0.9798387096774194}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6534296028880866, "accuracy_n": 277, "auc": 0.6534296028880866}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6927710843373494, "accuracy_n": 996, "auc": 0.6927710843373494}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7374749498997996, "accuracy_n": 499, "auc": 0.7374749498997996}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.778, "accuracy_n": 500, "auc": 0.778}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8327759197324415, "accuracy_n": 299, "auc": 0.8327759197324415}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9639278557114228, "accuracy_n": 499, "auc": 0.9639278557114228}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.815, "accuracy_n": 400, "auc": 0.815}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9668874172185431, "accuracy_n": 302, "auc": 0.9668874172185431}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9533518690145196, "accuracy_n": 322, "auc": 0.9533518690145196}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9536184210526316, "accuracy_n": 292, "auc": 0.9536184210526316}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8966900702106319, "accuracy_n": 997, "auc": 0.8966900702106319}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.856425702811245, "accuracy_n": 996, "auc": 0.856425702811245}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7469879518072289, "accuracy_n": 996, "auc": 0.7469879518072289}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9848790322580645, "accuracy_n": 992, "auc": 0.9848790322580645}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6606498194945848, "accuracy_n": 277, "auc": 0.6606498194945848}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6977911646586346, "accuracy_n": 996, "auc": 0.6977911646586346}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.778, "accuracy_n": 500, "auc": 0.778}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8494983277591973, "accuracy_n": 299, "auc": 0.8494983277591973}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9639278557114228, "accuracy_n": 499, "auc": 0.9639278557114228}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8125, "accuracy_n": 400, "auc": 0.8125}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.956953642384106, "accuracy_n": 302, "auc": 0.956953642384106}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9652455977757183, "accuracy_n": 322, "auc": 0.9652455977757183}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9659774436090226, "accuracy_n": 292, "auc": 0.9659774436090226}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_easy", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9106425702811245, "accuracy_n": 996, "auc": 0.9106425702811245}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7128514056224899, "accuracy_n": 996, "auc": 0.7128514056224899}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9808467741935484, "accuracy_n": 992, "auc": 0.9808467741935484}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.779783393501805, "accuracy_n": 277, "auc": 0.779783393501805}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7329317269076305, "accuracy_n": 996, "auc": 0.7329317269076305}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.749498997995992, "accuracy_n": 499, "auc": 0.749498997995992}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.842809364548495, "accuracy_n": 299, "auc": 0.842809364548495}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9599198396793587, "accuracy_n": 499, "auc": 0.9599198396793587}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8175, "accuracy_n": 400, "auc": 0.8175}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8543046357615894, "accuracy_n": 302, "auc": 0.8543046357615894}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9107970342910101, "accuracy_n": 322, "auc": 0.9107970342910101}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9647556390977444, "accuracy_n": 292, "auc": 0.9647556390977444}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9518072289156626, "accuracy_n": 996, "auc": 0.9518072289156626}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8704819277108434, "accuracy_n": 996, "auc": 0.8704819277108434}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.969758064516129, "accuracy_n": 992, "auc": 0.969758064516129}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6967509025270758, "accuracy_n": 277, "auc": 0.6967509025270758}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.822289156626506, "accuracy_n": 996, "auc": 0.822289156626506}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.47695390781563124, "accuracy_n": 499, "auc": 0.47695390781563124}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.664, "accuracy_n": 500, "auc": 0.664}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7090301003344481, "accuracy_n": 299, "auc": 0.7090301003344481}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.845691382765531, "accuracy_n": 499, "auc": 0.845691382765531}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4975, "accuracy_n": 400, "auc": 0.4975}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9830089589125734, "accuracy_n": 322, "auc": 0.9830089589125734}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6870770676691729, "accuracy_n": 292, "auc": 0.6870770676691729}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9097291875626881, "accuracy_n": 997, "auc": 0.9097291875626881}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9397590361445783, "accuracy_n": 996, "auc": 0.9397590361445783}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7730923694779116, "accuracy_n": 996, "auc": 0.7730923694779116}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8699596774193549, "accuracy_n": 992, "auc": 0.8699596774193549}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8664259927797834, "accuracy_n": 277, "auc": 0.8664259927797834}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 996, "auc": 0.8333333333333334}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5731462925851704, "accuracy_n": 499, "auc": 0.5731462925851704}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.648, "accuracy_n": 500, "auc": 0.648}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6755852842809364, "accuracy_n": 299, "auc": 0.6755852842809364}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8637274549098196, "accuracy_n": 499, "auc": 0.8637274549098196}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 400, "auc": 0.71}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9967562557924003, "accuracy_n": 322, "auc": 0.9967562557924003}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5623120300751879, "accuracy_n": 292, "auc": 0.5623120300751879}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9157472417251755, "accuracy_n": 997, "auc": 0.9157472417251755}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9397590361445783, "accuracy_n": 996, "auc": 0.9397590361445783}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8363453815261044, "accuracy_n": 996, "auc": 0.8363453815261044}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9758064516129032, "accuracy_n": 992, "auc": 0.9758064516129032}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8231046931407943, "accuracy_n": 277, "auc": 0.8231046931407943}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8032128514056225, "accuracy_n": 996, "auc": 0.8032128514056225}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5531062124248497, "accuracy_n": 499, "auc": 0.5531062124248497}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.702, "accuracy_n": 500, "auc": 0.702}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 299, "auc": 0.782608695652174}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9118236472945892, "accuracy_n": 499, "auc": 0.9118236472945892}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6175, "accuracy_n": 400, "auc": 0.6175}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.991890639481001, "accuracy_n": 322, "auc": 0.991890639481001}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7437969924812031, "accuracy_n": 292, "auc": 0.7437969924812031}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9528112449799196, "accuracy_n": 996, "auc": 0.9528112449799196}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9397590361445783, "accuracy_n": 996, "auc": 0.9397590361445783}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9939516129032258, "accuracy_n": 992, "auc": 0.9939516129032258}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8267148014440433, "accuracy_n": 277, "auc": 0.8267148014440433}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8242971887550201, "accuracy_n": 996, "auc": 0.8242971887550201}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7294589178356713, "accuracy_n": 499, "auc": 0.7294589178356713}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.766, "accuracy_n": 500, "auc": 0.766}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8327759197324415, "accuracy_n": 299, "auc": 0.8327759197324415}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9599198396793587, "accuracy_n": 499, "auc": 0.9599198396793587}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.805, "accuracy_n": 400, "auc": 0.805}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9536423841059603, "accuracy_n": 302, "auc": 0.9536423841059603}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8908325610132839, "accuracy_n": 322, "auc": 0.8908325610132839}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9720864661654135, "accuracy_n": 292, "auc": 0.9720864661654135}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9408224674022067, "accuracy_n": 997, "auc": 0.9408224674022067}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9538152610441767, "accuracy_n": 996, "auc": 0.9538152610441767}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9136546184738956, "accuracy_n": 996, "auc": 0.9136546184738956}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9949596774193549, "accuracy_n": 992, "auc": 0.9949596774193549}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.851985559566787, "accuracy_n": 277, "auc": 0.851985559566787}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7841365461847389, "accuracy_n": 996, "auc": 0.7841365461847389}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7034068136272545, "accuracy_n": 499, "auc": 0.7034068136272545}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8160535117056856, "accuracy_n": 299, "auc": 0.8160535117056856}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9619238476953907, "accuracy_n": 499, "auc": 0.9619238476953907}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.815, "accuracy_n": 400, "auc": 0.815}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9898439913500154, "accuracy_n": 322, "auc": 0.9898439913500154}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8134398496240602, "accuracy_n": 292, "auc": 0.8134398496240602}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
