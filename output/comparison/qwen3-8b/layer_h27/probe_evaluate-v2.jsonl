{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9386934673366835, "accuracy_n": 995, "auc": 0.9386934673366835}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7103935418768921, "accuracy_n": 991, "auc": 0.7103935418768921}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6993987975951904, "accuracy_n": 499, "auc": 0.6993987975951904}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.758, "accuracy_n": 500, "auc": 0.758}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7959866220735786, "accuracy_n": 299, "auc": 0.7959866220735786}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8020050125313283, "accuracy_n": 399, "auc": 0.8020050125313283}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9873725671918443, "accuracy_n": 322, "auc": 0.9873725671918443}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9170582706766917, "accuracy_n": 292, "auc": 0.9170582706766917}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.70588508492569, "accuracy_n": 1902, "auc": 0.70588508492569}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7309578757931613, "accuracy_n": 2000, "auc": 0.7309578757931613}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9131652661064424, "accuracy_n": 59, "auc": 0.9131652661064424}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9692307692307693, "accuracy_n": 23, "auc": 0.9692307692307693}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9376884422110553, "accuracy_n": 995, "auc": 0.9376884422110553}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9446680080482898, "accuracy_n": 994, "auc": 0.9446680080482898}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7978339350180506, "accuracy_n": 277, "auc": 0.7978339350180506}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7235116044399597, "accuracy_n": 991, "auc": 0.7235116044399597}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6472945891783567, "accuracy_n": 499, "auc": 0.6472945891783567}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.676, "accuracy_n": 500, "auc": 0.676}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 299, "auc": 0.6923076923076923}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6541353383458647, "accuracy_n": 399, "auc": 0.6541353383458647}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9503311258278145, "accuracy_n": 302, "auc": 0.9503311258278145}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9837812789620018, "accuracy_n": 322, "auc": 0.9837812789620018}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9233552631578948, "accuracy_n": 292, "auc": 0.9233552631578948}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7350528573956121, "accuracy_n": 1902, "auc": 0.7350528573956121}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7018743766499707, "accuracy_n": 2000, "auc": 0.7018743766499707}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8795518207282913, "accuracy_n": 59, "auc": 0.8795518207282913}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9923076923076923, "accuracy_n": 23, "auc": 0.9923076923076923}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9366834170854271, "accuracy_n": 995, "auc": 0.9366834170854271}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9356136820925554, "accuracy_n": 994, "auc": 0.9356136820925554}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8339350180505415, "accuracy_n": 277, "auc": 0.8339350180505415}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7558022199798183, "accuracy_n": 991, "auc": 0.7558022199798183}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5731462925851704, "accuracy_n": 499, "auc": 0.5731462925851704}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.576, "accuracy_n": 500, "auc": 0.576}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5150501672240803, "accuracy_n": 299, "auc": 0.5150501672240803}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7092731829573935, "accuracy_n": 399, "auc": 0.7092731829573935}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9304635761589404, "accuracy_n": 302, "auc": 0.9304635761589404}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9711538461538461, "accuracy_n": 322, "auc": 0.9711538461538461}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9192199248120301, "accuracy_n": 292, "auc": 0.9192199248120301}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6799075548478415, "accuracy_n": 1902, "auc": 0.6799075548478415}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6876972587103944, "accuracy_n": 2000, "auc": 0.6876972587103944}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7044817927170869, "accuracy_n": 59, "auc": 0.7044817927170869}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9346733668341709, "accuracy_n": 995, "auc": 0.9346733668341709}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9305835010060363, "accuracy_n": 994, "auc": 0.9305835010060363}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8736462093862816, "accuracy_n": 277, "auc": 0.8736462093862816}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8244197780020182, "accuracy_n": 991, "auc": 0.8244197780020182}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5230460921843687, "accuracy_n": 499, "auc": 0.5230460921843687}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 500, "auc": 0.67}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5451505016722408, "accuracy_n": 299, "auc": 0.5451505016722408}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7218045112781954, "accuracy_n": 399, "auc": 0.7218045112781954}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7119205298013245, "accuracy_n": 302, "auc": 0.7119205298013245}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6499073215940685, "accuracy_n": 322, "auc": 0.6499073215940685}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7234022556390978, "accuracy_n": 292, "auc": 0.7234022556390978}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8929440389294404, "accuracy_n": 411, "auc": 0.8929440389294404}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5174761146496816, "accuracy_n": 1902, "auc": 0.5174761146496816}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5250285353012438, "accuracy_n": 2000, "auc": 0.5250285353012438}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6834733893557423, "accuracy_n": 59, "auc": 0.6834733893557423}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9195979899497487, "accuracy_n": 995, "auc": 0.9195979899497487}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8592057761732852, "accuracy_n": 277, "auc": 0.8592057761732852}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8617558022199798, "accuracy_n": 991, "auc": 0.8617558022199798}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.672, "accuracy_n": 500, "auc": 0.672}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.731829573934837, "accuracy_n": 399, "auc": 0.731829573934837}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9892261353104727, "accuracy_n": 322, "auc": 0.9892261353104727}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8767857142857143, "accuracy_n": 292, "auc": 0.8767857142857143}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.627650610403397, "accuracy_n": 1902, "auc": 0.627650610403397}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6902521810373544, "accuracy_n": 2000, "auc": 0.6902521810373544}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8473389355742296, "accuracy_n": 59, "auc": 0.8473389355742296}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9692307692307691, "accuracy_n": 23, "auc": 0.9692307692307691}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9276381909547738, "accuracy_n": 995, "auc": 0.9276381909547738}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7132796780684104, "accuracy_n": 994, "auc": 0.7132796780684104}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7725631768953068, "accuracy_n": 277, "auc": 0.7725631768953068}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6609485368314834, "accuracy_n": 991, "auc": 0.6609485368314834}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.751503006012024, "accuracy_n": 499, "auc": 0.751503006012024}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8494983277591973, "accuracy_n": 299, "auc": 0.8494983277591973}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8295739348370927, "accuracy_n": 399, "auc": 0.8295739348370927}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7582781456953642, "accuracy_n": 302, "auc": 0.7582781456953642}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9679101019462465, "accuracy_n": 322, "auc": 0.9679101019462465}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9612312030075187, "accuracy_n": 292, "auc": 0.9612312030075187}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5592201875442322, "accuracy_n": 1902, "auc": 0.5592201875442322}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6857475548673071, "accuracy_n": 2000, "auc": 0.6857475548673071}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7408963585434174, "accuracy_n": 59, "auc": 0.7408963585434174}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.914572864321608, "accuracy_n": 995, "auc": 0.914572864321608}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7494969818913481, "accuracy_n": 994, "auc": 0.7494969818913481}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5848375451263538, "accuracy_n": 277, "auc": 0.5848375451263538}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7204843592330978, "accuracy_n": 991, "auc": 0.7204843592330978}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7394789579158316, "accuracy_n": 499, "auc": 0.7394789579158316}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.782, "accuracy_n": 500, "auc": 0.782}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 299, "auc": 0.8461538461538461}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8145363408521303, "accuracy_n": 399, "auc": 0.8145363408521303}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9503311258278145, "accuracy_n": 302, "auc": 0.9503311258278145}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9888399752857585, "accuracy_n": 322, "auc": 0.9888399752857585}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9610432330827068, "accuracy_n": 292, "auc": 0.9610432330827068}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5806816171266808, "accuracy_n": 1902, "auc": 0.5806816171266808}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6930091763126488, "accuracy_n": 2000, "auc": 0.6930091763126488}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8851540616246498, "accuracy_n": 59, "auc": 0.8851540616246498}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8804020100502512, "accuracy_n": 995, "auc": 0.8804020100502512}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7867203219315896, "accuracy_n": 994, "auc": 0.7867203219315896}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6389891696750902, "accuracy_n": 277, "auc": 0.6389891696750902}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7043390514631686, "accuracy_n": 991, "auc": 0.7043390514631686}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7394789579158316, "accuracy_n": 499, "auc": 0.7394789579158316}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 299, "auc": 0.8461538461538461}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8170426065162907, "accuracy_n": 399, "auc": 0.8170426065162907}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.956953642384106, "accuracy_n": 302, "auc": 0.956953642384106}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9625424776027185, "accuracy_n": 322, "auc": 0.9625424776027185}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9563909774436091, "accuracy_n": 292, "auc": 0.9563909774436091}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5730714791224345, "accuracy_n": 1902, "auc": 0.5730714791224345}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.668032159609619, "accuracy_n": 2000, "auc": 0.668032159609619}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8921568627450981, "accuracy_n": 59, "auc": 0.8921568627450981}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9417085427135679, "accuracy_n": 995, "auc": 0.9417085427135679}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7173038229376257, "accuracy_n": 994, "auc": 0.7173038229376257}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7588294651866802, "accuracy_n": 991, "auc": 0.7588294651866802}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.749498997995992, "accuracy_n": 499, "auc": 0.749498997995992}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.776, "accuracy_n": 500, "auc": 0.776}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.842809364548495, "accuracy_n": 299, "auc": 0.842809364548495}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8270676691729323, "accuracy_n": 399, "auc": 0.8270676691729323}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8708609271523179, "accuracy_n": 302, "auc": 0.8708609271523179}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9158943466172382, "accuracy_n": 322, "auc": 0.9158943466172382}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.968327067669173, "accuracy_n": 292, "auc": 0.968327067669173}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5640978857041754, "accuracy_n": 1902, "auc": 0.5640978857041754}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6474687362137732, "accuracy_n": 2000, "auc": 0.6474687362137732}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.623249299719888, "accuracy_n": 59, "auc": 0.623249299719888}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9216080402010051, "accuracy_n": 995, "auc": 0.9216080402010051}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8672032193158954, "accuracy_n": 994, "auc": 0.8672032193158954}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6714801444043321, "accuracy_n": 277, "auc": 0.6714801444043321}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8456104944500504, "accuracy_n": 991, "auc": 0.8456104944500504}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4649298597194389, "accuracy_n": 499, "auc": 0.4649298597194389}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.658, "accuracy_n": 500, "auc": 0.658}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.725752508361204, "accuracy_n": 299, "auc": 0.725752508361204}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5162907268170426, "accuracy_n": 399, "auc": 0.5162907268170426}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9851328390485017, "accuracy_n": 322, "auc": 0.9851328390485017}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6862312030075189, "accuracy_n": 292, "auc": 0.6862312030075189}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6540400521939137, "accuracy_n": 1902, "auc": 0.6540400521939137}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6884855432811244, "accuracy_n": 2000, "auc": 0.6884855432811244}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9873949579831932, "accuracy_n": 59, "auc": 0.9873949579831932}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9065326633165829, "accuracy_n": 995, "auc": 0.9065326633165829}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7736418511066399, "accuracy_n": 994, "auc": 0.7736418511066399}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8700361010830325, "accuracy_n": 277, "auc": 0.8700361010830325}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8284561049445005, "accuracy_n": 991, "auc": 0.8284561049445005}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56312625250501, "accuracy_n": 499, "auc": 0.56312625250501}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.632, "accuracy_n": 500, "auc": 0.632}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6688963210702341, "accuracy_n": 299, "auc": 0.6688963210702341}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6967418546365914, "accuracy_n": 399, "auc": 0.6967418546365914}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9966404077849861, "accuracy_n": 322, "auc": 0.9966404077849861}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5615131578947368, "accuracy_n": 292, "auc": 0.5615131578947368}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6551618895966029, "accuracy_n": 1902, "auc": 0.6551618895966029}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7395729858478912, "accuracy_n": 2000, "auc": 0.7395729858478912}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9551820728291316, "accuracy_n": 59, "auc": 0.9551820728291316}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.91356783919598, "accuracy_n": 995, "auc": 0.91356783919598}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8319919517102615, "accuracy_n": 994, "auc": 0.8319919517102615}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8158844765342961, "accuracy_n": 277, "auc": 0.8158844765342961}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8042381432896064, "accuracy_n": 991, "auc": 0.8042381432896064}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5470941883767535, "accuracy_n": 499, "auc": 0.5470941883767535}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.698, "accuracy_n": 500, "auc": 0.698}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7859531772575251, "accuracy_n": 299, "auc": 0.7859531772575251}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6190476190476191, "accuracy_n": 399, "auc": 0.6190476190476191}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9915817114612295, "accuracy_n": 322, "auc": 0.9915817114612295}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7342575187969925, "accuracy_n": 292, "auc": 0.7342575187969925}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6546156227883935, "accuracy_n": 1902, "auc": 0.6546156227883935}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6952659910227592, "accuracy_n": 2000, "auc": 0.6952659910227592}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9873949579831932, "accuracy_n": 59, "auc": 0.9873949579831932}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8615384615384616, "accuracy_n": 23, "auc": 0.8615384615384616}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9386934673366835, "accuracy_n": 995, "auc": 0.9386934673366835}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.937625754527163, "accuracy_n": 994, "auc": 0.937625754527163}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8194945848375451, "accuracy_n": 277, "auc": 0.8194945848375451}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8183652875882946, "accuracy_n": 991, "auc": 0.8183652875882946}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7314629258517034, "accuracy_n": 499, "auc": 0.7314629258517034}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 500, "auc": 0.77}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8260869565217391, "accuracy_n": 299, "auc": 0.8260869565217391}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8020050125313283, "accuracy_n": 399, "auc": 0.8020050125313283}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9503311258278145, "accuracy_n": 302, "auc": 0.9503311258278145}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8767763361136856, "accuracy_n": 322, "auc": 0.8767763361136856}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9705357142857143, "accuracy_n": 292, "auc": 0.9705357142857143}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5827793259023354, "accuracy_n": 1902, "auc": 0.5827793259023354}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6633194583244552, "accuracy_n": 2000, "auc": 0.6633194583244552}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6554621848739496, "accuracy_n": 59, "auc": 0.6554621848739496}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9447236180904522, "accuracy_n": 995, "auc": 0.9447236180904522}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9185110663983903, "accuracy_n": 994, "auc": 0.9185110663983903}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.855595667870036, "accuracy_n": 277, "auc": 0.855595667870036}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7870837537840565, "accuracy_n": 991, "auc": 0.7870837537840565}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7154308617234469, "accuracy_n": 499, "auc": 0.7154308617234469}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8193979933110368, "accuracy_n": 299, "auc": 0.8193979933110368}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8145363408521303, "accuracy_n": 399, "auc": 0.8145363408521303}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.989805375347544, "accuracy_n": 322, "auc": 0.989805375347544}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8161184210526315, "accuracy_n": 292, "auc": 0.8161184210526315}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6041434447983015, "accuracy_n": 1902, "auc": 0.6041434447983015}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7546964454167956, "accuracy_n": 2000, "auc": 0.7546964454167956}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.984593837535014, "accuracy_n": 59, "auc": 0.984593837535014}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7457286432160805, "accuracy_n": 995, "auc": 0.7457286432160805}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7364185110663984, "accuracy_n": 994, "auc": 0.7364185110663984}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6570397111913358, "accuracy_n": 277, "auc": 0.6570397111913358}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5933400605449042, "accuracy_n": 991, "auc": 0.5933400605449042}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3286573146292585, "accuracy_n": 499, "auc": 0.3286573146292585}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.346, "accuracy_n": 500, "auc": 0.346}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.39464882943143814, "accuracy_n": 299, "auc": 0.39464882943143814}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.37844611528822053, "accuracy_n": 399, "auc": 0.37844611528822053}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9238410596026491, "accuracy_n": 302, "auc": 0.9238410596026491}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8102409638554217, "accuracy_n": 322, "auc": 0.8102409638554217}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6912593984962406, "accuracy_n": 292, "auc": 0.6912593984962406}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5961070559610706, "accuracy_n": 411, "auc": 0.5961070559610706}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7786237172682235, "accuracy_n": 1902, "auc": 0.7786237172682235}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5961011925305035, "accuracy_n": 2000, "auc": 0.5961011925305035}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8235294117647058, "accuracy_n": 59, "auc": 0.8235294117647058}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9175050301810865, "accuracy_n": 994, "auc": 0.9175050301810865}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8339350180505415, "accuracy_n": 277, "auc": 0.8339350180505415}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7961654894046418, "accuracy_n": 991, "auc": 0.7961654894046418}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5551102204408818, "accuracy_n": 499, "auc": 0.5551102204408818}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.702, "accuracy_n": 500, "auc": 0.702}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 299, "auc": 0.7692307692307693}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.568922305764411, "accuracy_n": 399, "auc": 0.568922305764411}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9885696632684585, "accuracy_n": 322, "auc": 0.9885696632684585}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7544172932330827, "accuracy_n": 292, "auc": 0.7544172932330827}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6607904281670205, "accuracy_n": 1902, "auc": 0.6607904281670205}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7269254200766477, "accuracy_n": 2000, "auc": 0.7269254200766477}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9761904761904762, "accuracy_n": 59, "auc": 0.9761904761904762}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9115577889447236, "accuracy_n": 995, "auc": 0.9115577889447236}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9154929577464789, "accuracy_n": 994, "auc": 0.9154929577464789}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7292418772563177, "accuracy_n": 277, "auc": 0.7292418772563177}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.768920282542886, "accuracy_n": 991, "auc": 0.768920282542886}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.38276553106212424, "accuracy_n": 499, "auc": 0.38276553106212424}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.588, "accuracy_n": 500, "auc": 0.588}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3258145363408521, "accuracy_n": 399, "auc": 0.3258145363408521}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9870250231696015, "accuracy_n": 322, "auc": 0.9870250231696015}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6961936090225563, "accuracy_n": 292, "auc": 0.6961936090225563}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6386931617126681, "accuracy_n": 1902, "auc": 0.6386931617126681}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6732300360430116, "accuracy_n": 2000, "auc": 0.6732300360430116}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9817927170868347, "accuracy_n": 59, "auc": 0.9817927170868347}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9195979899497487, "accuracy_n": 995, "auc": 0.9195979899497487}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9255533199195171, "accuracy_n": 994, "auc": 0.9255533199195171}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8592057761732852, "accuracy_n": 277, "auc": 0.8592057761732852}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6811301715438951, "accuracy_n": 991, "auc": 0.6811301715438951}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5190380761523046, "accuracy_n": 499, "auc": 0.5190380761523046}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.718, "accuracy_n": 500, "auc": 0.718}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7491638795986622, "accuracy_n": 299, "auc": 0.7491638795986622}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6240601503759399, "accuracy_n": 399, "auc": 0.6240601503759399}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9823911028730307, "accuracy_n": 322, "auc": 0.9823911028730307}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9180451127819549, "accuracy_n": 292, "auc": 0.9180451127819549}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9805352798053528, "accuracy_n": 411, "auc": 0.9805352798053528}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5720972664543523, "accuracy_n": 1902, "auc": 0.5720972664543523}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6541261395363727, "accuracy_n": 2000, "auc": 0.6541261395363727}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9607843137254901, "accuracy_n": 59, "auc": 0.9607843137254901}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
