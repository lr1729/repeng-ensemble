{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9376884422110553, "accuracy_n": 995, "auc": 0.9376884422110553}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9416498993963782, "accuracy_n": 994, "auc": 0.9416498993963782}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5848375451263538, "accuracy_n": 277, "auc": 0.5848375451263538}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6165489404641776, "accuracy_n": 991, "auc": 0.6165489404641776}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6352705410821643, "accuracy_n": 499, "auc": 0.6352705410821643}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.732, "accuracy_n": 500, "auc": 0.732}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7658862876254181, "accuracy_n": 299, "auc": 0.7658862876254181}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9717717021933889, "accuracy_n": 322, "auc": 0.9717717021933889}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8906015037593985, "accuracy_n": 292, "auc": 0.8906015037593985}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6361276981599434, "accuracy_n": 1902, "auc": 0.6361276981599434}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6992254203767561, "accuracy_n": 2000, "auc": 0.6992254203767561}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.865546218487395, "accuracy_n": 59, "auc": 0.865546218487395}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9230769230769231, "accuracy_n": 23, "auc": 0.9230769230769231}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9366834170854271, "accuracy_n": 995, "auc": 0.9366834170854271}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9436619718309859, "accuracy_n": 994, "auc": 0.9436619718309859}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5993945509586277, "accuracy_n": 991, "auc": 0.5993945509586277}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5170340681362725, "accuracy_n": 499, "auc": 0.5170340681362725}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.436, "accuracy_n": 500, "auc": 0.436}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4782608695652174, "accuracy_n": 299, "auc": 0.4782608695652174}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5739348370927319, "accuracy_n": 399, "auc": 0.5739348370927319}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9238410596026491, "accuracy_n": 302, "auc": 0.9238410596026491}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9830089589125734, "accuracy_n": 322, "auc": 0.9830089589125734}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9037593984962405, "accuracy_n": 292, "auc": 0.9037593984962405}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6386024858457183, "accuracy_n": 1902, "auc": 0.6386024858457183}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7081056261310334, "accuracy_n": 2000, "auc": 0.7081056261310334}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8557422969187675, "accuracy_n": 59, "auc": 0.8557422969187675}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.976923076923077, "accuracy_n": 23, "auc": 0.976923076923077}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9366834170854271, "accuracy_n": 995, "auc": 0.9366834170854271}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8447653429602888, "accuracy_n": 277, "auc": 0.8447653429602888}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6559031281533805, "accuracy_n": 991, "auc": 0.6559031281533805}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4729458917835671, "accuracy_n": 499, "auc": 0.4729458917835671}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.332, "accuracy_n": 500, "auc": 0.332}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3177257525083612, "accuracy_n": 299, "auc": 0.3177257525083612}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6190476190476191, "accuracy_n": 399, "auc": 0.6190476190476191}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9701986754966887, "accuracy_n": 302, "auc": 0.9701986754966887}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.981425702811245, "accuracy_n": 322, "auc": 0.981425702811245}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8881578947368421, "accuracy_n": 292, "auc": 0.8881578947368421}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7087126238499646, "accuracy_n": 1902, "auc": 0.7087126238499646}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6792322028252199, "accuracy_n": 2000, "auc": 0.6792322028252199}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6554621848739495, "accuracy_n": 59, "auc": 0.6554621848739495}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.935678391959799, "accuracy_n": 995, "auc": 0.935678391959799}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9094567404426559, "accuracy_n": 994, "auc": 0.9094567404426559}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8158844765342961, "accuracy_n": 277, "auc": 0.8158844765342961}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7769929364278506, "accuracy_n": 991, "auc": 0.7769929364278506}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.40080160320641284, "accuracy_n": 499, "auc": 0.40080160320641284}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.376, "accuracy_n": 500, "auc": 0.376}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 399, "auc": 0.5714285714285714}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8741721854304636, "accuracy_n": 302, "auc": 0.8741721854304636}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5871177015755329, "accuracy_n": 322, "auc": 0.5871177015755329}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5095864661654135, "accuracy_n": 292, "auc": 0.5095864661654135}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.656934306569343, "accuracy_n": 411, "auc": 0.656934306569343}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5083056882519462, "accuracy_n": 1902, "auc": 0.5083056882519462}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5434997033929249, "accuracy_n": 2000, "auc": 0.5434997033929249}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7338935574229692, "accuracy_n": 59, "auc": 0.7338935574229692}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8700361010830325, "accuracy_n": 277, "auc": 0.8700361010830325}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8748738647830474, "accuracy_n": 991, "auc": 0.8748738647830474}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.40681362725450904, "accuracy_n": 499, "auc": 0.40681362725450904}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.414, "accuracy_n": 500, "auc": 0.414}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38127090301003347, "accuracy_n": 299, "auc": 0.38127090301003347}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.40100250626566414, "accuracy_n": 399, "auc": 0.40100250626566414}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9668874172185431, "accuracy_n": 302, "auc": 0.9668874172185431}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9864071671300587, "accuracy_n": 322, "auc": 0.9864071671300587}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8533834586466166, "accuracy_n": 292, "auc": 0.8533834586466166}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6409600583864119, "accuracy_n": 1902, "auc": 0.6409600583864119}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6330465297972567, "accuracy_n": 2000, "auc": 0.6330465297972567}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.84593837535014, "accuracy_n": 59, "auc": 0.84593837535014}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 23, "auc": 0.9}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8773869346733668, "accuracy_n": 995, "auc": 0.8773869346733668}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5667870036101083, "accuracy_n": 277, "auc": 0.5667870036101083}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6680121089808274, "accuracy_n": 991, "auc": 0.6680121089808274}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7635270541082164, "accuracy_n": 499, "auc": 0.7635270541082164}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8494983277591973, "accuracy_n": 299, "auc": 0.8494983277591973}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8195488721804511, "accuracy_n": 399, "auc": 0.8195488721804511}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8675496688741722, "accuracy_n": 302, "auc": 0.8675496688741722}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9691844300278036, "accuracy_n": 322, "auc": 0.9691844300278036}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.943656015037594, "accuracy_n": 292, "auc": 0.943656015037594}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5016598106864827, "accuracy_n": 1902, "auc": 0.5016598106864827}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6794652869685956, "accuracy_n": 2000, "auc": 0.6794652869685956}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7464985994397759, "accuracy_n": 59, "auc": 0.7464985994397759}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8201005025125628, "accuracy_n": 995, "auc": 0.8201005025125628}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5691220988900101, "accuracy_n": 991, "auc": 0.5691220988900101}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7535070140280561, "accuracy_n": 499, "auc": 0.7535070140280561}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.782, "accuracy_n": 500, "auc": 0.782}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8494983277591973, "accuracy_n": 299, "auc": 0.8494983277591973}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8145363408521303, "accuracy_n": 399, "auc": 0.8145363408521303}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9602649006622517, "accuracy_n": 302, "auc": 0.9602649006622517}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.907051282051282, "accuracy_n": 322, "auc": 0.907051282051282}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9139567669172933, "accuracy_n": 292, "auc": 0.9139567669172933}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5200990799716914, "accuracy_n": 1902, "auc": 0.5200990799716914}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6476838138568023, "accuracy_n": 2000, "auc": 0.6476838138568023}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.784313725490196, "accuracy_n": 59, "auc": 0.784313725490196}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6763819095477387, "accuracy_n": 995, "auc": 0.6763819095477387}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6539235412474849, "accuracy_n": 994, "auc": 0.6539235412474849}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5398587285570131, "accuracy_n": 991, "auc": 0.5398587285570131}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7254509018036072, "accuracy_n": 499, "auc": 0.7254509018036072}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8528428093645485, "accuracy_n": 299, "auc": 0.8528428093645485}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8220551378446115, "accuracy_n": 399, "auc": 0.8220551378446115}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9701986754966887, "accuracy_n": 302, "auc": 0.9701986754966887}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9053521779425394, "accuracy_n": 322, "auc": 0.9053521779425394}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.894266917293233, "accuracy_n": 292, "auc": 0.894266917293233}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.975669099756691, "accuracy_n": 411, "auc": 0.975669099756691}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5125409147204529, "accuracy_n": 1902, "auc": 0.5125409147204529}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6326123730666771, "accuracy_n": 2000, "auc": 0.6326123730666771}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8291316526610644, "accuracy_n": 59, "auc": 0.8291316526610644}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8733668341708543, "accuracy_n": 995, "auc": 0.8733668341708543}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9094567404426559, "accuracy_n": 994, "auc": 0.9094567404426559}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.628158844765343, "accuracy_n": 277, "auc": 0.628158844765343}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5015136226034309, "accuracy_n": 991, "auc": 0.5015136226034309}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7454909819639278, "accuracy_n": 499, "auc": 0.7454909819639278}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.778, "accuracy_n": 500, "auc": 0.778}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8528428093645485, "accuracy_n": 299, "auc": 0.8528428093645485}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8295739348370927, "accuracy_n": 399, "auc": 0.8295739348370927}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9139072847682119, "accuracy_n": 302, "auc": 0.9139072847682119}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9540469570590052, "accuracy_n": 322, "auc": 0.9540469570590052}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9411654135338346, "accuracy_n": 292, "auc": 0.9411654135338346}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5124966825902335, "accuracy_n": 1902, "auc": 0.5124966825902335}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6617734001974713, "accuracy_n": 2000, "auc": 0.6617734001974713}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6414565826330532, "accuracy_n": 59, "auc": 0.6414565826330532}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8221105527638191, "accuracy_n": 995, "auc": 0.8221105527638191}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8822937625754527, "accuracy_n": 994, "auc": 0.8822937625754527}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6931407942238267, "accuracy_n": 277, "auc": 0.6931407942238267}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7769929364278506, "accuracy_n": 991, "auc": 0.7769929364278506}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38877755511022044, "accuracy_n": 499, "auc": 0.38877755511022044}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.536, "accuracy_n": 500, "auc": 0.536}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6889632107023411, "accuracy_n": 299, "auc": 0.6889632107023411}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.40601503759398494, "accuracy_n": 399, "auc": 0.40601503759398494}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9704587581093606, "accuracy_n": 322, "auc": 0.9704587581093606}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5490601503759398, "accuracy_n": 292, "auc": 0.5490601503759398}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5412685774946921, "accuracy_n": 1902, "auc": 0.5412685774946921}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6703379920151175, "accuracy_n": 2000, "auc": 0.6703379920151175}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9551820728291317, "accuracy_n": 59, "auc": 0.9551820728291317}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9095477386934674, "accuracy_n": 995, "auc": 0.9095477386934674}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8621730382293763, "accuracy_n": 994, "auc": 0.8621730382293763}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8628158844765343, "accuracy_n": 277, "auc": 0.8628158844765343}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7729566094853683, "accuracy_n": 991, "auc": 0.7729566094853683}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4529058116232465, "accuracy_n": 499, "auc": 0.4529058116232465}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 500, "auc": 0.54}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68561872909699, "accuracy_n": 299, "auc": 0.68561872909699}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7167919799498746, "accuracy_n": 399, "auc": 0.7167919799498746}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9986870559159716, "accuracy_n": 322, "auc": 0.9986870559159716}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5918233082706766, "accuracy_n": 292, "auc": 0.5918233082706766}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5871771054493984, "accuracy_n": 1902, "auc": 0.5871771054493984}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7235391976503518, "accuracy_n": 2000, "auc": 0.7235391976503518}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9621848739495797, "accuracy_n": 59, "auc": 0.9621848739495797}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9015075376884422, "accuracy_n": 995, "auc": 0.9015075376884422}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8742454728370221, "accuracy_n": 994, "auc": 0.8742454728370221}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8194945848375451, "accuracy_n": 277, "auc": 0.8194945848375451}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7053481331987891, "accuracy_n": 991, "auc": 0.7053481331987891}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5130260521042084, "accuracy_n": 499, "auc": 0.5130260521042084}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 500, "auc": 0.67}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7892976588628763, "accuracy_n": 299, "auc": 0.7892976588628763}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 399, "auc": 0.6666666666666666}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9924312635156008, "accuracy_n": 322, "auc": 0.9924312635156008}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6649906015037594, "accuracy_n": 292, "auc": 0.6649906015037594}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5602065640481245, "accuracy_n": 1902, "auc": 0.5602065640481245}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6974547811760046, "accuracy_n": 2000, "auc": 0.6974547811760046}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.984593837535014, "accuracy_n": 59, "auc": 0.984593837535014}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9386934673366835, "accuracy_n": 995, "auc": 0.9386934673366835}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9496981891348089, "accuracy_n": 994, "auc": 0.9496981891348089}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.851985559566787, "accuracy_n": 277, "auc": 0.851985559566787}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5287588294651867, "accuracy_n": 991, "auc": 0.5287588294651867}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7394789579158316, "accuracy_n": 499, "auc": 0.7394789579158316}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.746, "accuracy_n": 500, "auc": 0.746}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.822742474916388, "accuracy_n": 299, "auc": 0.822742474916388}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8145363408521303, "accuracy_n": 399, "auc": 0.8145363408521303}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7019867549668874, "accuracy_n": 302, "auc": 0.7019867549668874}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7997374111831943, "accuracy_n": 322, "auc": 0.7997374111831943}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9609492481203007, "accuracy_n": 292, "auc": 0.9609492481203007}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5137738853503184, "accuracy_n": 1902, "auc": 0.5137738853503184}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5784408171349857, "accuracy_n": 2000, "auc": 0.5784408171349857}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5392156862745098, "accuracy_n": 59, "auc": 0.5392156862745098}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8923076923076924, "accuracy_n": 23, "auc": 0.8923076923076924}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9316582914572864, "accuracy_n": 995, "auc": 0.9316582914572864}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9215291750503019, "accuracy_n": 994, "auc": 0.9215291750503019}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8194945848375451, "accuracy_n": 277, "auc": 0.8194945848375451}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7325933400605449, "accuracy_n": 991, "auc": 0.7325933400605449}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6533066132264529, "accuracy_n": 499, "auc": 0.6533066132264529}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.766, "accuracy_n": 500, "auc": 0.766}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8294314381270903, "accuracy_n": 299, "auc": 0.8294314381270903}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8070175438596491, "accuracy_n": 399, "auc": 0.8070175438596491}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9911183194315724, "accuracy_n": 322, "auc": 0.9911183194315724}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7711936090225564, "accuracy_n": 292, "auc": 0.7711936090225564}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5485436571125265, "accuracy_n": 1902, "auc": 0.5485436571125265}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7563560445320762, "accuracy_n": 2000, "auc": 0.7563560445320762}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9747899159663864, "accuracy_n": 59, "auc": 0.9747899159663864}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9105527638190954, "accuracy_n": 995, "auc": 0.9105527638190954}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6780684104627767, "accuracy_n": 994, "auc": 0.6780684104627767}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.776173285198556, "accuracy_n": 277, "auc": 0.776173285198556}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6337033299697276, "accuracy_n": 991, "auc": 0.6337033299697276}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3026052104208417, "accuracy_n": 499, "auc": 0.3026052104208417}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.376, "accuracy_n": 500, "auc": 0.376}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.46488294314381273, "accuracy_n": 299, "auc": 0.46488294314381273}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2606516290726817, "accuracy_n": 399, "auc": 0.2606516290726817}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9602649006622517, "accuracy_n": 302, "auc": 0.9602649006622517}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8459993821439604, "accuracy_n": 322, "auc": 0.8459993821439604}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5514332706766917, "accuracy_n": 292, "auc": 0.5514332706766917}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6545012165450121, "accuracy_n": 411, "auc": 0.6545012165450121}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7385239738145789, "accuracy_n": 1902, "auc": 0.7385239738145789}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6037274456078644, "accuracy_n": 2000, "auc": 0.6037274456078644}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7773109243697478, "accuracy_n": 59, "auc": 0.7773109243697478}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8542713567839196, "accuracy_n": 995, "auc": 0.8542713567839196}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9295774647887324, "accuracy_n": 994, "auc": 0.9295774647887324}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8339350180505415, "accuracy_n": 277, "auc": 0.8339350180505415}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7497477295660948, "accuracy_n": 991, "auc": 0.7497477295660948}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.45691382765531063, "accuracy_n": 499, "auc": 0.45691382765531063}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.622, "accuracy_n": 500, "auc": 0.622}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7357859531772575, "accuracy_n": 299, "auc": 0.7357859531772575}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5213032581453634, "accuracy_n": 399, "auc": 0.5213032581453634}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9834437086092715, "accuracy_n": 302, "auc": 0.9834437086092715}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9864071671300587, "accuracy_n": 322, "auc": 0.9864071671300587}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6002349624060151, "accuracy_n": 292, "auc": 0.6002349624060151}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5603226733899505, "accuracy_n": 1902, "auc": 0.5603226733899505}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7110036723257096, "accuracy_n": 2000, "auc": 0.7110036723257096}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9635854341736694, "accuracy_n": 59, "auc": 0.9635854341736694}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8562814070351759, "accuracy_n": 995, "auc": 0.8562814070351759}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8050541516245487, "accuracy_n": 277, "auc": 0.8050541516245487}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7426841574167508, "accuracy_n": 991, "auc": 0.7426841574167508}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3627254509018036, "accuracy_n": 499, "auc": 0.3627254509018036}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 500, "auc": 0.51}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6655518394648829, "accuracy_n": 299, "auc": 0.6655518394648829}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3032581453634085, "accuracy_n": 399, "auc": 0.3032581453634085}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9842446709916589, "accuracy_n": 322, "auc": 0.9842446709916589}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5437969924812031, "accuracy_n": 292, "auc": 0.5437969924812031}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 411, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5406316348195329, "accuracy_n": 1902, "auc": 0.5406316348195329}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6635445395787879, "accuracy_n": 2000, "auc": 0.6635445395787879}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9663865546218487, "accuracy_n": 59, "auc": 0.9663865546218487}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.91356783919598, "accuracy_n": 995, "auc": 0.91356783919598}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9416498993963782, "accuracy_n": 994, "auc": 0.9416498993963782}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7111913357400722, "accuracy_n": 277, "auc": 0.7111913357400722}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7214934409687185, "accuracy_n": 991, "auc": 0.7214934409687185}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3667334669338677, "accuracy_n": 499, "auc": 0.3667334669338677}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 500, "auc": 0.64}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.725752508361204, "accuracy_n": 299, "auc": 0.725752508361204}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5162907268170426, "accuracy_n": 399, "auc": 0.5162907268170426}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867549668874173, "accuracy_n": 302, "auc": 0.9867549668874173}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9703042940994748, "accuracy_n": 322, "auc": 0.9703042940994748}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8637687969924813, "accuracy_n": 292, "auc": 0.8637687969924813}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.975669099756691, "accuracy_n": 411, "auc": 0.975669099756691}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5278463375796179, "accuracy_n": 1902, "auc": 0.5278463375796179}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6290420841923934, "accuracy_n": 2000, "auc": 0.6290420841923934}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9145658263305322, "accuracy_n": 59, "auc": 0.9145658263305322}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
