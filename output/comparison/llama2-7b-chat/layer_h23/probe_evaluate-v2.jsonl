{"key": "result_0", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9133937562940584, "accuracy_n": 993, "auc": 0.9133937562940584}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9195171026156942, "accuracy_n": 994, "auc": 0.9195171026156942}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6340579710144928, "accuracy_n": 276, "auc": 0.6340579710144928}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5615615615615616, "accuracy_n": 999, "auc": 0.5615615615615616}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49, "accuracy_n": 500, "auc": 0.49}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6312625250501002, "accuracy_n": 499, "auc": 0.6312625250501002}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5050167224080268, "accuracy_n": 299, "auc": 0.5050167224080268}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.595, "accuracy_n": 400, "auc": 0.595}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6490066225165563, "accuracy_n": 302, "auc": 0.6490066225165563}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5837967253629904, "accuracy_n": 322, "auc": 0.5837967253629904}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5822133458646617, "accuracy_n": 292, "auc": 0.5822133458646617}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461165048543689, "accuracy_n": 412, "auc": 0.5461165048543689}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6464326786978061, "accuracy_n": 1902, "auc": 0.6464326786978061}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5138935155591169, "accuracy_n": 2000, "auc": 0.5138935155591169}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5840336134453782, "accuracy_n": 59, "auc": 0.5840336134453782}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9405840886203424, "accuracy_n": 993, "auc": 0.9405840886203424}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9195171026156942, "accuracy_n": 994, "auc": 0.9195171026156942}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6847826086956522, "accuracy_n": 276, "auc": 0.6847826086956522}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5705705705705706, "accuracy_n": 999, "auc": 0.5705705705705706}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.514, "accuracy_n": 500, "auc": 0.514}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.43812709030100333, "accuracy_n": 299, "auc": 0.43812709030100333}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5525, "accuracy_n": 400, "auc": 0.5525}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6556291390728477, "accuracy_n": 302, "auc": 0.6556291390728477}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5605112758727216, "accuracy_n": 322, "auc": 0.5605112758727216}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5309680451127818, "accuracy_n": 292, "auc": 0.5309680451127818}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5558252427184466, "accuracy_n": 412, "auc": 0.5558252427184466}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6796819709837226, "accuracy_n": 1902, "auc": 0.6796819709837226}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5203833583923796, "accuracy_n": 2000, "auc": 0.5203833583923796}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5672268907563025, "accuracy_n": 59, "auc": 0.5672268907563025}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9538461538461538, "accuracy_n": 23, "auc": 0.9538461538461538}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9425981873111783, "accuracy_n": 993, "auc": 0.9425981873111783}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8410462776659959, "accuracy_n": 994, "auc": 0.8410462776659959}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7463768115942029, "accuracy_n": 276, "auc": 0.7463768115942029}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6156156156156156, "accuracy_n": 999, "auc": 0.6156156156156156}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.304, "accuracy_n": 500, "auc": 0.304}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48296593186372744, "accuracy_n": 499, "auc": 0.48296593186372744}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.33444816053511706, "accuracy_n": 299, "auc": 0.33444816053511706}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.23, "accuracy_n": 400, "auc": 0.23}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5993377483443708, "accuracy_n": 302, "auc": 0.5993377483443708}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5128591288229842, "accuracy_n": 322, "auc": 0.5128591288229842}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5644736842105262, "accuracy_n": 292, "auc": 0.5644736842105262}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5970873786407767, "accuracy_n": 412, "auc": 0.5970873786407767}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5969159147204528, "accuracy_n": 1902, "auc": 0.5969159147204528}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.518231581600958, "accuracy_n": 2000, "auc": 0.518231581600958}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9003021148036254, "accuracy_n": 993, "auc": 0.9003021148036254}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6790744466800804, "accuracy_n": 994, "auc": 0.6790744466800804}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5905797101449275, "accuracy_n": 276, "auc": 0.5905797101449275}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6086086086086087, "accuracy_n": 999, "auc": 0.6086086086086087}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.516, "accuracy_n": 500, "auc": 0.516}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.43478260869565216, "accuracy_n": 299, "auc": 0.43478260869565216}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6425, "accuracy_n": 400, "auc": 0.6425}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5288075378436824, "accuracy_n": 322, "auc": 0.5288075378436824}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5773966165413533, "accuracy_n": 292, "auc": 0.5773966165413533}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6213592233009708, "accuracy_n": 412, "auc": 0.6213592233009708}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5476523796886058, "accuracy_n": 1902, "auc": 0.5476523796886058}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5305850411998732, "accuracy_n": 2000, "auc": 0.5305850411998732}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5742296918767507, "accuracy_n": 59, "auc": 0.5742296918767507}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5166163141993958, "accuracy_n": 993, "auc": 0.5166163141993958}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5925553319919518, "accuracy_n": 994, "auc": 0.5925553319919518}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6186186186186187, "accuracy_n": 999, "auc": 0.6186186186186187}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.312, "accuracy_n": 500, "auc": 0.312}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3026052104208417, "accuracy_n": 499, "auc": 0.3026052104208417}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3277591973244147, "accuracy_n": 299, "auc": 0.3277591973244147}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3425, "accuracy_n": 400, "auc": 0.3425}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5695364238410596, "accuracy_n": 302, "auc": 0.5695364238410596}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5717871485943775, "accuracy_n": 322, "auc": 0.5717871485943775}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5161654135338345, "accuracy_n": 292, "auc": 0.5161654135338345}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6067961165048543, "accuracy_n": 412, "auc": 0.6067961165048543}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.547493144019816, "accuracy_n": 1902, "auc": 0.547493144019816}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5171226812879449, "accuracy_n": 2000, "auc": 0.5171226812879449}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6036414565826331, "accuracy_n": 59, "auc": 0.6036414565826331}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7935548841893253, "accuracy_n": 993, "auc": 0.7935548841893253}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6277665995975855, "accuracy_n": 994, "auc": 0.6277665995975855}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5985985985985987, "accuracy_n": 999, "auc": 0.5985985985985987}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.616, "accuracy_n": 500, "auc": 0.616}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6513026052104208, "accuracy_n": 499, "auc": 0.6513026052104208}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6086956521739131, "accuracy_n": 299, "auc": 0.6086956521739131}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5728476821192053, "accuracy_n": 302, "auc": 0.5728476821192053}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8593605189990732, "accuracy_n": 322, "auc": 0.8593605189990732}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7024436090225563, "accuracy_n": 292, "auc": 0.7024436090225563}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5436893203883495, "accuracy_n": 412, "auc": 0.5436893203883495}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5050944355980184, "accuracy_n": 1902, "auc": 0.5050944355980184}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5598541073327471, "accuracy_n": 2000, "auc": 0.5598541073327471}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6694677871148459, "accuracy_n": 59, "auc": 0.6694677871148459}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9063444108761329, "accuracy_n": 993, "auc": 0.9063444108761329}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6509054325955734, "accuracy_n": 994, "auc": 0.6509054325955734}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5072463768115942, "accuracy_n": 276, "auc": 0.5072463768115942}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6006006006006006, "accuracy_n": 999, "auc": 0.6006006006006006}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 500, "auc": 0.62}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6533066132264529, "accuracy_n": 499, "auc": 0.6533066132264529}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.655, "accuracy_n": 400, "auc": 0.655}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5562913907284768, "accuracy_n": 302, "auc": 0.5562913907284768}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6743126351560087, "accuracy_n": 322, "auc": 0.6743126351560087}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6251409774436091, "accuracy_n": 292, "auc": 0.6251409774436091}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5356234518754424, "accuracy_n": 1902, "auc": 0.5356234518754424}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5306195536588709, "accuracy_n": 2000, "auc": 0.5306195536588709}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6120448179271709, "accuracy_n": 59, "auc": 0.6120448179271709}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7915407854984894, "accuracy_n": 993, "auc": 0.7915407854984894}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5075075075075075, "accuracy_n": 999, "auc": 0.5075075075075075}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 500, "auc": 0.61}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5529801324503312, "accuracy_n": 302, "auc": 0.5529801324503312}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7737874575223973, "accuracy_n": 322, "auc": 0.7737874575223973}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.706156015037594, "accuracy_n": 292, "auc": 0.706156015037594}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5090388358103326, "accuracy_n": 1902, "auc": 0.5090388358103326}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5245248534721034, "accuracy_n": 2000, "auc": 0.5245248534721034}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8459214501510574, "accuracy_n": 993, "auc": 0.8459214501510574}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7464788732394366, "accuracy_n": 994, "auc": 0.7464788732394366}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5398550724637681, "accuracy_n": 276, "auc": 0.5398550724637681}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5475475475475475, "accuracy_n": 999, "auc": 0.5475475475475475}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.624, "accuracy_n": 500, "auc": 0.624}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6492985971943888, "accuracy_n": 499, "auc": 0.6492985971943888}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5953177257525084, "accuracy_n": 299, "auc": 0.5953177257525084}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6525, "accuracy_n": 400, "auc": 0.6525}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7098779734321903, "accuracy_n": 322, "auc": 0.7098779734321903}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5153195488721805, "accuracy_n": 292, "auc": 0.5153195488721805}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5606796116504854, "accuracy_n": 412, "auc": 0.5606796116504854}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5151273885350318, "accuracy_n": 1902, "auc": 0.5151273885350318}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5428634737140108, "accuracy_n": 2000, "auc": 0.5428634737140108}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5042016806722689, "accuracy_n": 59, "auc": 0.5042016806722689}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.837865055387714, "accuracy_n": 993, "auc": 0.837865055387714}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8008048289738431, "accuracy_n": 994, "auc": 0.8008048289738431}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6340579710144928, "accuracy_n": 276, "auc": 0.6340579710144928}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6186186186186187, "accuracy_n": 999, "auc": 0.6186186186186187}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.442, "accuracy_n": 500, "auc": 0.442}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4408817635270541, "accuracy_n": 499, "auc": 0.4408817635270541}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 400, "auc": 0.32}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7251655629139073, "accuracy_n": 302, "auc": 0.7251655629139073}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7227371022551745, "accuracy_n": 322, "auc": 0.7227371022551745}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5113251879699248, "accuracy_n": 292, "auc": 0.5113251879699248}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8883495145631068, "accuracy_n": 412, "auc": 0.8883495145631068}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5647005484784148, "accuracy_n": 1902, "auc": 0.5647005484784148}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5365131812584343, "accuracy_n": 2000, "auc": 0.5365131812584343}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5476190476190477, "accuracy_n": 59, "auc": 0.5476190476190477}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.918429003021148, "accuracy_n": 993, "auc": 0.918429003021148}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7877263581488934, "accuracy_n": 994, "auc": 0.7877263581488934}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5905797101449275, "accuracy_n": 276, "auc": 0.5905797101449275}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5525525525525525, "accuracy_n": 999, "auc": 0.5525525525525525}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 500, "auc": 0.52}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49498997995991983, "accuracy_n": 499, "auc": 0.49498997995991983}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.45484949832775917, "accuracy_n": 299, "auc": 0.45484949832775917}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48, "accuracy_n": 400, "auc": 0.48}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8410596026490066, "accuracy_n": 302, "auc": 0.8410596026490066}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7566419524250849, "accuracy_n": 322, "auc": 0.7566419524250849}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7132988721804512, "accuracy_n": 292, "auc": 0.7132988721804512}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6868932038834952, "accuracy_n": 412, "auc": 0.6868932038834952}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6393754423213022, "accuracy_n": 1902, "auc": 0.6393754423213022}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5414944795071022, "accuracy_n": 2000, "auc": 0.5414944795071022}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6582633053221288, "accuracy_n": 59, "auc": 0.6582633053221288}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7442094662638469, "accuracy_n": 993, "auc": 0.7442094662638469}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 994, "auc": 0.545271629778672}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5217391304347826, "accuracy_n": 276, "auc": 0.5217391304347826}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6166166166166166, "accuracy_n": 999, "auc": 0.6166166166166166}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.564, "accuracy_n": 500, "auc": 0.564}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49298597194388777, "accuracy_n": 499, "auc": 0.49298597194388777}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5083612040133779, "accuracy_n": 299, "auc": 0.5083612040133779}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 400, "auc": 0.56}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7682119205298014, "accuracy_n": 302, "auc": 0.7682119205298014}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9501467408093913, "accuracy_n": 322, "auc": 0.9501467408093913}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6450422932330827, "accuracy_n": 292, "auc": 0.6450422932330827}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8446601941747572, "accuracy_n": 412, "auc": 0.8446601941747572}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5013557147912243, "accuracy_n": 1902, "auc": 0.5013557147912243}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5736005698056998, "accuracy_n": 2000, "auc": 0.5736005698056998}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5406162464985994, "accuracy_n": 59, "auc": 0.5406162464985994}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5649546827794562, "accuracy_n": 993, "auc": 0.5649546827794562}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.56841046277666, "accuracy_n": 994, "auc": 0.56841046277666}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5434782608695652, "accuracy_n": 276, "auc": 0.5434782608695652}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6166166166166166, "accuracy_n": 999, "auc": 0.6166166166166166}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.496, "accuracy_n": 500, "auc": 0.496}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4649298597194389, "accuracy_n": 499, "auc": 0.4649298597194389}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.43478260869565216, "accuracy_n": 299, "auc": 0.43478260869565216}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.535, "accuracy_n": 400, "auc": 0.535}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6059602649006622, "accuracy_n": 302, "auc": 0.6059602649006622}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.736252703120173, "accuracy_n": 322, "auc": 0.736252703120173}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5820958646616541, "accuracy_n": 292, "auc": 0.5820958646616541}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7597087378640777, "accuracy_n": 412, "auc": 0.7597087378640777}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5397724256900212, "accuracy_n": 1902, "auc": 0.5397724256900212}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5250635479408067, "accuracy_n": 2000, "auc": 0.5250635479408067}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5756302521008403, "accuracy_n": 59, "auc": 0.5756302521008403}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7230614300100705, "accuracy_n": 993, "auc": 0.7230614300100705}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5814889336016097, "accuracy_n": 994, "auc": 0.5814889336016097}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6176176176176176, "accuracy_n": 999, "auc": 0.6176176176176176}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.376, "accuracy_n": 500, "auc": 0.376}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3627254509018036, "accuracy_n": 499, "auc": 0.3627254509018036}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.29431438127090304, "accuracy_n": 299, "auc": 0.29431438127090304}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.34, "accuracy_n": 400, "auc": 0.34}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6258278145695364, "accuracy_n": 302, "auc": 0.6258278145695364}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6830784677170219, "accuracy_n": 322, "auc": 0.6830784677170219}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5147086466165414, "accuracy_n": 292, "auc": 0.5147086466165414}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9053398058252428, "accuracy_n": 412, "auc": 0.9053398058252428}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5405299009200284, "accuracy_n": 1902, "auc": 0.5405299009200284}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5215047632195222, "accuracy_n": 2000, "auc": 0.5215047632195222}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5826330532212884, "accuracy_n": 59, "auc": 0.5826330532212884}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8318227593152064, "accuracy_n": 993, "auc": 0.8318227593152064}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8229376257545271, "accuracy_n": 994, "auc": 0.8229376257545271}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5275275275275275, "accuracy_n": 999, "auc": 0.5275275275275275}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.252, "accuracy_n": 500, "auc": 0.252}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3927855711422846, "accuracy_n": 499, "auc": 0.3927855711422846}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2325, "accuracy_n": 400, "auc": 0.2325}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5198675496688742, "accuracy_n": 302, "auc": 0.5198675496688742}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5196169292554835, "accuracy_n": 322, "auc": 0.5196169292554835}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6164003759398495, "accuracy_n": 292, "auc": 0.6164003759398495}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5606796116504854, "accuracy_n": 412, "auc": 0.5606796116504854}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9709096337579618, "accuracy_n": 1902, "auc": 0.9709096337579618}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5051798699330459, "accuracy_n": 2000, "auc": 0.5051798699330459}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5770308123249299, "accuracy_n": 59, "auc": 0.5770308123249299}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5750251762336355, "accuracy_n": 993, "auc": 0.5750251762336355}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6186186186186187, "accuracy_n": 999, "auc": 0.6186186186186187}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.356, "accuracy_n": 500, "auc": 0.356}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.35785953177257523, "accuracy_n": 299, "auc": 0.35785953177257523}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2625, "accuracy_n": 400, "auc": 0.2625}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6754966887417219, "accuracy_n": 302, "auc": 0.6754966887417219}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6340747605807846, "accuracy_n": 322, "auc": 0.6340747605807846}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6754464285714286, "accuracy_n": 292, "auc": 0.6754464285714286}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7330097087378641, "accuracy_n": 412, "auc": 0.7330097087378641}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.591094966383581, "accuracy_n": 1902, "auc": 0.591094966383581}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5091042866474798, "accuracy_n": 2000, "auc": 0.5091042866474798}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5740181268882175, "accuracy_n": 993, "auc": 0.5740181268882175}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6247484909456741, "accuracy_n": 994, "auc": 0.6247484909456741}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6186186186186187, "accuracy_n": 999, "auc": 0.6186186186186187}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.280561122244489, "accuracy_n": 499, "auc": 0.280561122244489}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3411371237458194, "accuracy_n": 299, "auc": 0.3411371237458194}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.305, "accuracy_n": 400, "auc": 0.305}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5298013245033113, "accuracy_n": 302, "auc": 0.5298013245033113}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5408943466172382, "accuracy_n": 322, "auc": 0.5408943466172382}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6408364661654136, "accuracy_n": 292, "auc": 0.6408364661654136}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6092233009708737, "accuracy_n": 412, "auc": 0.6092233009708737}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5846547682236377, "accuracy_n": 1902, "auc": 0.5846547682236377}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5017411285474056, "accuracy_n": 2000, "auc": 0.5017411285474056}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.572829131652661, "accuracy_n": 59, "auc": 0.572829131652661}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.56797583081571, "accuracy_n": 993, "auc": 0.56797583081571}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6046277665995976, "accuracy_n": 994, "auc": 0.6046277665995976}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6176176176176176, "accuracy_n": 999, "auc": 0.6176176176176176}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2565130260521042, "accuracy_n": 499, "auc": 0.2565130260521042}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3875, "accuracy_n": 400, "auc": 0.3875}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5198675496688742, "accuracy_n": 302, "auc": 0.5198675496688742}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5102718566573988, "accuracy_n": 322, "auc": 0.5102718566573988}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.624906015037594, "accuracy_n": 292, "auc": 0.624906015037594}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5655339805825242, "accuracy_n": 412, "auc": 0.5655339805825242}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5818117480537863, "accuracy_n": 1902, "auc": 0.5818117480537863}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5037603574890536, "accuracy_n": 2000, "auc": 0.5037603574890536}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
