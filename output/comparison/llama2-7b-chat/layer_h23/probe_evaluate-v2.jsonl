{"key": "result_0", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9129129129129129, "accuracy_n": 999, "auc": 0.9129129129129129}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9166666666666666, "accuracy_n": 996, "auc": 0.9166666666666666}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.631768953068592, "accuracy_n": 277, "auc": 0.631768953068592}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5592369477911646, "accuracy_n": 996, "auc": 0.5592369477911646}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49498997995991983, "accuracy_n": 499, "auc": 0.49498997995991983}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.628, "accuracy_n": 500, "auc": 0.628}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49328859060402686, "accuracy_n": 298, "auc": 0.49328859060402686}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5975, "accuracy_n": 400, "auc": 0.5975}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6336633663366337, "accuracy_n": 303, "auc": 0.6336633663366337}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.568736484399135, "accuracy_n": 322, "auc": 0.568736484399135}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5954887218045113, "accuracy_n": 292, "auc": 0.5954887218045113}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5558252427184466, "accuracy_n": 412, "auc": 0.5558252427184466}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6464216206652512, "accuracy_n": 1902, "auc": 0.6464216206652512}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5097335137984813, "accuracy_n": 2000, "auc": 0.5097335137984813}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5644257703081232, "accuracy_n": 59, "auc": 0.5644257703081232}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.923076923076923, "accuracy_n": 23, "auc": 0.923076923076923}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9409409409409409, "accuracy_n": 999, "auc": 0.9409409409409409}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9186746987951807, "accuracy_n": 996, "auc": 0.9186746987951807}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6967509025270758, "accuracy_n": 277, "auc": 0.6967509025270758}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5692771084337349, "accuracy_n": 996, "auc": 0.5692771084337349}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.533066132264529, "accuracy_n": 499, "auc": 0.533066132264529}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.616, "accuracy_n": 500, "auc": 0.616}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4429530201342282, "accuracy_n": 298, "auc": 0.4429530201342282}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5675, "accuracy_n": 400, "auc": 0.5675}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6204620462046204, "accuracy_n": 303, "auc": 0.6204620462046204}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5637164040778498, "accuracy_n": 322, "auc": 0.5637164040778498}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5466165413533834, "accuracy_n": 292, "auc": 0.5466165413533834}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5631067961165048, "accuracy_n": 412, "auc": 0.5631067961165048}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6795968241330502, "accuracy_n": 1902, "auc": 0.6795968241330502}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5147398210754082, "accuracy_n": 2000, "auc": 0.5147398210754082}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.546218487394958, "accuracy_n": 59, "auc": 0.546218487394958}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9461538461538461, "accuracy_n": 23, "auc": 0.9461538461538461}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.93993993993994, "accuracy_n": 999, "auc": 0.93993993993994}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8403614457831325, "accuracy_n": 996, "auc": 0.8403614457831325}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7472924187725631, "accuracy_n": 277, "auc": 0.7472924187725631}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6134538152610441, "accuracy_n": 996, "auc": 0.6134538152610441}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.30861723446893785, "accuracy_n": 499, "auc": 0.30861723446893785}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48, "accuracy_n": 500, "auc": 0.48}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2375, "accuracy_n": 400, "auc": 0.2375}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5610561056105611, "accuracy_n": 303, "auc": 0.5610561056105611}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5219725054062403, "accuracy_n": 322, "auc": 0.5219725054062403}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5804981203007519, "accuracy_n": 292, "auc": 0.5804981203007519}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6067961165048543, "accuracy_n": 412, "auc": 0.6067961165048543}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5968252388535031, "accuracy_n": 1902, "auc": 0.5968252388535031}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5125695376030747, "accuracy_n": 2000, "auc": 0.5125695376030747}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5826330532212884, "accuracy_n": 59, "auc": 0.5826330532212884}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8998998998998999, "accuracy_n": 999, "auc": 0.8998998998998999}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6767068273092369, "accuracy_n": 996, "auc": 0.6767068273092369}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.606425702811245, "accuracy_n": 996, "auc": 0.606425702811245}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5130260521042084, "accuracy_n": 499, "auc": 0.5130260521042084}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 500, "auc": 0.63}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4463087248322148, "accuracy_n": 298, "auc": 0.4463087248322148}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6375, "accuracy_n": 400, "auc": 0.6375}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5317423540315107, "accuracy_n": 322, "auc": 0.5317423540315107}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5664473684210526, "accuracy_n": 292, "auc": 0.5664473684210526}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6213592233009708, "accuracy_n": 412, "auc": 0.6213592233009708}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5476004069355981, "accuracy_n": 1902, "auc": 0.5476004069355981}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5242897686064669, "accuracy_n": 2000, "auc": 0.5242897686064669}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5224089635854342, "accuracy_n": 59, "auc": 0.5224089635854342}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5903614457831325, "accuracy_n": 996, "auc": 0.5903614457831325}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3425, "accuracy_n": 400, "auc": 0.3425}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5874587458745875, "accuracy_n": 303, "auc": 0.5874587458745875}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5798192771084337, "accuracy_n": 322, "auc": 0.5798192771084337}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5362781954887219, "accuracy_n": 292, "auc": 0.5362781954887219}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6140776699029126, "accuracy_n": 412, "auc": 0.6140776699029126}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.547456652512385, "accuracy_n": 1902, "auc": 0.547456652512385}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5134808665928401, "accuracy_n": 2000, "auc": 0.5134808665928401}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5896358543417367, "accuracy_n": 59, "auc": 0.5896358543417367}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7937937937937938, "accuracy_n": 999, "auc": 0.7937937937937938}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6265060240963856, "accuracy_n": 996, "auc": 0.6265060240963856}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5983935742971888, "accuracy_n": 996, "auc": 0.5983935742971888}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.646, "accuracy_n": 500, "auc": 0.646}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6140939597315436, "accuracy_n": 298, "auc": 0.6140939597315436}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6625, "accuracy_n": 400, "auc": 0.6625}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8592060549891876, "accuracy_n": 322, "auc": 0.8592060549891876}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.703547932330827, "accuracy_n": 292, "auc": 0.703547932330827}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5436893203883495, "accuracy_n": 412, "auc": 0.5436893203883495}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.505088906581741, "accuracy_n": 1902, "auc": 0.505088906581741}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5618468267044403, "accuracy_n": 2000, "auc": 0.5618468267044403}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6414565826330532, "accuracy_n": 59, "auc": 0.6414565826330532}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9039039039039038, "accuracy_n": 999, "auc": 0.9039039039039038}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6526104417670683, "accuracy_n": 996, "auc": 0.6526104417670683}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5973895582329317, "accuracy_n": 996, "auc": 0.5973895582329317}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.652, "accuracy_n": 500, "auc": 0.652}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5939597315436241, "accuracy_n": 298, "auc": 0.5939597315436241}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6698331788693235, "accuracy_n": 322, "auc": 0.6698331788693235}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6240601503759399, "accuracy_n": 292, "auc": 0.6240601503759399}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5048543689320388, "accuracy_n": 412, "auc": 0.5048543689320388}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5356278750884642, "accuracy_n": 1902, "auc": 0.5356278750884642}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5304815038228801, "accuracy_n": 2000, "auc": 0.5304815038228801}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7907907907907908, "accuracy_n": 999, "auc": 0.7907907907907908}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6154618473895582, "accuracy_n": 996, "auc": 0.6154618473895582}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5060240963855421, "accuracy_n": 996, "auc": 0.5060240963855421}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.628, "accuracy_n": 500, "auc": 0.628}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6073825503355704, "accuracy_n": 298, "auc": 0.6073825503355704}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5676567656765676, "accuracy_n": 303, "auc": 0.5676567656765676}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7630522088353414, "accuracy_n": 322, "auc": 0.7630522088353414}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7134398496240602, "accuracy_n": 292, "auc": 0.7134398496240602}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5072815533980582, "accuracy_n": 412, "auc": 0.5072815533980582}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5090393887119604, "accuracy_n": 1902, "auc": 0.5090393887119604}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5277075024083695, "accuracy_n": 2000, "auc": 0.5277075024083695}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8508508508508509, "accuracy_n": 999, "auc": 0.8508508508508509}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7459839357429718, "accuracy_n": 996, "auc": 0.7459839357429718}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5441767068273092, "accuracy_n": 996, "auc": 0.5441767068273092}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6212424849699398, "accuracy_n": 499, "auc": 0.6212424849699398}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.644, "accuracy_n": 500, "auc": 0.644}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5973154362416108, "accuracy_n": 298, "auc": 0.5973154362416108}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7184893419833179, "accuracy_n": 322, "auc": 0.7184893419833179}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5002349624060151, "accuracy_n": 292, "auc": 0.5002349624060151}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5606796116504854, "accuracy_n": 412, "auc": 0.5606796116504854}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5151395523708422, "accuracy_n": 1902, "auc": 0.5151395523708422}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5431785874700766, "accuracy_n": 2000, "auc": 0.5431785874700766}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5084033613445378, "accuracy_n": 59, "auc": 0.5084033613445378}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8388388388388388, "accuracy_n": 999, "auc": 0.8388388388388388}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8012048192771084, "accuracy_n": 996, "auc": 0.8012048192771084}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6425992779783394, "accuracy_n": 277, "auc": 0.6425992779783394}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.43887775551102204, "accuracy_n": 499, "auc": 0.43887775551102204}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.44, "accuracy_n": 500, "auc": 0.44}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3221476510067114, "accuracy_n": 298, "auc": 0.3221476510067114}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 400, "auc": 0.32}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7425742574257426, "accuracy_n": 303, "auc": 0.7425742574257426}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7298038307074451, "accuracy_n": 322, "auc": 0.7298038307074451}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5062499999999999, "accuracy_n": 292, "auc": 0.5062499999999999}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8907766990291263, "accuracy_n": 412, "auc": 0.8907766990291263}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5647071832979476, "accuracy_n": 1902, "auc": 0.5647071832979476}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5319575366707382, "accuracy_n": 2000, "auc": 0.5319575366707382}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5378151260504201, "accuracy_n": 59, "auc": 0.5378151260504201}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9169169169169169, "accuracy_n": 999, "auc": 0.9169169169169169}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.786144578313253, "accuracy_n": 996, "auc": 0.786144578313253}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5502008032128514, "accuracy_n": 996, "auc": 0.5502008032128514}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5110220440881763, "accuracy_n": 499, "auc": 0.5110220440881763}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.496, "accuracy_n": 500, "auc": 0.496}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4597315436241611, "accuracy_n": 298, "auc": 0.4597315436241611}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4875, "accuracy_n": 400, "auc": 0.4875}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8481848184818482, "accuracy_n": 303, "auc": 0.8481848184818482}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7476444238492431, "accuracy_n": 322, "auc": 0.7476444238492431}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7201362781954888, "accuracy_n": 292, "auc": 0.7201362781954888}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6893203883495146, "accuracy_n": 412, "auc": 0.6893203883495146}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.639345585633404, "accuracy_n": 1902, "auc": 0.639345585633404}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5430345354673037, "accuracy_n": 2000, "auc": 0.5430345354673037}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6456582633053222, "accuracy_n": 59, "auc": 0.6456582633053222}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7447447447447447, "accuracy_n": 999, "auc": 0.7447447447447447}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5411646586345381, "accuracy_n": 996, "auc": 0.5411646586345381}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6154618473895582, "accuracy_n": 996, "auc": 0.6154618473895582}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5591182364729459, "accuracy_n": 499, "auc": 0.5591182364729459}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49, "accuracy_n": 500, "auc": 0.49}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4966442953020134, "accuracy_n": 298, "auc": 0.4966442953020134}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5575, "accuracy_n": 400, "auc": 0.5575}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7788778877887789, "accuracy_n": 303, "auc": 0.7788778877887789}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9514983008958913, "accuracy_n": 322, "auc": 0.9514983008958913}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6304041353383458, "accuracy_n": 292, "auc": 0.6304041353383458}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8495145631067961, "accuracy_n": 412, "auc": 0.8495145631067961}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.501386677282378, "accuracy_n": 1902, "auc": 0.501386677282378}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5709396091989208, "accuracy_n": 2000, "auc": 0.5709396091989208}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5378151260504201, "accuracy_n": 59, "auc": 0.5378151260504201}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5685685685685685, "accuracy_n": 999, "auc": 0.5685685685685685}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5662650602409639, "accuracy_n": 996, "auc": 0.5662650602409639}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6144578313253012, "accuracy_n": 996, "auc": 0.6144578313253012}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48697394789579157, "accuracy_n": 499, "auc": 0.48697394789579157}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.468, "accuracy_n": 500, "auc": 0.468}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4161073825503356, "accuracy_n": 298, "auc": 0.4161073825503356}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 400, "auc": 0.55}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6204620462046204, "accuracy_n": 303, "auc": 0.6204620462046204}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.743551127587272, "accuracy_n": 322, "auc": 0.743551127587272}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5659774436090226, "accuracy_n": 292, "auc": 0.5659774436090226}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7645631067961165, "accuracy_n": 412, "auc": 0.7645631067961165}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5397348283793348, "accuracy_n": 1902, "auc": 0.5397348283793348}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5211881489217608, "accuracy_n": 2000, "auc": 0.5211881489217608}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5742296918767507, "accuracy_n": 59, "auc": 0.5742296918767507}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7207207207207207, "accuracy_n": 999, "auc": 0.7207207207207207}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5803212851405622, "accuracy_n": 996, "auc": 0.5803212851405622}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6154618473895582, "accuracy_n": 996, "auc": 0.6154618473895582}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.354, "accuracy_n": 500, "auc": 0.354}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3475, "accuracy_n": 400, "auc": 0.3475}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6435643564356436, "accuracy_n": 303, "auc": 0.6435643564356436}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6894887241272784, "accuracy_n": 322, "auc": 0.6894887241272784}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5026315789473683, "accuracy_n": 292, "auc": 0.5026315789473683}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9101941747572816, "accuracy_n": 412, "auc": 0.9101941747572816}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5404458598726115, "accuracy_n": 1902, "auc": 0.5404458598726115}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5171741998861589, "accuracy_n": 2000, "auc": 0.5171741998861589}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5798319327731093, "accuracy_n": 59, "auc": 0.5798319327731093}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8308308308308309, "accuracy_n": 999, "auc": 0.8308308308308309}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.821285140562249, "accuracy_n": 996, "auc": 0.821285140562249}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5271084337349398, "accuracy_n": 996, "auc": 0.5271084337349398}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.25851703406813625, "accuracy_n": 499, "auc": 0.25851703406813625}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.402, "accuracy_n": 500, "auc": 0.402}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2175, "accuracy_n": 400, "auc": 0.2175}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5332483781278963, "accuracy_n": 322, "auc": 0.5332483781278963}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6341635338345866, "accuracy_n": 292, "auc": 0.6341635338345866}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.558252427184466, "accuracy_n": 412, "auc": 0.558252427184466}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9709195859872612, "accuracy_n": 1902, "auc": 0.9709195859872612}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5018311610491387, "accuracy_n": 2000, "auc": 0.5018311610491387}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5756302521008404, "accuracy_n": 59, "auc": 0.5756302521008404}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5775775775775776, "accuracy_n": 999, "auc": 0.5775775775775776}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3667334669338677, "accuracy_n": 499, "auc": 0.3667334669338677}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3624161073825503, "accuracy_n": 298, "auc": 0.3624161073825503}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2575, "accuracy_n": 400, "auc": 0.2575}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7095709570957096, "accuracy_n": 303, "auc": 0.7095709570957096}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6194006796416436, "accuracy_n": 322, "auc": 0.6194006796416436}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6941494360902255, "accuracy_n": 292, "auc": 0.6941494360902255}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7257281553398058, "accuracy_n": 412, "auc": 0.7257281553398058}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5910938605803255, "accuracy_n": 1902, "auc": 0.5910938605803255}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5114506336787581, "accuracy_n": 2000, "auc": 0.5114506336787581}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5805805805805806, "accuracy_n": 999, "auc": 0.5805805805805806}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6224899598393574, "accuracy_n": 996, "auc": 0.6224899598393574}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3186372745490982, "accuracy_n": 499, "auc": 0.3186372745490982}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.348993288590604, "accuracy_n": 298, "auc": 0.348993288590604}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.315, "accuracy_n": 400, "auc": 0.315}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5278035217794252, "accuracy_n": 322, "auc": 0.5278035217794252}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6573778195488722, "accuracy_n": 292, "auc": 0.6573778195488722}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5970873786407767, "accuracy_n": 412, "auc": 0.5970873786407767}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5846790958952583, "accuracy_n": 1902, "auc": 0.5846790958952583}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5013459859009102, "accuracy_n": 2000, "auc": 0.5013459859009102}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5672268907563026, "accuracy_n": 59, "auc": 0.5672268907563026}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5705705705705706, "accuracy_n": 999, "auc": 0.5705705705705706}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6004016064257028, "accuracy_n": 996, "auc": 0.6004016064257028}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2665330661322645, "accuracy_n": 499, "auc": 0.2665330661322645}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.262, "accuracy_n": 500, "auc": 0.262}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.30201342281879195, "accuracy_n": 298, "auc": 0.30201342281879195}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.39, "accuracy_n": 400, "auc": 0.39}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5217021933889404, "accuracy_n": 322, "auc": 0.5217021933889404}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6430686090225564, "accuracy_n": 292, "auc": 0.6430686090225564}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5558252427184466, "accuracy_n": 412, "auc": 0.5558252427184466}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5817829971691437, "accuracy_n": 1902, "auc": 0.5817829971691437}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5003911412019739, "accuracy_n": 2000, "auc": 0.5003911412019739}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5686274509803922, "accuracy_n": 59, "auc": 0.5686274509803922}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
