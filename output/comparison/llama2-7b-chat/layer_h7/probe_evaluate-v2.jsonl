{"key": "result_0", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6676676676676677, "accuracy_n": 999, "auc": 0.6676676676676677}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6847389558232931, "accuracy_n": 996, "auc": 0.6847389558232931}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6154618473895582, "accuracy_n": 996, "auc": 0.6154618473895582}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.27181208053691275, "accuracy_n": 298, "auc": 0.27181208053691275}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2, "accuracy_n": 400, "auc": 0.2}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5613222119246215, "accuracy_n": 322, "auc": 0.5613222119246215}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5795582706766917, "accuracy_n": 292, "auc": 0.5795582706766917}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5072815533980582, "accuracy_n": 412, "auc": 0.5072815533980582}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6554980537862702, "accuracy_n": 1902, "auc": 0.6554980537862702}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5223915833615935, "accuracy_n": 2000, "auc": 0.5223915833615935}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6288515406162465, "accuracy_n": 59, "auc": 0.6288515406162465}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5895895895895896, "accuracy_n": 999, "auc": 0.5895895895895896}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.8142570281124498, "accuracy_n": 996, "auc": 0.8142570281124498}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.608433734939759, "accuracy_n": 996, "auc": 0.608433734939759}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25851703406813625, "accuracy_n": 499, "auc": 0.25851703406813625}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2, "accuracy_n": 400, "auc": 0.2}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5489264751312943, "accuracy_n": 322, "auc": 0.5489264751312943}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5998120300751879, "accuracy_n": 292, "auc": 0.5998120300751879}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5388349514563107, "accuracy_n": 412, "auc": 0.5388349514563107}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6624126415428168, "accuracy_n": 1902, "auc": 0.6624126415428168}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5163058864249994, "accuracy_n": 2000, "auc": 0.5163058864249994}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5224089635854342, "accuracy_n": 59, "auc": 0.5224089635854342}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5572289156626506, "accuracy_n": 996, "auc": 0.5572289156626506}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5933734939759037, "accuracy_n": 996, "auc": 0.5933734939759037}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.274, "accuracy_n": 500, "auc": 0.274}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.1775, "accuracy_n": 400, "auc": 0.1775}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5184970651838121, "accuracy_n": 322, "auc": 0.5184970651838121}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5743890977443609, "accuracy_n": 292, "auc": 0.5743890977443609}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5606796116504854, "accuracy_n": 412, "auc": 0.5606796116504854}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5952041312809624, "accuracy_n": 1902, "auc": 0.5952041312809624}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5009538443378059, "accuracy_n": 2000, "auc": 0.5009538443378059}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 59, "auc": 0.5}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5512048192771084, "accuracy_n": 996, "auc": 0.5512048192771084}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2605210420841683, "accuracy_n": 499, "auc": 0.2605210420841683}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2, "accuracy_n": 400, "auc": 0.2}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5544871794871795, "accuracy_n": 322, "auc": 0.5544871794871795}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6138157894736842, "accuracy_n": 292, "auc": 0.6138157894736842}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5339805825242718, "accuracy_n": 412, "auc": 0.5339805825242718}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6859817321302194, "accuracy_n": 1902, "auc": 0.6859817321302194}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5049662928317122, "accuracy_n": 2000, "auc": 0.5049662928317122}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5672268907563025, "accuracy_n": 59, "auc": 0.5672268907563025}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5471887550200804, "accuracy_n": 996, "auc": 0.5471887550200804}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2865731462925852, "accuracy_n": 499, "auc": 0.2865731462925852}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.215, "accuracy_n": 400, "auc": 0.215}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5497374111831943, "accuracy_n": 322, "auc": 0.5497374111831943}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6078007518796993, "accuracy_n": 292, "auc": 0.6078007518796993}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.63041733014862, "accuracy_n": 1902, "auc": 0.63041733014862}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5038363849349615, "accuracy_n": 2000, "auc": 0.5038363849349615}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5420168067226891, "accuracy_n": 59, "auc": 0.5420168067226891}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5130522088353414, "accuracy_n": 996, "auc": 0.5130522088353414}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6134538152610441, "accuracy_n": 996, "auc": 0.6134538152610441}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.4308617234468938, "accuracy_n": 499, "auc": 0.4308617234468938}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.336, "accuracy_n": 500, "auc": 0.336}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.205, "accuracy_n": 400, "auc": 0.205}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5323988260735248, "accuracy_n": 322, "auc": 0.5323988260735248}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5805921052631579, "accuracy_n": 292, "auc": 0.5805921052631579}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5315533980582524, "accuracy_n": 412, "auc": 0.5315533980582524}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5053996372965323, "accuracy_n": 1902, "auc": 0.5053996372965323}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5070060291765327, "accuracy_n": 2000, "auc": 0.5070060291765327}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5425425425425425, "accuracy_n": 999, "auc": 0.5425425425425425}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7028112449799196, "accuracy_n": 996, "auc": 0.7028112449799196}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.37074148296593185, "accuracy_n": 499, "auc": 0.37074148296593185}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.396, "accuracy_n": 500, "auc": 0.396}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2975, "accuracy_n": 400, "auc": 0.2975}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5053676243435279, "accuracy_n": 322, "auc": 0.5053676243435279}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6238486842105264, "accuracy_n": 292, "auc": 0.6238486842105264}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5169902912621359, "accuracy_n": 412, "auc": 0.5169902912621359}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5146209306440198, "accuracy_n": 1902, "auc": 0.5146209306440198}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.503603800972151, "accuracy_n": 2000, "auc": 0.503603800972151}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5742296918767507, "accuracy_n": 59, "auc": 0.5742296918767507}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5095095095095095, "accuracy_n": 999, "auc": 0.5095095095095095}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6556224899598394, "accuracy_n": 996, "auc": 0.6556224899598394}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.280561122244489, "accuracy_n": 499, "auc": 0.280561122244489}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.30201342281879195, "accuracy_n": 298, "auc": 0.30201342281879195}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.1925, "accuracy_n": 400, "auc": 0.1925}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5597389558232931, "accuracy_n": 322, "auc": 0.5597389558232931}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6017857142857144, "accuracy_n": 292, "auc": 0.6017857142857144}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5097087378640777, "accuracy_n": 412, "auc": 0.5097087378640777}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6301331387119604, "accuracy_n": 1902, "auc": 0.6301331387119604}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5157506859976452, "accuracy_n": 2000, "auc": 0.5157506859976452}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5686274509803922, "accuracy_n": 59, "auc": 0.5686274509803922}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3687374749498998, "accuracy_n": 499, "auc": 0.3687374749498998}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.358, "accuracy_n": 500, "auc": 0.358}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.33, "accuracy_n": 400, "auc": 0.33}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5225903614457831, "accuracy_n": 322, "auc": 0.5225903614457831}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6146381578947369, "accuracy_n": 292, "auc": 0.6146381578947369}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5121359223300971, "accuracy_n": 412, "auc": 0.5121359223300971}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5304029547062986, "accuracy_n": 1902, "auc": 0.5304029547062986}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5016841079629746, "accuracy_n": 2000, "auc": 0.5016841079629746}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5135135135135135, "accuracy_n": 999, "auc": 0.5135135135135135}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5240963855421686, "accuracy_n": 996, "auc": 0.5240963855421686}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 500, "auc": 0.26}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.225, "accuracy_n": 400, "auc": 0.225}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5690261044176707, "accuracy_n": 322, "auc": 0.5690261044176707}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.556156015037594, "accuracy_n": 292, "auc": 0.556156015037594}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5339805825242718, "accuracy_n": 412, "auc": 0.5339805825242718}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5726037243453646, "accuracy_n": 1902, "auc": 0.5726037243453646}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5058431093624798, "accuracy_n": 2000, "auc": 0.5058431093624798}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5476190476190476, "accuracy_n": 59, "auc": 0.5476190476190476}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5155155155155156, "accuracy_n": 999, "auc": 0.5155155155155156}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5662650602409639, "accuracy_n": 996, "auc": 0.5662650602409639}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.32665330661322645, "accuracy_n": 499, "auc": 0.32665330661322645}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.244, "accuracy_n": 500, "auc": 0.244}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2625, "accuracy_n": 400, "auc": 0.2625}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5784291010194624, "accuracy_n": 322, "auc": 0.5784291010194624}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5890272556390977, "accuracy_n": 292, "auc": 0.5890272556390977}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 412, "auc": 0.5}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5969026450813871, "accuracy_n": 1902, "auc": 0.5969026450813871}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5071925965273464, "accuracy_n": 2000, "auc": 0.5071925965273464}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5560224089635855, "accuracy_n": 59, "auc": 0.5560224089635855}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5145145145145145, "accuracy_n": 999, "auc": 0.5145145145145145}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6495983935742972, "accuracy_n": 996, "auc": 0.6495983935742972}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6144578313253012, "accuracy_n": 996, "auc": 0.6144578313253012}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26452905811623245, "accuracy_n": 499, "auc": 0.26452905811623245}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.27181208053691275, "accuracy_n": 298, "auc": 0.27181208053691275}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.205, "accuracy_n": 400, "auc": 0.205}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6348084646277418, "accuracy_n": 322, "auc": 0.6348084646277418}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.540625, "accuracy_n": 292, "auc": 0.540625}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5242718446601942, "accuracy_n": 412, "auc": 0.5242718446601942}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5586728149327671, "accuracy_n": 1902, "auc": 0.5586728149327671}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5123244491261345, "accuracy_n": 2000, "auc": 0.5123244491261345}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5644257703081232, "accuracy_n": 59, "auc": 0.5644257703081232}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5025025025025025, "accuracy_n": 999, "auc": 0.5025025025025025}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6847389558232931, "accuracy_n": 996, "auc": 0.6847389558232931}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5773092369477911, "accuracy_n": 996, "auc": 0.5773092369477911}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.23, "accuracy_n": 400, "auc": 0.23}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5834491813407476, "accuracy_n": 322, "auc": 0.5834491813407476}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5307800751879699, "accuracy_n": 292, "auc": 0.5307800751879699}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5558252427184466, "accuracy_n": 412, "auc": 0.5558252427184466}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5456619338287332, "accuracy_n": 1902, "auc": 0.5456619338287332}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5077457962324399, "accuracy_n": 2000, "auc": 0.5077457962324399}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5798319327731093, "accuracy_n": 59, "auc": 0.5798319327731093}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5005005005005005, "accuracy_n": 999, "auc": 0.5005005005005005}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5050200803212851, "accuracy_n": 996, "auc": 0.5050200803212851}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6154618473895582, "accuracy_n": 996, "auc": 0.6154618473895582}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26452905811623245, "accuracy_n": 499, "auc": 0.26452905811623245}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25838926174496646, "accuracy_n": 298, "auc": 0.25838926174496646}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 400, "auc": 0.25}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5308928019771393, "accuracy_n": 322, "auc": 0.5308928019771393}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5793703007518797, "accuracy_n": 292, "auc": 0.5793703007518797}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5194174757281553, "accuracy_n": 412, "auc": 0.5194174757281553}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5563119249823072, "accuracy_n": 1902, "auc": 0.5563119249823072}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5072231075418226, "accuracy_n": 2000, "auc": 0.5072231075418226}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5742296918767507, "accuracy_n": 59, "auc": 0.5742296918767507}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6046046046046046, "accuracy_n": 999, "auc": 0.6046046046046046}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5040160642570282, "accuracy_n": 996, "auc": 0.5040160642570282}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5110441767068273, "accuracy_n": 996, "auc": 0.5110441767068273}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.27, "accuracy_n": 500, "auc": 0.27}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2575, "accuracy_n": 400, "auc": 0.2575}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5649907321594069, "accuracy_n": 322, "auc": 0.5649907321594069}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5648966165413534, "accuracy_n": 292, "auc": 0.5648966165413534}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5145631067961165, "accuracy_n": 412, "auc": 0.5145631067961165}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.9073325813871196, "accuracy_n": 1902, "auc": 0.9073325813871196}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5037553556834017, "accuracy_n": 2000, "auc": 0.5037553556834017}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5574229691876751, "accuracy_n": 59, "auc": 0.5574229691876751}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5115115115115115, "accuracy_n": 999, "auc": 0.5115115115115115}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.27655310621242485, "accuracy_n": 499, "auc": 0.27655310621242485}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2483221476510067, "accuracy_n": 298, "auc": 0.2483221476510067}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2675, "accuracy_n": 400, "auc": 0.2675}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.514886468952734, "accuracy_n": 322, "auc": 0.514886468952734}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5963815789473685, "accuracy_n": 292, "auc": 0.5963815789473685}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5266990291262136, "accuracy_n": 412, "auc": 0.5266990291262136}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5156039897381458, "accuracy_n": 1902, "auc": 0.5156039897381458}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5012749602606541, "accuracy_n": 2000, "auc": 0.5012749602606541}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5742296918767508, "accuracy_n": 59, "auc": 0.5742296918767508}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5115115115115115, "accuracy_n": 999, "auc": 0.5115115115115115}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5210843373493976, "accuracy_n": 996, "auc": 0.5210843373493976}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6194779116465864, "accuracy_n": 996, "auc": 0.6194779116465864}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 400, "auc": 0.26}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5290199258572753, "accuracy_n": 322, "auc": 0.5290199258572753}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.575281954887218, "accuracy_n": 292, "auc": 0.575281954887218}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5509708737864077, "accuracy_n": 412, "auc": 0.5509708737864077}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5058817675159235, "accuracy_n": 1902, "auc": 0.5058817675159235}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5009838551717171, "accuracy_n": 2000, "auc": 0.5009838551717171}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5756302521008403, "accuracy_n": 59, "auc": 0.5756302521008403}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5165165165165165, "accuracy_n": 999, "auc": 0.5165165165165165}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5100401606425703, "accuracy_n": 996, "auc": 0.5100401606425703}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6194779116465864, "accuracy_n": 996, "auc": 0.6194779116465864}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.280561122244489, "accuracy_n": 499, "auc": 0.280561122244489}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2575, "accuracy_n": 400, "auc": 0.2575}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.535275718257646, "accuracy_n": 322, "auc": 0.535275718257646}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5761983082706768, "accuracy_n": 292, "auc": 0.5761983082706768}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5461165048543689, "accuracy_n": 412, "auc": 0.5461165048543689}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5077262473460722, "accuracy_n": 1902, "auc": 0.5077262473460722}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5015415565018972, "accuracy_n": 2000, "auc": 0.5015415565018972}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5700280112044819, "accuracy_n": 59, "auc": 0.5700280112044819}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
