{"key": "result_0", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9124748490945674, "accuracy_n": 994, "auc": 0.9124748490945674}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9259259259259259, "accuracy_n": 999, "auc": 0.9259259259259259}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6859205776173285, "accuracy_n": 277, "auc": 0.6859205776173285}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.544904137235116, "accuracy_n": 991, "auc": 0.544904137235116}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5651302605210421, "accuracy_n": 499, "auc": 0.5651302605210421}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 498, "auc": 0.6164658634538153}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4816053511705686, "accuracy_n": 299, "auc": 0.4816053511705686}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.625, "accuracy_n": 400, "auc": 0.625}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5132450331125827, "accuracy_n": 302, "auc": 0.5132450331125827}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6186283595922151, "accuracy_n": 322, "auc": 0.6186283595922151}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5605263157894737, "accuracy_n": 292, "auc": 0.5605263157894737}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6870433032554848, "accuracy_n": 1902, "auc": 0.6870433032554848}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5041650035662875, "accuracy_n": 2000, "auc": 0.5041650035662875}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6414565826330532, "accuracy_n": 59, "auc": 0.6414565826330532}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9446680080482898, "accuracy_n": 994, "auc": 0.9446680080482898}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.934934934934935, "accuracy_n": 999, "auc": 0.934934934934935}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5983854692230071, "accuracy_n": 991, "auc": 0.5983854692230071}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5951903807615231, "accuracy_n": 499, "auc": 0.5951903807615231}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6385542168674698, "accuracy_n": 498, "auc": 0.6385542168674698}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5618729096989966, "accuracy_n": 299, "auc": 0.5618729096989966}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6275, "accuracy_n": 400, "auc": 0.6275}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6192052980132451, "accuracy_n": 302, "auc": 0.6192052980132451}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.506371640407785, "accuracy_n": 322, "auc": 0.506371640407785}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5833646616541354, "accuracy_n": 292, "auc": 0.5833646616541354}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7524294497523001, "accuracy_n": 1902, "auc": 0.7524294497523001}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5001920693370306, "accuracy_n": 2000, "auc": 0.5001920693370306}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9507042253521126, "accuracy_n": 994, "auc": 0.9507042253521126}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.938938938938939, "accuracy_n": 999, "auc": 0.938938938938939}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6462093862815884, "accuracy_n": 277, "auc": 0.6462093862815884}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6104944500504541, "accuracy_n": 991, "auc": 0.6104944500504541}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36072144288577157, "accuracy_n": 499, "auc": 0.36072144288577157}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4939759036144578, "accuracy_n": 498, "auc": 0.4939759036144578}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2375, "accuracy_n": 400, "auc": 0.2375}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5662251655629139, "accuracy_n": 302, "auc": 0.5662251655629139}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5928328699413037, "accuracy_n": 322, "auc": 0.5928328699413037}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5412124060150376, "accuracy_n": 292, "auc": 0.5412124060150376}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5472154963680388, "accuracy_n": 413, "auc": 0.5472154963680388}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5707990534324132, "accuracy_n": 1902, "auc": 0.5707990534324132}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5168065671707486, "accuracy_n": 2000, "auc": 0.5168065671707486}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8259557344064387, "accuracy_n": 994, "auc": 0.8259557344064387}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6196196196196196, "accuracy_n": 999, "auc": 0.6196196196196196}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6064581231079718, "accuracy_n": 991, "auc": 0.6064581231079718}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5070140280561122, "accuracy_n": 499, "auc": 0.5070140280561122}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6244979919678715, "accuracy_n": 498, "auc": 0.6244979919678715}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4816053511705686, "accuracy_n": 299, "auc": 0.4816053511705686}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 400, "auc": 0.68}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.543046357615894, "accuracy_n": 302, "auc": 0.543046357615894}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6305220883534136, "accuracy_n": 322, "auc": 0.6305220883534136}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.630686090225564, "accuracy_n": 292, "auc": 0.630686090225564}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6053268765133172, "accuracy_n": 413, "auc": 0.6053268765133172}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5204352441613588, "accuracy_n": 1902, "auc": 0.5204352441613588}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.533639143730887, "accuracy_n": 2000, "auc": 0.533639143730887}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5336134453781513, "accuracy_n": 59, "auc": 0.5336134453781513}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5285285285285285, "accuracy_n": 999, "auc": 0.5285285285285285}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4969939879759519, "accuracy_n": 499, "auc": 0.4969939879759519}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5401606425702812, "accuracy_n": 498, "auc": 0.5401606425702812}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5451505016722408, "accuracy_n": 299, "auc": 0.5451505016722408}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 400, "auc": 0.55}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6193234476367007, "accuracy_n": 322, "auc": 0.6193234476367007}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5081766917293232, "accuracy_n": 292, "auc": 0.5081766917293232}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7021791767554479, "accuracy_n": 413, "auc": 0.7021791767554479}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5418491242038216, "accuracy_n": 1902, "auc": 0.5418491242038216}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5298082607821424, "accuracy_n": 2000, "auc": 0.5298082607821424}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5854341736694678, "accuracy_n": 59, "auc": 0.5854341736694678}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8893360160965795, "accuracy_n": 994, "auc": 0.8893360160965795}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6586586586586587, "accuracy_n": 999, "auc": 0.6586586586586587}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7285570131180625, "accuracy_n": 991, "auc": 0.7285570131180625}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 499, "auc": 0.6172344689378757}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6325301204819277, "accuracy_n": 498, "auc": 0.6325301204819277}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6220735785953178, "accuracy_n": 299, "auc": 0.6220735785953178}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6625, "accuracy_n": 400, "auc": 0.6625}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8079470198675497, "accuracy_n": 302, "auc": 0.8079470198675497}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8664658634538153, "accuracy_n": 322, "auc": 0.8664658634538153}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.794125939849624, "accuracy_n": 292, "auc": 0.794125939849624}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5738498789346247, "accuracy_n": 413, "auc": 0.5738498789346247}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5146751150035386, "accuracy_n": 1902, "auc": 0.5146751150035386}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5728257901102297, "accuracy_n": 2000, "auc": 0.5728257901102297}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9446680080482898, "accuracy_n": 994, "auc": 0.9446680080482898}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7007007007007007, "accuracy_n": 999, "auc": 0.7007007007007007}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6389891696750902, "accuracy_n": 277, "auc": 0.6389891696750902}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6528758829465187, "accuracy_n": 991, "auc": 0.6528758829465187}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6506024096385542, "accuracy_n": 498, "auc": 0.6506024096385542}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6086956521739131, "accuracy_n": 299, "auc": 0.6086956521739131}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 400, "auc": 0.65}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.652317880794702, "accuracy_n": 302, "auc": 0.652317880794702}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7669524250849552, "accuracy_n": 322, "auc": 0.7669524250849552}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6278665413533835, "accuracy_n": 292, "auc": 0.6278665413533835}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5690072639225182, "accuracy_n": 413, "auc": 0.5690072639225182}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5576223018400566, "accuracy_n": 1902, "auc": 0.5576223018400566}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5528410756283018, "accuracy_n": 2000, "auc": 0.5528410756283018}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 59, "auc": 0.5}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7847082494969819, "accuracy_n": 994, "auc": 0.7847082494969819}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6416416416416416, "accuracy_n": 999, "auc": 0.6416416416416416}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6720484359233098, "accuracy_n": 991, "auc": 0.6720484359233098}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6265060240963856, "accuracy_n": 498, "auc": 0.6265060240963856}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6655629139072847, "accuracy_n": 302, "auc": 0.6655629139072847}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8199721964782205, "accuracy_n": 322, "auc": 0.8199721964782205}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7407894736842106, "accuracy_n": 292, "auc": 0.7407894736842106}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6004842615012107, "accuracy_n": 413, "auc": 0.6004842615012107}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5257132430997877, "accuracy_n": 1902, "auc": 0.5257132430997877}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5604438202190991, "accuracy_n": 2000, "auc": 0.5604438202190991}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5896358543417366, "accuracy_n": 59, "auc": 0.5896358543417366}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7107107107107107, "accuracy_n": 999, "auc": 0.7107107107107107}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7386478304742684, "accuracy_n": 991, "auc": 0.7386478304742684}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6232464929859719, "accuracy_n": 499, "auc": 0.6232464929859719}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6365461847389559, "accuracy_n": 498, "auc": 0.6365461847389559}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5695364238410596, "accuracy_n": 302, "auc": 0.5695364238410596}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7874575223972815, "accuracy_n": 322, "auc": 0.7874575223972815}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7007048872180451, "accuracy_n": 292, "auc": 0.7007048872180451}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5479221956829441, "accuracy_n": 1902, "auc": 0.5479221956829441}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.555135403880801, "accuracy_n": 2000, "auc": 0.555135403880801}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5868347338935574, "accuracy_n": 59, "auc": 0.5868347338935574}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5372233400402414, "accuracy_n": 994, "auc": 0.5372233400402414}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6376376376376376, "accuracy_n": 999, "auc": 0.6376376376376376}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7075812274368231, "accuracy_n": 277, "auc": 0.7075812274368231}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6589303733602422, "accuracy_n": 991, "auc": 0.6589303733602422}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.49298597194388777, "accuracy_n": 499, "auc": 0.49298597194388777}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4919678714859438, "accuracy_n": 498, "auc": 0.4919678714859438}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.451505016722408, "accuracy_n": 299, "auc": 0.451505016722408}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4725, "accuracy_n": 400, "auc": 0.4725}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.695364238410596, "accuracy_n": 302, "auc": 0.695364238410596}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6900293481618783, "accuracy_n": 322, "auc": 0.6900293481618783}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5060620300751879, "accuracy_n": 292, "auc": 0.5060620300751879}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8547215496368039, "accuracy_n": 413, "auc": 0.8547215496368039}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5196125265392781, "accuracy_n": 1902, "auc": 0.5196125265392781}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5246018812791418, "accuracy_n": 2000, "auc": 0.5246018812791418}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9114688128772636, "accuracy_n": 994, "auc": 0.9114688128772636}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7327327327327328, "accuracy_n": 999, "auc": 0.7327327327327328}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6606498194945848, "accuracy_n": 277, "auc": 0.6606498194945848}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6781029263370333, "accuracy_n": 991, "auc": 0.6781029263370333}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5250501002004008, "accuracy_n": 499, "auc": 0.5250501002004008}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4919678714859438, "accuracy_n": 498, "auc": 0.4919678714859438}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.43812709030100333, "accuracy_n": 299, "auc": 0.43812709030100333}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 400, "auc": 0.52}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6556291390728477, "accuracy_n": 302, "auc": 0.6556291390728477}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7173308619091752, "accuracy_n": 322, "auc": 0.7173308619091752}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5888627819548872, "accuracy_n": 292, "auc": 0.5888627819548872}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6828087167070218, "accuracy_n": 413, "auc": 0.6828087167070218}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6069079529370136, "accuracy_n": 1902, "auc": 0.6069079529370136}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5559957144529175, "accuracy_n": 2000, "auc": 0.5559957144529175}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5406162464985995, "accuracy_n": 59, "auc": 0.5406162464985995}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5633802816901409, "accuracy_n": 994, "auc": 0.5633802816901409}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5275275275275275, "accuracy_n": 999, "auc": 0.5275275275275275}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5667870036101083, "accuracy_n": 277, "auc": 0.5667870036101083}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6215943491422805, "accuracy_n": 991, "auc": 0.6215943491422805}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5551102204408818, "accuracy_n": 499, "auc": 0.5551102204408818}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5562248995983936, "accuracy_n": 498, "auc": 0.5562248995983936}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5150501672240803, "accuracy_n": 299, "auc": 0.5150501672240803}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5725, "accuracy_n": 400, "auc": 0.5725}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6556291390728477, "accuracy_n": 302, "auc": 0.6556291390728477}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7874575223972815, "accuracy_n": 322, "auc": 0.7874575223972815}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5883928571428572, "accuracy_n": 292, "auc": 0.5883928571428572}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.784503631961259, "accuracy_n": 413, "auc": 0.784503631961259}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5731168170559094, "accuracy_n": 1902, "auc": 0.5731168170559094}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5498279879036333, "accuracy_n": 2000, "auc": 0.5498279879036333}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5271629778672032, "accuracy_n": 994, "auc": 0.5271629778672032}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5355355355355356, "accuracy_n": 999, "auc": 0.5355355355355356}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6185671039354188, "accuracy_n": 991, "auc": 0.6185671039354188}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.48296593186372744, "accuracy_n": 499, "auc": 0.48296593186372744}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 498, "auc": 0.5160642570281124}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.451505016722408, "accuracy_n": 299, "auc": 0.451505016722408}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.525, "accuracy_n": 400, "auc": 0.525}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5728476821192053, "accuracy_n": 302, "auc": 0.5728476821192053}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7031587890021626, "accuracy_n": 322, "auc": 0.7031587890021626}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5532424812030075, "accuracy_n": 292, "auc": 0.5532424812030075}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6973365617433414, "accuracy_n": 413, "auc": 0.6973365617433414}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5278651362349611, "accuracy_n": 1902, "auc": 0.5278651362349611}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5215317729700422, "accuracy_n": 2000, "auc": 0.5215317729700422}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5798319327731093, "accuracy_n": 59, "auc": 0.5798319327731093}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5261569416498993, "accuracy_n": 994, "auc": 0.5261569416498993}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5235235235235235, "accuracy_n": 999, "auc": 0.5235235235235235}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6175580221997982, "accuracy_n": 991, "auc": 0.6175580221997982}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.501002004008016, "accuracy_n": 499, "auc": 0.501002004008016}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5100401606425703, "accuracy_n": 498, "auc": 0.5100401606425703}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.47157190635451507, "accuracy_n": 299, "auc": 0.47157190635451507}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5025, "accuracy_n": 400, "auc": 0.5025}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.652317880794702, "accuracy_n": 302, "auc": 0.652317880794702}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6805684275563794, "accuracy_n": 322, "auc": 0.6805684275563794}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5051691729323308, "accuracy_n": 292, "auc": 0.5051691729323308}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9007263922518159, "accuracy_n": 413, "auc": 0.9007263922518159}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5057601291578203, "accuracy_n": 1902, "auc": 0.5057601291578203}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.519498038792004, "accuracy_n": 2000, "auc": 0.519498038792004}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5756302521008404, "accuracy_n": 59, "auc": 0.5756302521008404}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9225352112676056, "accuracy_n": 994, "auc": 0.9225352112676056}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7637637637637638, "accuracy_n": 999, "auc": 0.7637637637637638}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5176589303733602, "accuracy_n": 991, "auc": 0.5176589303733602}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3393574297188755, "accuracy_n": 498, "auc": 0.3393574297188755}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.33444816053511706, "accuracy_n": 299, "auc": 0.33444816053511706}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 400, "auc": 0.31}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5099337748344371, "accuracy_n": 302, "auc": 0.5099337748344371}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5924080939141181, "accuracy_n": 322, "auc": 0.5924080939141181}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5499765037593984, "accuracy_n": 292, "auc": 0.5499765037593984}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9684934536447275, "accuracy_n": 1902, "auc": 0.9684934536447275}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5069745178009262, "accuracy_n": 2000, "auc": 0.5069745178009262}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.57703081232493, "accuracy_n": 59, "auc": 0.57703081232493}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5261569416498993, "accuracy_n": 994, "auc": 0.5261569416498993}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5965965965965966, "accuracy_n": 999, "auc": 0.5965965965965966}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5771947527749748, "accuracy_n": 991, "auc": 0.5771947527749748}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3867735470941884, "accuracy_n": 499, "auc": 0.3867735470941884}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.25502008032128515, "accuracy_n": 498, "auc": 0.25502008032128515}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36789297658862874, "accuracy_n": 299, "auc": 0.36789297658862874}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2875, "accuracy_n": 400, "auc": 0.2875}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5794701986754967, "accuracy_n": 302, "auc": 0.5794701986754967}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5932190299660178, "accuracy_n": 322, "auc": 0.5932190299660178}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6615131578947369, "accuracy_n": 292, "auc": 0.6615131578947369}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6610169491525424, "accuracy_n": 413, "auc": 0.6610169491525424}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.555490313163482, "accuracy_n": 1902, "auc": 0.555490313163482}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.504253535526325, "accuracy_n": 2000, "auc": 0.504253535526325}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5765765765765766, "accuracy_n": 999, "auc": 0.5765765765765766}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.579212916246216, "accuracy_n": 991, "auc": 0.579212916246216}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.30861723446893785, "accuracy_n": 499, "auc": 0.30861723446893785}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3132530120481928, "accuracy_n": 498, "auc": 0.3132530120481928}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2325, "accuracy_n": 400, "auc": 0.2325}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5482893110905159, "accuracy_n": 322, "auc": 0.5482893110905159}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6363486842105264, "accuracy_n": 292, "auc": 0.6363486842105264}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5494537331917906, "accuracy_n": 1902, "auc": 0.5494537331917906}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.502334842878279, "accuracy_n": 2000, "auc": 0.502334842878279}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.572829131652661, "accuracy_n": 59, "auc": 0.572829131652661}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5435435435435435, "accuracy_n": 999, "auc": 0.5435435435435435}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5893037336024218, "accuracy_n": 991, "auc": 0.5893037336024218}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2685370741482966, "accuracy_n": 499, "auc": 0.2685370741482966}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3253012048192771, "accuracy_n": 498, "auc": 0.3253012048192771}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.21, "accuracy_n": 400, "auc": 0.21}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.533402842137782, "accuracy_n": 322, "auc": 0.533402842137782}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6294407894736842, "accuracy_n": 292, "auc": 0.6294407894736842}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5452959129511676, "accuracy_n": 1902, "auc": 0.5452959129511676}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5032711808963035, "accuracy_n": 2000, "auc": 0.5032711808963035}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
