{"key": "result_0", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9246231155778895, "accuracy_n": 995, "auc": 0.9246231155778895}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8805220883534136, "accuracy_n": 996, "auc": 0.8805220883534136}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6570397111913358, "accuracy_n": 277, "auc": 0.6570397111913358}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6112224448897795, "accuracy_n": 998, "auc": 0.6112224448897795}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4225352112676056, "accuracy_n": 497, "auc": 0.4225352112676056}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.606425702811245, "accuracy_n": 498, "auc": 0.606425702811245}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4180602006688963, "accuracy_n": 299, "auc": 0.4180602006688963}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.506265664160401, "accuracy_n": 399, "auc": 0.506265664160401}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6176629595304295, "accuracy_n": 322, "auc": 0.6176629595304295}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.611795112781955, "accuracy_n": 292, "auc": 0.611795112781955}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.720873786407767, "accuracy_n": 412, "auc": 0.720873786407767}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6633448336871904, "accuracy_n": 1902, "auc": 0.6633448336871904}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5237385696236342, "accuracy_n": 2000, "auc": 0.5237385696236342}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.511204481792717, "accuracy_n": 59, "auc": 0.511204481792717}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9467336683417086, "accuracy_n": 995, "auc": 0.9467336683417086}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9156626506024096, "accuracy_n": 996, "auc": 0.9156626506024096}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5521042084168337, "accuracy_n": 998, "auc": 0.5521042084168337}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.47484909456740443, "accuracy_n": 497, "auc": 0.47484909456740443}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6144578313253012, "accuracy_n": 498, "auc": 0.6144578313253012}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4280936454849498, "accuracy_n": 299, "auc": 0.4280936454849498}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.49874686716791977, "accuracy_n": 399, "auc": 0.49874686716791977}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7128712871287128, "accuracy_n": 303, "auc": 0.7128712871287128}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7047806611059623, "accuracy_n": 322, "auc": 0.7047806611059623}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.606109022556391, "accuracy_n": 292, "auc": 0.606109022556391}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5315533980582524, "accuracy_n": 412, "auc": 0.5315533980582524}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7163548301486199, "accuracy_n": 1902, "auc": 0.7163548301486199}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5278245446606225, "accuracy_n": 2000, "auc": 0.5278245446606225}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5084033613445378, "accuracy_n": 59, "auc": 0.5084033613445378}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 23, "auc": 0.9}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9447236180904522, "accuracy_n": 995, "auc": 0.9447236180904522}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8192771084337349, "accuracy_n": 996, "auc": 0.8192771084337349}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7436823104693141, "accuracy_n": 277, "auc": 0.7436823104693141}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6162324649298597, "accuracy_n": 998, "auc": 0.6162324649298597}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.29577464788732394, "accuracy_n": 497, "auc": 0.29577464788732394}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5441767068273092, "accuracy_n": 498, "auc": 0.5441767068273092}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6270627062706271, "accuracy_n": 303, "auc": 0.6270627062706271}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5299274019153537, "accuracy_n": 322, "auc": 0.5299274019153537}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6363721804511279, "accuracy_n": 292, "auc": 0.6363721804511279}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7184466019417476, "accuracy_n": 412, "auc": 0.7184466019417476}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5809204706298655, "accuracy_n": 1902, "auc": 0.5809204706298655}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5139730442689812, "accuracy_n": 2000, "auc": 0.5139730442689812}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7587939698492462, "accuracy_n": 995, "auc": 0.7587939698492462}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.570281124497992, "accuracy_n": 996, "auc": 0.570281124497992}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6072144288577155, "accuracy_n": 998, "auc": 0.6072144288577155}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5412474849094567, "accuracy_n": 497, "auc": 0.5412474849094567}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6325301204819277, "accuracy_n": 498, "auc": 0.6325301204819277}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.46153846153846156, "accuracy_n": 299, "auc": 0.46153846153846156}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6591478696741855, "accuracy_n": 399, "auc": 0.6591478696741855}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5778112449799196, "accuracy_n": 322, "auc": 0.5778112449799196}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5011278195488722, "accuracy_n": 292, "auc": 0.5011278195488722}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8276699029126213, "accuracy_n": 412, "auc": 0.8276699029126213}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5203429095895259, "accuracy_n": 1902, "auc": 0.5203429095895259}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5252516158333158, "accuracy_n": 2000, "auc": 0.5252516158333158}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.511204481792717, "accuracy_n": 59, "auc": 0.511204481792717}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5883534136546185, "accuracy_n": 996, "auc": 0.5883534136546185}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6182364729458918, "accuracy_n": 998, "auc": 0.6182364729458918}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.38028169014084506, "accuracy_n": 497, "auc": 0.38028169014084506}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4357429718875502, "accuracy_n": 498, "auc": 0.4357429718875502}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3210702341137124, "accuracy_n": 299, "auc": 0.3210702341137124}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5955552981155391, "accuracy_n": 322, "auc": 0.5955552981155391}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5165883458646617, "accuracy_n": 292, "auc": 0.5165883458646617}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5485436893203883, "accuracy_n": 412, "auc": 0.5485436893203883}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5492027158527955, "accuracy_n": 1902, "auc": 0.5492027158527955}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5128986564149658, "accuracy_n": 2000, "auc": 0.5128986564149658}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.515406162464986, "accuracy_n": 59, "auc": 0.515406162464986}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5557788944723618, "accuracy_n": 995, "auc": 0.5557788944723618}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6355421686746988, "accuracy_n": 996, "auc": 0.6355421686746988}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 998, "auc": 0.6172344689378757}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6297786720321932, "accuracy_n": 497, "auc": 0.6297786720321932}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6365461847389559, "accuracy_n": 498, "auc": 0.6365461847389559}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6641604010025063, "accuracy_n": 399, "auc": 0.6641604010025063}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7776490577695396, "accuracy_n": 322, "auc": 0.7776490577695396}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6384398496240601, "accuracy_n": 292, "auc": 0.6384398496240601}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5339805825242718, "accuracy_n": 412, "auc": 0.5339805825242718}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5454109164897382, "accuracy_n": 1902, "auc": 0.5454109164897382}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5338992376247826, "accuracy_n": 2000, "auc": 0.5338992376247826}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6008403361344538, "accuracy_n": 59, "auc": 0.6008403361344538}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7457286432160805, "accuracy_n": 995, "auc": 0.7457286432160805}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6285140562248996, "accuracy_n": 996, "auc": 0.6285140562248996}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6132264529058116, "accuracy_n": 998, "auc": 0.6132264529058116}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 497, "auc": 0.6237424547283702}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6546184738955824, "accuracy_n": 498, "auc": 0.6546184738955824}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6020066889632107, "accuracy_n": 299, "auc": 0.6020066889632107}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6541353383458647, "accuracy_n": 399, "auc": 0.6541353383458647}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5874587458745875, "accuracy_n": 303, "auc": 0.5874587458745875}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6287843682421996, "accuracy_n": 322, "auc": 0.6287843682421996}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5129699248120301, "accuracy_n": 292, "auc": 0.5129699248120301}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.515373982661005, "accuracy_n": 1902, "auc": 0.515373982661005}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5115721775560977, "accuracy_n": 2000, "auc": 0.5115721775560977}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5447236180904522, "accuracy_n": 995, "auc": 0.5447236180904522}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5491967871485943, "accuracy_n": 996, "auc": 0.5491967871485943}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6102204408817635, "accuracy_n": 998, "auc": 0.6102204408817635}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 497, "auc": 0.6237424547283702}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6325301204819277, "accuracy_n": 498, "auc": 0.6325301204819277}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6641604010025063, "accuracy_n": 399, "auc": 0.6641604010025063}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.636963696369637, "accuracy_n": 303, "auc": 0.636963696369637}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7177170219338893, "accuracy_n": 322, "auc": 0.7177170219338893}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.631578947368421, "accuracy_n": 292, "auc": 0.631578947368421}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5461165048543689, "accuracy_n": 412, "auc": 0.5461165048543689}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5362106334041047, "accuracy_n": 1902, "auc": 0.5362106334041047}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5191018957843782, "accuracy_n": 2000, "auc": 0.5191018957843782}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5980392156862745, "accuracy_n": 59, "auc": 0.5980392156862745}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5829145728643216, "accuracy_n": 995, "auc": 0.5829145728643216}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.641566265060241, "accuracy_n": 996, "auc": 0.641566265060241}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5551102204408818, "accuracy_n": 998, "auc": 0.5551102204408818}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 497, "auc": 0.6237424547283702}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6506024096385542, "accuracy_n": 498, "auc": 0.6506024096385542}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5986622073578596, "accuracy_n": 299, "auc": 0.5986622073578596}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6591478696741855, "accuracy_n": 399, "auc": 0.6591478696741855}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6336633663366337, "accuracy_n": 303, "auc": 0.6336633663366337}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.690454124189064, "accuracy_n": 322, "auc": 0.690454124189064}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.531062030075188, "accuracy_n": 292, "auc": 0.531062030075188}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5242718446601942, "accuracy_n": 412, "auc": 0.5242718446601942}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5198370046001415, "accuracy_n": 1902, "auc": 0.5198370046001415}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5297942557263172, "accuracy_n": 2000, "auc": 0.5297942557263172}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5168067226890756, "accuracy_n": 59, "auc": 0.5168067226890756}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8100502512562814, "accuracy_n": 995, "auc": 0.8100502512562814}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7931726907630522, "accuracy_n": 996, "auc": 0.7931726907630522}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6182364729458918, "accuracy_n": 998, "auc": 0.6182364729458918}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2716297786720322, "accuracy_n": 497, "auc": 0.2716297786720322}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.39759036144578314, "accuracy_n": 498, "auc": 0.39759036144578314}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.411371237458194, "accuracy_n": 299, "auc": 0.411371237458194}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3533834586466165, "accuracy_n": 399, "auc": 0.3533834586466165}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7429718875502008, "accuracy_n": 322, "auc": 0.7429718875502008}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5261748120300752, "accuracy_n": 292, "auc": 0.5261748120300752}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6699029126213593, "accuracy_n": 412, "auc": 0.6699029126213593}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5123949486907289, "accuracy_n": 1902, "auc": 0.5123949486907289}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5790180255072082, "accuracy_n": 2000, "auc": 0.5790180255072082}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5644257703081232, "accuracy_n": 59, "auc": 0.5644257703081232}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8615384615384615, "accuracy_n": 23, "auc": 0.8615384615384615}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8603015075376884, "accuracy_n": 995, "auc": 0.8603015075376884}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7981927710843374, "accuracy_n": 996, "auc": 0.7981927710843374}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 998, "auc": 0.6152304609218436}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4647887323943662, "accuracy_n": 497, "auc": 0.4647887323943662}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4718875502008032, "accuracy_n": 498, "auc": 0.4718875502008032}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.46488294314381273, "accuracy_n": 299, "auc": 0.46488294314381273}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.45363408521303256, "accuracy_n": 399, "auc": 0.45363408521303256}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9207920792079208, "accuracy_n": 303, "auc": 0.9207920792079208}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7566033364226135, "accuracy_n": 322, "auc": 0.7566033364226135}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6343045112781955, "accuracy_n": 292, "auc": 0.6343045112781955}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.779126213592233, "accuracy_n": 412, "auc": 0.779126213592233}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6127565463552724, "accuracy_n": 1902, "auc": 0.6127565463552724}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5718049215766892, "accuracy_n": 2000, "auc": 0.5718049215766892}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7338935574229692, "accuracy_n": 59, "auc": 0.7338935574229692}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7135678391959799, "accuracy_n": 995, "auc": 0.7135678391959799}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7650602409638554, "accuracy_n": 996, "auc": 0.7650602409638554}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6202404809619239, "accuracy_n": 998, "auc": 0.6202404809619239}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 497, "auc": 0.5714285714285714}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4899598393574297, "accuracy_n": 498, "auc": 0.4899598393574297}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5083612040133779, "accuracy_n": 299, "auc": 0.5083612040133779}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5588972431077694, "accuracy_n": 399, "auc": 0.5588972431077694}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7425742574257426, "accuracy_n": 303, "auc": 0.7425742574257426}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9859051590979302, "accuracy_n": 322, "auc": 0.9859051590979302}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7425751879699249, "accuracy_n": 292, "auc": 0.7425751879699249}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8713592233009708, "accuracy_n": 412, "auc": 0.8713592233009708}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5180721426043878, "accuracy_n": 1902, "auc": 0.5180721426043878}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6377482271099867, "accuracy_n": 2000, "auc": 0.6377482271099867}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 59, "auc": 0.5294117647058824}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6150753768844222, "accuracy_n": 995, "auc": 0.6150753768844222}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5281124497991968, "accuracy_n": 996, "auc": 0.5281124497991968}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5703971119133574, "accuracy_n": 277, "auc": 0.5703971119133574}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6212424849699398, "accuracy_n": 998, "auc": 0.6212424849699398}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.49295774647887325, "accuracy_n": 497, "auc": 0.49295774647887325}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.36789297658862874, "accuracy_n": 299, "auc": 0.36789297658862874}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.39097744360902253, "accuracy_n": 399, "auc": 0.39097744360902253}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7800818659252393, "accuracy_n": 322, "auc": 0.7800818659252393}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6180451127819548, "accuracy_n": 292, "auc": 0.6180451127819548}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7888349514563107, "accuracy_n": 412, "auc": 0.7888349514563107}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5272105007077141, "accuracy_n": 1902, "auc": 0.5272105007077141}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5339282480975632, "accuracy_n": 2000, "auc": 0.5339282480975632}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5560224089635855, "accuracy_n": 59, "auc": 0.5560224089635855}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7889447236180904, "accuracy_n": 995, "auc": 0.7889447236180904}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5732931726907631, "accuracy_n": 996, "auc": 0.5732931726907631}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5581162324649298, "accuracy_n": 998, "auc": 0.5581162324649298}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2837022132796781, "accuracy_n": 497, "auc": 0.2837022132796781}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2709030100334448, "accuracy_n": 299, "auc": 0.2709030100334448}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.19548872180451127, "accuracy_n": 399, "auc": 0.19548872180451127}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6765676567656765, "accuracy_n": 303, "auc": 0.6765676567656765}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6764558232931728, "accuracy_n": 322, "auc": 0.6764558232931728}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5169642857142857, "accuracy_n": 292, "auc": 0.5169642857142857}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9805825242718447, "accuracy_n": 412, "auc": 0.9805825242718447}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5126669762915782, "accuracy_n": 1902, "auc": 0.5126669762915782}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5450252541167361, "accuracy_n": 2000, "auc": 0.5450252541167361}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5350140056022409, "accuracy_n": 59, "auc": 0.5350140056022409}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8241206030150754, "accuracy_n": 995, "auc": 0.8241206030150754}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8112449799196787, "accuracy_n": 996, "auc": 0.8112449799196787}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6182364729458918, "accuracy_n": 998, "auc": 0.6182364729458918}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3501006036217304, "accuracy_n": 497, "auc": 0.3501006036217304}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.26305220883534136, "accuracy_n": 498, "auc": 0.26305220883534136}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2882205513784461, "accuracy_n": 399, "auc": 0.2882205513784461}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51233781278962, "accuracy_n": 322, "auc": 0.51233781278962}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5374295112781955, "accuracy_n": 292, "auc": 0.5374295112781955}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6262135922330098, "accuracy_n": 412, "auc": 0.6262135922330098}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9348095806794056, "accuracy_n": 1902, "auc": 0.9348095806794056}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5064018110537905, "accuracy_n": 2000, "auc": 0.5064018110537905}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.592436974789916, "accuracy_n": 59, "auc": 0.592436974789916}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5025125628140703, "accuracy_n": 995, "auc": 0.5025125628140703}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6014056224899599, "accuracy_n": 996, "auc": 0.6014056224899599}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6182364729458918, "accuracy_n": 998, "auc": 0.6182364729458918}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3762575452716298, "accuracy_n": 497, "auc": 0.3762575452716298}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.26104417670682734, "accuracy_n": 498, "auc": 0.26104417670682734}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4180602006688963, "accuracy_n": 299, "auc": 0.4180602006688963}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.693069306930693, "accuracy_n": 303, "auc": 0.693069306930693}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.654772937905468, "accuracy_n": 322, "auc": 0.654772937905468}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.678453947368421, "accuracy_n": 292, "auc": 0.678453947368421}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8276699029126213, "accuracy_n": 412, "auc": 0.8276699029126213}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6038481953290871, "accuracy_n": 1902, "auc": 0.6038481953290871}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5148053447294474, "accuracy_n": 2000, "auc": 0.5148053447294474}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.592436974789916, "accuracy_n": 59, "auc": 0.592436974789916}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5447236180904522, "accuracy_n": 995, "auc": 0.5447236180904522}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6746987951807228, "accuracy_n": 996, "auc": 0.6746987951807228}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 998, "auc": 0.6192384769539078}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3782696177062374, "accuracy_n": 497, "auc": 0.3782696177062374}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.28313253012048195, "accuracy_n": 498, "auc": 0.28313253012048195}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.391304347826087, "accuracy_n": 299, "auc": 0.391304347826087}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2957393483709273, "accuracy_n": 399, "auc": 0.2957393483709273}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.594059405940594, "accuracy_n": 303, "auc": 0.594059405940594}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5672690763052208, "accuracy_n": 322, "auc": 0.5672690763052208}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6339050751879699, "accuracy_n": 292, "auc": 0.6339050751879699}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7184466019417476, "accuracy_n": 412, "auc": 0.7184466019417476}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6066690994338287, "accuracy_n": 1902, "auc": 0.6066690994338287}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5006372300400446, "accuracy_n": 2000, "auc": 0.5006372300400446}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5756302521008403, "accuracy_n": 59, "auc": 0.5756302521008403}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5346733668341709, "accuracy_n": 995, "auc": 0.5346733668341709}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6606425702811245, "accuracy_n": 996, "auc": 0.6606425702811245}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 998, "auc": 0.6192384769539078}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3199195171026157, "accuracy_n": 497, "auc": 0.3199195171026157}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.25502008032128515, "accuracy_n": 498, "auc": 0.25502008032128515}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3277591973244147, "accuracy_n": 299, "auc": 0.3277591973244147}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.37844611528822053, "accuracy_n": 399, "auc": 0.37844611528822053}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5315299660179178, "accuracy_n": 322, "auc": 0.5315299660179178}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6019266917293233, "accuracy_n": 292, "auc": 0.6019266917293233}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6043689320388349, "accuracy_n": 412, "auc": 0.6043689320388349}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6017294762915781, "accuracy_n": 1902, "auc": 0.6017294762915781}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5062162440641071, "accuracy_n": 2000, "auc": 0.5062162440641071}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5784313725490196, "accuracy_n": 59, "auc": 0.5784313725490196}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
