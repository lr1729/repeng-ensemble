{"key": "result_0", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9488465396188566, "accuracy_n": 997, "auc": 0.9488465396188566}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.907537688442211, "accuracy_n": 995, "auc": 0.907537688442211}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5478348439073515, "accuracy_n": 993, "auc": 0.5478348439073515}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.46680080482897385, "accuracy_n": 497, "auc": 0.46680080482897385}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.494, "accuracy_n": 500, "auc": 0.494}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.44107744107744107, "accuracy_n": 297, "auc": 0.44107744107744107}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5527638190954773, "accuracy_n": 398, "auc": 0.5527638190954773}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7029702970297029, "accuracy_n": 303, "auc": 0.7029702970297029}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5670759962928638, "accuracy_n": 322, "auc": 0.5670759962928638}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5184680451127819, "accuracy_n": 292, "auc": 0.5184680451127819}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6430102176220807, "accuracy_n": 1902, "auc": 0.6430102176220807}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5129256661654857, "accuracy_n": 2000, "auc": 0.5129256661654857}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5770308123249299, "accuracy_n": 59, "auc": 0.5770308123249299}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8676028084252758, "accuracy_n": 997, "auc": 0.8676028084252758}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8673366834170855, "accuracy_n": 995, "auc": 0.8673366834170855}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3440643863179074, "accuracy_n": 497, "auc": 0.3440643863179074}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.392, "accuracy_n": 500, "auc": 0.392}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.37037037037037035, "accuracy_n": 297, "auc": 0.37037037037037035}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49246231155778897, "accuracy_n": 398, "auc": 0.49246231155778897}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6270627062706271, "accuracy_n": 303, "auc": 0.6270627062706271}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5226289774482545, "accuracy_n": 322, "auc": 0.5226289774482545}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5031954887218045, "accuracy_n": 292, "auc": 0.5031954887218045}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6294475406935598, "accuracy_n": 1902, "auc": 0.6294475406935598}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5032046568811341, "accuracy_n": 2000, "auc": 0.5032046568811341}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 59, "auc": 0.5294117647058824}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.872617853560682, "accuracy_n": 997, "auc": 0.872617853560682}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6432160804020101, "accuracy_n": 995, "auc": 0.6432160804020101}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2558922558922559, "accuracy_n": 297, "auc": 0.2558922558922559}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3391959798994975, "accuracy_n": 398, "auc": 0.3391959798994975}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6027957985789312, "accuracy_n": 322, "auc": 0.6027957985789312}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5091165413533834, "accuracy_n": 292, "auc": 0.5091165413533834}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5399515738498789, "accuracy_n": 413, "auc": 0.5399515738498789}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5332360226468507, "accuracy_n": 1902, "auc": 0.5332360226468507}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5062087413556293, "accuracy_n": 2000, "auc": 0.5062087413556293}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5206030150753769, "accuracy_n": 995, "auc": 0.5206030150753769}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5740072202166066, "accuracy_n": 277, "auc": 0.5740072202166066}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5709969788519638, "accuracy_n": 993, "auc": 0.5709969788519638}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.36217303822937624, "accuracy_n": 497, "auc": 0.36217303822937624}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.436, "accuracy_n": 500, "auc": 0.436}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.39057239057239057, "accuracy_n": 297, "auc": 0.39057239057239057}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.41959798994974873, "accuracy_n": 398, "auc": 0.41959798994974873}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6270627062706271, "accuracy_n": 303, "auc": 0.6270627062706271}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5564179796107507, "accuracy_n": 322, "auc": 0.5564179796107507}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5182330827067669, "accuracy_n": 292, "auc": 0.5182330827067669}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5375302663438256, "accuracy_n": 413, "auc": 0.5375302663438256}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5048207492922858, "accuracy_n": 1902, "auc": 0.5048207492922858}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5008137937795545, "accuracy_n": 2000, "auc": 0.5008137937795545}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5145436308926781, "accuracy_n": 997, "auc": 0.5145436308926781}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.28619528619528617, "accuracy_n": 297, "auc": 0.28619528619528617}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2613065326633166, "accuracy_n": 398, "auc": 0.2613065326633166}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5643564356435643, "accuracy_n": 303, "auc": 0.5643564356435643}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5639674080939141, "accuracy_n": 322, "auc": 0.5639674080939141}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5358082706766917, "accuracy_n": 292, "auc": 0.5358082706766917}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5542982572540693, "accuracy_n": 1902, "auc": 0.5542982572540693}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.507903853291038, "accuracy_n": 2000, "auc": 0.507903853291038}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5295887662988967, "accuracy_n": 997, "auc": 0.5295887662988967}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5407035175879397, "accuracy_n": 995, "auc": 0.5407035175879397}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.525679758308157, "accuracy_n": 993, "auc": 0.525679758308157}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5472837022132797, "accuracy_n": 497, "auc": 0.5472837022132797}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.466, "accuracy_n": 500, "auc": 0.466}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48484848484848486, "accuracy_n": 297, "auc": 0.48484848484848486}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6180904522613065, "accuracy_n": 398, "auc": 0.6180904522613065}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6850092678405932, "accuracy_n": 322, "auc": 0.6850092678405932}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5766917293233083, "accuracy_n": 292, "auc": 0.5766917293233083}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6295399515738499, "accuracy_n": 413, "auc": 0.6295399515738499}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5077627388535031, "accuracy_n": 1902, "auc": 0.5077627388535031}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5504537137906785, "accuracy_n": 2000, "auc": 0.5504537137906785}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5896358543417366, "accuracy_n": 59, "auc": 0.5896358543417366}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59679037111334, "accuracy_n": 997, "auc": 0.59679037111334}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5557788944723618, "accuracy_n": 995, "auc": 0.5557788944723618}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5589123867069486, "accuracy_n": 993, "auc": 0.5589123867069486}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5231388329979879, "accuracy_n": 497, "auc": 0.5231388329979879}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.49, "accuracy_n": 500, "auc": 0.49}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.46464646464646464, "accuracy_n": 297, "auc": 0.46464646464646464}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.592964824120603, "accuracy_n": 398, "auc": 0.592964824120603}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6848934198331789, "accuracy_n": 322, "auc": 0.6848934198331789}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5293233082706768, "accuracy_n": 292, "auc": 0.5293233082706768}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.559322033898305, "accuracy_n": 413, "auc": 0.559322033898305}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5096392869780608, "accuracy_n": 1902, "auc": 0.5096392869780608}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5055570060791946, "accuracy_n": 2000, "auc": 0.5055570060791946}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5798319327731092, "accuracy_n": 59, "auc": 0.5798319327731092}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7111334002006018, "accuracy_n": 997, "auc": 0.7111334002006018}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5768844221105528, "accuracy_n": 995, "auc": 0.5768844221105528}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5196374622356495, "accuracy_n": 993, "auc": 0.5196374622356495}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5271629778672032, "accuracy_n": 497, "auc": 0.5271629778672032}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.562, "accuracy_n": 500, "auc": 0.562}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.47474747474747475, "accuracy_n": 297, "auc": 0.47474747474747475}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5979899497487438, "accuracy_n": 398, "auc": 0.5979899497487438}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.76328390485017, "accuracy_n": 322, "auc": 0.76328390485017}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6024436090225564, "accuracy_n": 292, "auc": 0.6024436090225564}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6343825665859564, "accuracy_n": 413, "auc": 0.6343825665859564}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5014331210191083, "accuracy_n": 1902, "auc": 0.5014331210191083}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5369683455727517, "accuracy_n": 2000, "auc": 0.5369683455727517}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5406162464985994, "accuracy_n": 59, "auc": 0.5406162464985994}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7041123370110332, "accuracy_n": 997, "auc": 0.7041123370110332}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5608040201005026, "accuracy_n": 995, "auc": 0.5608040201005026}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5599194360523666, "accuracy_n": 993, "auc": 0.5599194360523666}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5171026156941649, "accuracy_n": 497, "auc": 0.5171026156941649}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.534, "accuracy_n": 500, "auc": 0.534}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.48484848484848486, "accuracy_n": 297, "auc": 0.48484848484848486}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6130653266331658, "accuracy_n": 398, "auc": 0.6130653266331658}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7424698795180723, "accuracy_n": 322, "auc": 0.7424698795180723}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.66203007518797, "accuracy_n": 292, "auc": 0.66203007518797}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6101694915254238, "accuracy_n": 413, "auc": 0.6101694915254238}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5107782643312102, "accuracy_n": 1902, "auc": 0.5107782643312102}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5908382926236371, "accuracy_n": 2000, "auc": 0.5908382926236371}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.726890756302521, "accuracy_n": 59, "auc": 0.726890756302521}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8084252758274825, "accuracy_n": 997, "auc": 0.8084252758274825}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5226130653266332, "accuracy_n": 995, "auc": 0.5226130653266332}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5427995971802618, "accuracy_n": 993, "auc": 0.5427995971802618}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3641851106639839, "accuracy_n": 497, "auc": 0.3641851106639839}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.30303030303030304, "accuracy_n": 297, "auc": 0.30303030303030304}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3316582914572864, "accuracy_n": 398, "auc": 0.3316582914572864}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7458745874587459, "accuracy_n": 303, "auc": 0.7458745874587459}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7255946864380598, "accuracy_n": 322, "auc": 0.7255946864380598}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5364661654135338, "accuracy_n": 292, "auc": 0.5364661654135338}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 413, "auc": 0.7142857142857143}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5158726999292286, "accuracy_n": 1902, "auc": 0.5158726999292286}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5242127407994286, "accuracy_n": 2000, "auc": 0.5242127407994286}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5784313725490197, "accuracy_n": 59, "auc": 0.5784313725490197}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5626880641925778, "accuracy_n": 997, "auc": 0.5626880641925778}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6271356783919598, "accuracy_n": 995, "auc": 0.6271356783919598}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3380281690140845, "accuracy_n": 497, "auc": 0.3380281690140845}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.342, "accuracy_n": 500, "auc": 0.342}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.31313131313131315, "accuracy_n": 297, "auc": 0.31313131313131315}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.27889447236180903, "accuracy_n": 398, "auc": 0.27889447236180903}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6996699669966997, "accuracy_n": 303, "auc": 0.6996699669966997}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7232391102873031, "accuracy_n": 322, "auc": 0.7232391102873031}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5739661654135338, "accuracy_n": 292, "auc": 0.5739661654135338}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5090377300070772, "accuracy_n": 1902, "auc": 0.5090377300070772}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5255487230890352, "accuracy_n": 2000, "auc": 0.5255487230890352}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5145436308926781, "accuracy_n": 997, "auc": 0.5145436308926781}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5185929648241207, "accuracy_n": 995, "auc": 0.5185929648241207}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.527693856998993, "accuracy_n": 993, "auc": 0.527693856998993}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4869215291750503, "accuracy_n": 497, "auc": 0.4869215291750503}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.388, "accuracy_n": 500, "auc": 0.388}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4107744107744108, "accuracy_n": 297, "auc": 0.4107744107744108}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6055276381909548, "accuracy_n": 398, "auc": 0.6055276381909548}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6765676567656765, "accuracy_n": 303, "auc": 0.6765676567656765}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6700262588816804, "accuracy_n": 322, "auc": 0.6700262588816804}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6522556390977443, "accuracy_n": 292, "auc": 0.6522556390977443}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5631004511677282, "accuracy_n": 1902, "auc": 0.5631004511677282}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5229372803582093, "accuracy_n": 2000, "auc": 0.5229372803582093}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.603641456582633, "accuracy_n": 59, "auc": 0.603641456582633}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5446339017051154, "accuracy_n": 997, "auc": 0.5446339017051154}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528643216080402, "accuracy_n": 995, "auc": 0.528643216080402}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3702213279678068, "accuracy_n": 497, "auc": 0.3702213279678068}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.376, "accuracy_n": 500, "auc": 0.376}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.367003367003367, "accuracy_n": 297, "auc": 0.367003367003367}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.4648241206030151, "accuracy_n": 398, "auc": 0.4648241206030151}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6515678097003398, "accuracy_n": 322, "auc": 0.6515678097003398}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5437734962406015, "accuracy_n": 292, "auc": 0.5437734962406015}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6561743341404358, "accuracy_n": 413, "auc": 0.6561743341404358}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5014917285916489, "accuracy_n": 1902, "auc": 0.5014917285916489}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5060831960337682, "accuracy_n": 2000, "auc": 0.5060831960337682}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5105315947843531, "accuracy_n": 997, "auc": 0.5105315947843531}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5597989949748744, "accuracy_n": 995, "auc": 0.5597989949748744}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5709969788519638, "accuracy_n": 993, "auc": 0.5709969788519638}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.41851106639839036, "accuracy_n": 497, "auc": 0.41851106639839036}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.346, "accuracy_n": 500, "auc": 0.346}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.3468013468013468, "accuracy_n": 297, "auc": 0.3468013468013468}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.45226130653266333, "accuracy_n": 398, "auc": 0.45226130653266333}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5841056533827618, "accuracy_n": 322, "auc": 0.5841056533827618}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5178101503759398, "accuracy_n": 292, "auc": 0.5178101503759398}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.7046004842615012, "accuracy_n": 413, "auc": 0.7046004842615012}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5559901362349611, "accuracy_n": 1902, "auc": 0.5559901362349611}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5015450577658536, "accuracy_n": 2000, "auc": 0.5015450577658536}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6028084252758275, "accuracy_n": 997, "auc": 0.6028084252758275}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5587939698492462, "accuracy_n": 995, "auc": 0.5587939698492462}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6022155085599195, "accuracy_n": 993, "auc": 0.6022155085599195}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.28772635814889336, "accuracy_n": 497, "auc": 0.28772635814889336}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.24579124579124578, "accuracy_n": 297, "auc": 0.24579124579124578}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.18090452261306533, "accuracy_n": 398, "auc": 0.18090452261306533}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5660140562248995, "accuracy_n": 322, "auc": 0.5660140562248995}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5400845864661654, "accuracy_n": 292, "auc": 0.5400845864661654}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.920196390658174, "accuracy_n": 1902, "auc": 0.920196390658174}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5046261700473871, "accuracy_n": 2000, "auc": 0.5046261700473871}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5576730190571715, "accuracy_n": 997, "auc": 0.5576730190571715}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5376884422110553, "accuracy_n": 995, "auc": 0.5376884422110553}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 497, "auc": 0.2857142857142857}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.33, "accuracy_n": 500, "auc": 0.33}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2727272727272727, "accuracy_n": 297, "auc": 0.2727272727272727}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.30402010050251255, "accuracy_n": 398, "auc": 0.30402010050251255}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6171617161716172, "accuracy_n": 303, "auc": 0.6171617161716172}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5326305220883534, "accuracy_n": 322, "auc": 0.5326305220883534}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862077067669174, "accuracy_n": 292, "auc": 0.5862077067669174}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5302160739561217, "accuracy_n": 1902, "auc": 0.5302160739561217}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5192669553708888, "accuracy_n": 2000, "auc": 0.5192669553708888}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6022408963585435, "accuracy_n": 59, "auc": 0.6022408963585435}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5205616850551655, "accuracy_n": 997, "auc": 0.5205616850551655}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5326633165829145, "accuracy_n": 995, "auc": 0.5326633165829145}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.34, "accuracy_n": 500, "auc": 0.34}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2828282828282828, "accuracy_n": 297, "auc": 0.2828282828282828}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.24874371859296482, "accuracy_n": 398, "auc": 0.24874371859296482}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5171455050973124, "accuracy_n": 322, "auc": 0.5171455050973124}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5567904135338346, "accuracy_n": 292, "auc": 0.5567904135338346}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5231754246284501, "accuracy_n": 1902, "auc": 0.5231754246284501}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5085750956095151, "accuracy_n": 2000, "auc": 0.5085750956095151}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5406218655967904, "accuracy_n": 997, "auc": 0.5406218655967904}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5336683417085427, "accuracy_n": 995, "auc": 0.5336683417085427}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2937625754527163, "accuracy_n": 497, "auc": 0.2937625754527163}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.328, "accuracy_n": 500, "auc": 0.328}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.2828282828282828, "accuracy_n": 297, "auc": 0.2828282828282828}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5322829780661107, "accuracy_n": 322, "auc": 0.5322829780661107}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.550516917293233, "accuracy_n": 292, "auc": 0.550516917293233}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.52253627034678, "accuracy_n": 1902, "auc": 0.52253627034678}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5071465799153495, "accuracy_n": 2000, "auc": 0.5071465799153495}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5924369747899161, "accuracy_n": 59, "auc": 0.5924369747899161}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h23", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
