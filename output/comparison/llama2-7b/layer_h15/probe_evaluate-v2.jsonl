{"key": "result_0", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9207622868605817, "accuracy_n": 997, "auc": 0.9207622868605817}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8723618090452261, "accuracy_n": 995, "auc": 0.8723618090452261}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5961732124874118, "accuracy_n": 993, "auc": 0.5961732124874118}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.44064386317907445, "accuracy_n": 497, "auc": 0.44064386317907445}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.484, "accuracy_n": 500, "auc": 0.484}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4175084175084175, "accuracy_n": 297, "auc": 0.4175084175084175}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5778894472361809, "accuracy_n": 398, "auc": 0.5778894472361809}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5967330861909175, "accuracy_n": 322, "auc": 0.5967330861909175}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.503641917293233, "accuracy_n": 292, "auc": 0.503641917293233}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5447941888619855, "accuracy_n": 413, "auc": 0.5447941888619855}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.631646983368719, "accuracy_n": 1902, "auc": 0.631646983368719}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5006847471937369, "accuracy_n": 2000, "auc": 0.5006847471937369}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5812324929971988, "accuracy_n": 59, "auc": 0.5812324929971988}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.925777331995988, "accuracy_n": 997, "auc": 0.925777331995988}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8733668341708543, "accuracy_n": 995, "auc": 0.8733668341708543}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6072507552870091, "accuracy_n": 993, "auc": 0.6072507552870091}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39235412474849096, "accuracy_n": 497, "auc": 0.39235412474849096}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.498, "accuracy_n": 500, "auc": 0.498}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39730639730639733, "accuracy_n": 297, "auc": 0.39730639730639733}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5753768844221105, "accuracy_n": 398, "auc": 0.5753768844221105}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6949335804757492, "accuracy_n": 322, "auc": 0.6949335804757492}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.574389097744361, "accuracy_n": 292, "auc": 0.574389097744361}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7654690817409766, "accuracy_n": 1902, "auc": 0.7654690817409766}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5242877678842062, "accuracy_n": 2000, "auc": 0.5242877678842062}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5378151260504201, "accuracy_n": 59, "auc": 0.5378151260504201}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7923076923076923, "accuracy_n": 23, "auc": 0.7923076923076923}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.708124373119358, "accuracy_n": 997, "auc": 0.708124373119358}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7035175879396985, "accuracy_n": 995, "auc": 0.7035175879396985}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.43661971830985913, "accuracy_n": 497, "auc": 0.43661971830985913}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4, "accuracy_n": 500, "auc": 0.4}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39057239057239057, "accuracy_n": 297, "auc": 0.39057239057239057}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5276381909547738, "accuracy_n": 398, "auc": 0.5276381909547738}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5754170528266914, "accuracy_n": 322, "auc": 0.5754170528266914}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6507988721804512, "accuracy_n": 292, "auc": 0.6507988721804512}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6901307059447983, "accuracy_n": 1902, "auc": 0.6901307059447983}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5220914750224832, "accuracy_n": 2000, "auc": 0.5220914750224832}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5392156862745098, "accuracy_n": 59, "auc": 0.5392156862745098}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7753259779338014, "accuracy_n": 997, "auc": 0.7753259779338014}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5226130653266332, "accuracy_n": 995, "auc": 0.5226130653266332}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5191146881287726, "accuracy_n": 497, "auc": 0.5191146881287726}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.452, "accuracy_n": 500, "auc": 0.452}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4612794612794613, "accuracy_n": 297, "auc": 0.4612794612794613}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6055276381909548, "accuracy_n": 398, "auc": 0.6055276381909548}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6336633663366337, "accuracy_n": 303, "auc": 0.6336633663366337}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5406626506024097, "accuracy_n": 322, "auc": 0.5406626506024097}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.53125, "accuracy_n": 292, "auc": 0.53125}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5609927901627743, "accuracy_n": 1902, "auc": 0.5609927901627743}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5084750594964782, "accuracy_n": 2000, "auc": 0.5084750594964782}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5826330532212886, "accuracy_n": 59, "auc": 0.5826330532212886}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5145436308926781, "accuracy_n": 997, "auc": 0.5145436308926781}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5246231155778894, "accuracy_n": 995, "auc": 0.5246231155778894}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39235412474849096, "accuracy_n": 497, "auc": 0.39235412474849096}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.414, "accuracy_n": 500, "auc": 0.414}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4377104377104377, "accuracy_n": 297, "auc": 0.4377104377104377}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.550251256281407, "accuracy_n": 398, "auc": 0.550251256281407}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5322057460611678, "accuracy_n": 322, "auc": 0.5322057460611678}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5132988721804512, "accuracy_n": 292, "auc": 0.5132988721804512}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6106544143665958, "accuracy_n": 1902, "auc": 0.6106544143665958}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5241842305072131, "accuracy_n": 2000, "auc": 0.5241842305072131}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5322128851540616, "accuracy_n": 59, "auc": 0.5322128851540616}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7301905717151455, "accuracy_n": 997, "auc": 0.7301905717151455}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5889447236180905, "accuracy_n": 995, "auc": 0.5889447236180905}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6183282980866063, "accuracy_n": 993, "auc": 0.6183282980866063}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5774647887323944, "accuracy_n": 497, "auc": 0.5774647887323944}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.532, "accuracy_n": 500, "auc": 0.532}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5084175084175084, "accuracy_n": 297, "auc": 0.5084175084175084}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6206030150753769, "accuracy_n": 398, "auc": 0.6206030150753769}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6155197713932654, "accuracy_n": 322, "auc": 0.6155197713932654}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6524906015037594, "accuracy_n": 292, "auc": 0.6524906015037594}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 413, "auc": 0.7142857142857143}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6129633315640481, "accuracy_n": 1902, "auc": 0.6129633315640481}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5281471611251661, "accuracy_n": 2000, "auc": 0.5281471611251661}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8996990972918756, "accuracy_n": 997, "auc": 0.8996990972918756}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8753768844221106, "accuracy_n": 995, "auc": 0.8753768844221106}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6203423967774421, "accuracy_n": 993, "auc": 0.6203423967774421}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 497, "auc": 0.5553319919517102}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.598, "accuracy_n": 500, "auc": 0.598}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.494949494949495, "accuracy_n": 297, "auc": 0.494949494949495}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6231155778894473, "accuracy_n": 398, "auc": 0.6231155778894473}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5544554455445545, "accuracy_n": 303, "auc": 0.5544554455445545}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6911492122335495, "accuracy_n": 322, "auc": 0.6911492122335495}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6588345864661656, "accuracy_n": 292, "auc": 0.6588345864661656}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6610169491525424, "accuracy_n": 413, "auc": 0.6610169491525424}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.566902202760085, "accuracy_n": 1902, "auc": 0.566902202760085}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5563593457238063, "accuracy_n": 2000, "auc": 0.5563593457238063}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6106442577030812, "accuracy_n": 59, "auc": 0.6106442577030812}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.905717151454363, "accuracy_n": 997, "auc": 0.905717151454363}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6603015075376885, "accuracy_n": 995, "auc": 0.6603015075376885}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5848375451263538, "accuracy_n": 277, "auc": 0.5848375451263538}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5633802816901409, "accuracy_n": 497, "auc": 0.5633802816901409}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.562, "accuracy_n": 500, "auc": 0.562}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5218855218855218, "accuracy_n": 297, "auc": 0.5218855218855218}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6381909547738693, "accuracy_n": 398, "auc": 0.6381909547738693}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5676567656765676, "accuracy_n": 303, "auc": 0.5676567656765676}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7963005869632376, "accuracy_n": 322, "auc": 0.7963005869632376}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7449248120300752, "accuracy_n": 292, "auc": 0.7449248120300752}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6949152542372882, "accuracy_n": 413, "auc": 0.6949152542372882}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5926751592356688, "accuracy_n": 1902, "auc": 0.5926751592356688}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5773709309060571, "accuracy_n": 2000, "auc": 0.5773709309060571}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6148459383753502, "accuracy_n": 59, "auc": 0.6148459383753502}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8224674022066198, "accuracy_n": 997, "auc": 0.8224674022066198}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8090452261306532, "accuracy_n": 995, "auc": 0.8090452261306532}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5513078470824949, "accuracy_n": 497, "auc": 0.5513078470824949}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.532, "accuracy_n": 500, "auc": 0.532}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.468013468013468, "accuracy_n": 297, "auc": 0.468013468013468}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6231155778894473, "accuracy_n": 398, "auc": 0.6231155778894473}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7260194624652456, "accuracy_n": 322, "auc": 0.7260194624652456}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7038533834586467, "accuracy_n": 292, "auc": 0.7038533834586467}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7893462469733656, "accuracy_n": 413, "auc": 0.7893462469733656}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5595430820948337, "accuracy_n": 1902, "auc": 0.5595430820948337}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5704464311616494, "accuracy_n": 2000, "auc": 0.5704464311616494}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6344537815126051, "accuracy_n": 59, "auc": 0.6344537815126051}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.551654964894684, "accuracy_n": 997, "auc": 0.551654964894684}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5105527638190954, "accuracy_n": 995, "auc": 0.5105527638190954}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.552870090634441, "accuracy_n": 993, "auc": 0.552870090634441}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.44668008048289737, "accuracy_n": 497, "auc": 0.44668008048289737}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.474, "accuracy_n": 500, "auc": 0.474}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39730639730639733, "accuracy_n": 297, "auc": 0.39730639730639733}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5477386934673367, "accuracy_n": 398, "auc": 0.5477386934673367}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6567656765676567, "accuracy_n": 303, "auc": 0.6567656765676567}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.672343219029966, "accuracy_n": 322, "auc": 0.672343219029966}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5115601503759398, "accuracy_n": 292, "auc": 0.5115601503759398}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8135593220338984, "accuracy_n": 413, "auc": 0.8135593220338984}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5055898354564756, "accuracy_n": 1902, "auc": 0.5055898354564756}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5112215509799037, "accuracy_n": 2000, "auc": 0.5112215509799037}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8307692307692307, "accuracy_n": 23, "auc": 0.8307692307692307}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.905717151454363, "accuracy_n": 997, "auc": 0.905717151454363}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.628140703517588, "accuracy_n": 995, "auc": 0.628140703517588}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6498194945848376, "accuracy_n": 277, "auc": 0.6498194945848376}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6183282980866063, "accuracy_n": 993, "auc": 0.6183282980866063}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4305835010060362, "accuracy_n": 497, "auc": 0.4305835010060362}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.44, "accuracy_n": 500, "auc": 0.44}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.44107744107744107, "accuracy_n": 297, "auc": 0.44107744107744107}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.507537688442211, "accuracy_n": 398, "auc": 0.507537688442211}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.679989187519308, "accuracy_n": 322, "auc": 0.679989187519308}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.619031954887218, "accuracy_n": 292, "auc": 0.619031954887218}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5932203389830508, "accuracy_n": 413, "auc": 0.5932203389830508}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5033726999292285, "accuracy_n": 1902, "auc": 0.5033726999292285}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5094409081678486, "accuracy_n": 2000, "auc": 0.5094409081678486}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5070028011204482, "accuracy_n": 59, "auc": 0.5070028011204482}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5526579739217653, "accuracy_n": 997, "auc": 0.5526579739217653}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6080402010050251, "accuracy_n": 995, "auc": 0.6080402010050251}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6122860020140987, "accuracy_n": 993, "auc": 0.6122860020140987}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.47283702213279677, "accuracy_n": 497, "auc": 0.47283702213279677}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39, "accuracy_n": 500, "auc": 0.39}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.468013468013468, "accuracy_n": 297, "auc": 0.468013468013468}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5778894472361809, "accuracy_n": 398, "auc": 0.5778894472361809}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5469763670064876, "accuracy_n": 322, "auc": 0.5469763670064876}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6156954887218045, "accuracy_n": 292, "auc": 0.6156954887218045}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6876513317191283, "accuracy_n": 413, "auc": 0.6876513317191283}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6876404370134465, "accuracy_n": 1902, "auc": 0.6876404370134465}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5179824916794964, "accuracy_n": 2000, "auc": 0.5179824916794964}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.603641456582633, "accuracy_n": 59, "auc": 0.603641456582633}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5195586760280843, "accuracy_n": 997, "auc": 0.5195586760280843}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5587939698492462, "accuracy_n": 995, "auc": 0.5587939698492462}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5609264853977844, "accuracy_n": 993, "auc": 0.5609264853977844}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.324, "accuracy_n": 500, "auc": 0.324}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4208754208754209, "accuracy_n": 297, "auc": 0.4208754208754209}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.32914572864321606, "accuracy_n": 398, "auc": 0.32914572864321606}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6426668211306765, "accuracy_n": 322, "auc": 0.6426668211306765}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5393092105263159, "accuracy_n": 292, "auc": 0.5393092105263159}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6949152542372882, "accuracy_n": 413, "auc": 0.6949152542372882}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5997346072186837, "accuracy_n": 1902, "auc": 0.5997346072186837}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5077988153723494, "accuracy_n": 2000, "auc": 0.5077988153723494}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5854341736694677, "accuracy_n": 59, "auc": 0.5854341736694677}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8264794383149449, "accuracy_n": 997, "auc": 0.8264794383149449}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5336683417085427, "accuracy_n": 995, "auc": 0.5336683417085427}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4768611670020121, "accuracy_n": 497, "auc": 0.4768611670020121}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.438, "accuracy_n": 500, "auc": 0.438}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.45454545454545453, "accuracy_n": 297, "auc": 0.45454545454545453}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5678391959798995, "accuracy_n": 398, "auc": 0.5678391959798995}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5577557755775577, "accuracy_n": 303, "auc": 0.5577557755775577}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 322, "auc": 0.6164658634538153}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5166118421052632, "accuracy_n": 292, "auc": 0.5166118421052632}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8595641646489104, "accuracy_n": 413, "auc": 0.8595641646489104}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5608390835102619, "accuracy_n": 1902, "auc": 0.5608390835102619}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036278096392797, "accuracy_n": 2000, "auc": 0.5036278096392797}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5315947843530592, "accuracy_n": 997, "auc": 0.5315947843530592}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6050251256281407, "accuracy_n": 995, "auc": 0.6050251256281407}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6102719033232629, "accuracy_n": 993, "auc": 0.6102719033232629}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.26961770623742454, "accuracy_n": 497, "auc": 0.26961770623742454}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.246, "accuracy_n": 500, "auc": 0.246}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3265993265993266, "accuracy_n": 297, "auc": 0.3265993265993266}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.20603015075376885, "accuracy_n": 398, "auc": 0.20603015075376885}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5799930491195551, "accuracy_n": 322, "auc": 0.5799930491195551}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5272556390977443, "accuracy_n": 292, "auc": 0.5272556390977443}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9124248053786271, "accuracy_n": 1902, "auc": 0.9124248053786271}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.503938421770259, "accuracy_n": 2000, "auc": 0.503938421770259}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5938375350140056, "accuracy_n": 59, "auc": 0.5938375350140056}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5356068204613842, "accuracy_n": 997, "auc": 0.5356068204613842}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5095477386934674, "accuracy_n": 995, "auc": 0.5095477386934674}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5478348439073515, "accuracy_n": 993, "auc": 0.5478348439073515}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3460764587525151, "accuracy_n": 497, "auc": 0.3460764587525151}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2962962962962963, "accuracy_n": 297, "auc": 0.2962962962962963}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.27889447236180903, "accuracy_n": 398, "auc": 0.27889447236180903}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5001544640098857, "accuracy_n": 322, "auc": 0.5001544640098857}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5810855263157895, "accuracy_n": 292, "auc": 0.5810855263157895}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6222760290556901, "accuracy_n": 413, "auc": 0.6222760290556901}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5283704883227176, "accuracy_n": 1902, "auc": 0.5283704883227176}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5153050251140662, "accuracy_n": 2000, "auc": 0.5153050251140662}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5980392156862745, "accuracy_n": 59, "auc": 0.5980392156862745}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5245737211634904, "accuracy_n": 997, "auc": 0.5245737211634904}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5045317220543807, "accuracy_n": 993, "auc": 0.5045317220543807}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.30784708249496984, "accuracy_n": 497, "auc": 0.30784708249496984}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.256, "accuracy_n": 500, "auc": 0.256}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.26262626262626265, "accuracy_n": 297, "auc": 0.26262626262626265}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5082508250825083, "accuracy_n": 303, "auc": 0.5082508250825083}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.526896045721347, "accuracy_n": 322, "auc": 0.526896045721347}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5626644736842106, "accuracy_n": 292, "auc": 0.5626644736842106}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5496936924982307, "accuracy_n": 1902, "auc": 0.5496936924982307}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5092583422615564, "accuracy_n": 2000, "auc": 0.5092583422615564}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5973389355742297, "accuracy_n": 59, "auc": 0.5973389355742297}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5285857572718154, "accuracy_n": 997, "auc": 0.5285857572718154}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5085427135678392, "accuracy_n": 995, "auc": 0.5085427135678392}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5025176233635448, "accuracy_n": 993, "auc": 0.5025176233635448}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28772635814889336, "accuracy_n": 497, "auc": 0.28772635814889336}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.256, "accuracy_n": 500, "auc": 0.256}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.265993265993266, "accuracy_n": 297, "auc": 0.265993265993266}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.18592964824120603, "accuracy_n": 398, "auc": 0.18592964824120603}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.534735094223046, "accuracy_n": 322, "auc": 0.534735094223046}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5581766917293233, "accuracy_n": 292, "auc": 0.5581766917293233}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.547091737438075, "accuracy_n": 1902, "auc": 0.547091737438075}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.508496067080216, "accuracy_n": 2000, "auc": 0.508496067080216}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5966386554621849, "accuracy_n": 59, "auc": 0.5966386554621849}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
