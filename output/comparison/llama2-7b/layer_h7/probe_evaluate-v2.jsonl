{"key": "result_0", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5070281124497992, "accuracy_n": 996, "auc": 0.5070281124497992}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5907258064516129, "accuracy_n": 992, "auc": 0.5907258064516129}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.518555667001003, "accuracy_n": 997, "auc": 0.518555667001003}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25502008032128515, "accuracy_n": 498, "auc": 0.25502008032128515}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2483221476510067, "accuracy_n": 298, "auc": 0.2483221476510067}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5198675496688742, "accuracy_n": 302, "auc": 0.5198675496688742}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5314720420142107, "accuracy_n": 322, "auc": 0.5314720420142107}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270206766917294, "accuracy_n": 292, "auc": 0.5270206766917294}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5194174757281553, "accuracy_n": 412, "auc": 0.5194174757281553}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6220884200283086, "accuracy_n": 1902, "auc": 0.6220884200283086}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5142816556776997, "accuracy_n": 2000, "auc": 0.5142816556776997}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5546218487394958, "accuracy_n": 59, "auc": 0.5546218487394958}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5813253012048193, "accuracy_n": 996, "auc": 0.5813253012048193}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7026209677419355, "accuracy_n": 992, "auc": 0.7026209677419355}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5897693079237714, "accuracy_n": 997, "auc": 0.5897693079237714}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2751004016064257, "accuracy_n": 498, "auc": 0.2751004016064257}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2832080200501253, "accuracy_n": 399, "auc": 0.2832080200501253}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.543046357615894, "accuracy_n": 302, "auc": 0.543046357615894}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5424003707136238, "accuracy_n": 322, "auc": 0.5424003707136238}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5416118421052631, "accuracy_n": 292, "auc": 0.5416118421052631}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5509708737864077, "accuracy_n": 412, "auc": 0.5509708737864077}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7522547328379335, "accuracy_n": 1902, "auc": 0.7522547328379335}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5015935752806763, "accuracy_n": 2000, "auc": 0.5015935752806763}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5490196078431373, "accuracy_n": 59, "auc": 0.5490196078431373}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5150602409638554, "accuracy_n": 996, "auc": 0.5150602409638554}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5443548387096774, "accuracy_n": 992, "auc": 0.5443548387096774}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5144927536231884, "accuracy_n": 276, "auc": 0.5144927536231884}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5215646940822467, "accuracy_n": 997, "auc": 0.5215646940822467}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.342, "accuracy_n": 500, "auc": 0.342}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26506024096385544, "accuracy_n": 498, "auc": 0.26506024096385544}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21052631578947367, "accuracy_n": 399, "auc": 0.21052631578947367}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5199258572752549, "accuracy_n": 322, "auc": 0.5199258572752549}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.549953007518797, "accuracy_n": 292, "auc": 0.549953007518797}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5523774769992923, "accuracy_n": 1902, "auc": 0.5523774769992923}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5001590574197285, "accuracy_n": 2000, "auc": 0.5001590574197285}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5588235294117647, "accuracy_n": 59, "auc": 0.5588235294117647}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5150602409638554, "accuracy_n": 996, "auc": 0.5150602409638554}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5241935483870968, "accuracy_n": 992, "auc": 0.5241935483870968}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.26305220883534136, "accuracy_n": 498, "auc": 0.26305220883534136}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.30201342281879195, "accuracy_n": 298, "auc": 0.30201342281879195}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.19799498746867167, "accuracy_n": 399, "auc": 0.19799498746867167}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5496688741721855, "accuracy_n": 302, "auc": 0.5496688741721855}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5418983626814952, "accuracy_n": 322, "auc": 0.5418983626814952}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5523966165413534, "accuracy_n": 292, "auc": 0.5523966165413534}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5072815533980582, "accuracy_n": 412, "auc": 0.5072815533980582}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7216781670205237, "accuracy_n": 1902, "auc": 0.7216781670205237}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5173707708482762, "accuracy_n": 2000, "auc": 0.5173707708482762}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6050420168067226, "accuracy_n": 59, "auc": 0.6050420168067226}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6307692307692309, "accuracy_n": 23, "auc": 0.6307692307692309}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5433467741935484, "accuracy_n": 992, "auc": 0.5433467741935484}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2469879518072289, "accuracy_n": 498, "auc": 0.2469879518072289}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5461847389558233, "accuracy_n": 322, "auc": 0.5461847389558233}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5270206766917294, "accuracy_n": 292, "auc": 0.5270206766917294}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5364077669902912, "accuracy_n": 412, "auc": 0.5364077669902912}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6872025389242746, "accuracy_n": 1902, "auc": 0.6872025389242746}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5035697886937185, "accuracy_n": 2000, "auc": 0.5035697886937185}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5854341736694677, "accuracy_n": 59, "auc": 0.5854341736694677}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5080321285140562, "accuracy_n": 996, "auc": 0.5080321285140562}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.501008064516129, "accuracy_n": 992, "auc": 0.501008064516129}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5108695652173914, "accuracy_n": 276, "auc": 0.5108695652173914}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5285857572718154, "accuracy_n": 997, "auc": 0.5285857572718154}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.416, "accuracy_n": 500, "auc": 0.416}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5033112582781457, "accuracy_n": 302, "auc": 0.5033112582781457}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5067191844300278, "accuracy_n": 322, "auc": 0.5067191844300278}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5266917293233082, "accuracy_n": 292, "auc": 0.5266917293233082}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5169902912621359, "accuracy_n": 412, "auc": 0.5169902912621359}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.508791135881104, "accuracy_n": 1902, "auc": 0.508791135881104}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5203253374468183, "accuracy_n": 2000, "auc": 0.5203253374468183}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5588235294117647, "accuracy_n": 59, "auc": 0.5588235294117647}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5341365461847389, "accuracy_n": 996, "auc": 0.5341365461847389}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5574596774193549, "accuracy_n": 992, "auc": 0.5574596774193549}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5072463768115942, "accuracy_n": 276, "auc": 0.5072463768115942}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5827482447342026, "accuracy_n": 997, "auc": 0.5827482447342026}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 500, "auc": 0.36}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3755020080321285, "accuracy_n": 498, "auc": 0.3755020080321285}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5099337748344371, "accuracy_n": 302, "auc": 0.5099337748344371}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5593141797961074, "accuracy_n": 322, "auc": 0.5593141797961074}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5092105263157896, "accuracy_n": 292, "auc": 0.5092105263157896}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5145631067961165, "accuracy_n": 412, "auc": 0.5145631067961165}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5335832448690729, "accuracy_n": 1902, "auc": 0.5335832448690729}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5097145069370043, "accuracy_n": 2000, "auc": 0.5097145069370043}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5658263305322129, "accuracy_n": 59, "auc": 0.5658263305322129}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5341365461847389, "accuracy_n": 996, "auc": 0.5341365461847389}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5524193548387096, "accuracy_n": 992, "auc": 0.5524193548387096}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.285140562248996, "accuracy_n": 498, "auc": 0.285140562248996}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2986577181208054, "accuracy_n": 298, "auc": 0.2986577181208054}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5605112758727216, "accuracy_n": 322, "auc": 0.5605112758727216}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5479323308270677, "accuracy_n": 292, "auc": 0.5479323308270677}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 412, "auc": 0.5}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5233457183297948, "accuracy_n": 1902, "auc": 0.5233457183297948}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5042570367902812, "accuracy_n": 2000, "auc": 0.5042570367902812}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6050420168067226, "accuracy_n": 59, "auc": 0.6050420168067226}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5150602409638554, "accuracy_n": 996, "auc": 0.5150602409638554}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6512096774193549, "accuracy_n": 992, "auc": 0.6512096774193549}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6148445336008024, "accuracy_n": 997, "auc": 0.6148445336008024}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.348, "accuracy_n": 500, "auc": 0.348}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2891566265060241, "accuracy_n": 498, "auc": 0.2891566265060241}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.3007518796992481, "accuracy_n": 399, "auc": 0.3007518796992481}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.543046357615894, "accuracy_n": 302, "auc": 0.543046357615894}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5808619091751621, "accuracy_n": 322, "auc": 0.5808619091751621}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.537781954887218, "accuracy_n": 292, "auc": 0.537781954887218}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5218446601941747, "accuracy_n": 412, "auc": 0.5218446601941747}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5178797328379334, "accuracy_n": 1902, "auc": 0.5178797328379334}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.506533858722999, "accuracy_n": 2000, "auc": 0.506533858722999}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6024096385542169, "accuracy_n": 996, "auc": 0.6024096385542169}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5181451612903226, "accuracy_n": 992, "auc": 0.5181451612903226}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5217391304347826, "accuracy_n": 276, "auc": 0.5217391304347826}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5396188565697091, "accuracy_n": 997, "auc": 0.5396188565697091}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25100401606425704, "accuracy_n": 498, "auc": 0.25100401606425704}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.23825503355704697, "accuracy_n": 298, "auc": 0.23825503355704697}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5655120481927711, "accuracy_n": 322, "auc": 0.5655120481927711}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.534140037593985, "accuracy_n": 292, "auc": 0.534140037593985}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5017709439136588, "accuracy_n": 1902, "auc": 0.5017709439136588}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5061067045203318, "accuracy_n": 2000, "auc": 0.5061067045203318}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5868347338935574, "accuracy_n": 59, "auc": 0.5868347338935574}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5150602409638554, "accuracy_n": 996, "auc": 0.5150602409638554}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5715725806451613, "accuracy_n": 992, "auc": 0.5715725806451613}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2389558232931727, "accuracy_n": 498, "auc": 0.2389558232931727}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5283055298115539, "accuracy_n": 322, "auc": 0.5283055298115539}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5308975563909775, "accuracy_n": 292, "auc": 0.5308975563909775}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 412, "auc": 0.5}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5438749557678697, "accuracy_n": 1902, "auc": 0.5438749557678697}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5043100559301908, "accuracy_n": 2000, "auc": 0.5043100559301908}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5522088353413654, "accuracy_n": 996, "auc": 0.5522088353413654}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5221774193548387, "accuracy_n": 992, "auc": 0.5221774193548387}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.30120481927710846, "accuracy_n": 498, "auc": 0.30120481927710846}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5161801050355267, "accuracy_n": 322, "auc": 0.5161801050355267}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5643562030075188, "accuracy_n": 292, "auc": 0.5643562030075188}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5461165048543689, "accuracy_n": 412, "auc": 0.5461165048543689}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6022890127388535, "accuracy_n": 1902, "auc": 0.6022890127388535}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5130237015562618, "accuracy_n": 2000, "auc": 0.5130237015562618}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5040160642570282, "accuracy_n": 996, "auc": 0.5040160642570282}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5201612903225806, "accuracy_n": 992, "auc": 0.5201612903225806}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.274, "accuracy_n": 500, "auc": 0.274}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25100401606425704, "accuracy_n": 498, "auc": 0.25100401606425704}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.23154362416107382, "accuracy_n": 298, "auc": 0.23154362416107382}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.20802005012531327, "accuracy_n": 399, "auc": 0.20802005012531327}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5298013245033113, "accuracy_n": 302, "auc": 0.5298013245033113}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5762279888785913, "accuracy_n": 322, "auc": 0.5762279888785913}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5205122180451127, "accuracy_n": 292, "auc": 0.5205122180451127}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5024271844660194, "accuracy_n": 412, "auc": 0.5024271844660194}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5182844568294409, "accuracy_n": 1902, "auc": 0.5182844568294409}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5027990104427698, "accuracy_n": 2000, "auc": 0.5027990104427698}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5110441767068273, "accuracy_n": 996, "auc": 0.5110441767068273}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5776209677419355, "accuracy_n": 992, "auc": 0.5776209677419355}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6158475426278837, "accuracy_n": 997, "auc": 0.6158475426278837}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2389558232931727, "accuracy_n": 498, "auc": 0.2389558232931727}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.27181208053691275, "accuracy_n": 298, "auc": 0.27181208053691275}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5231788079470199, "accuracy_n": 302, "auc": 0.5231788079470199}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5332676861291319, "accuracy_n": 322, "auc": 0.5332676861291319}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5436325187969925, "accuracy_n": 292, "auc": 0.5436325187969925}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5728155339805825, "accuracy_n": 412, "auc": 0.5728155339805825}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5037464614295825, "accuracy_n": 1902, "auc": 0.5037464614295825}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5047662206056386, "accuracy_n": 2000, "auc": 0.5047662206056386}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5672690763052208, "accuracy_n": 996, "auc": 0.5672690763052208}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5332661290322581, "accuracy_n": 992, "auc": 0.5332661290322581}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5135406218655968, "accuracy_n": 997, "auc": 0.5135406218655968}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.25100401606425704, "accuracy_n": 498, "auc": 0.25100401606425704}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21804511278195488, "accuracy_n": 399, "auc": 0.21804511278195488}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5712079085573063, "accuracy_n": 322, "auc": 0.5712079085573063}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5439379699248121, "accuracy_n": 292, "auc": 0.5439379699248121}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 412, "auc": 0.5}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.8619128184713376, "accuracy_n": 1902, "auc": 0.8619128184713376}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5061757294383273, "accuracy_n": 2000, "auc": 0.5061757294383273}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5798319327731092, "accuracy_n": 59, "auc": 0.5798319327731092}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5411646586345381, "accuracy_n": 996, "auc": 0.5411646586345381}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5141129032258065, "accuracy_n": 992, "auc": 0.5141129032258065}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2429718875502008, "accuracy_n": 498, "auc": 0.2429718875502008}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2348993288590604, "accuracy_n": 298, "auc": 0.2348993288590604}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21804511278195488, "accuracy_n": 399, "auc": 0.21804511278195488}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5597775718257645, "accuracy_n": 322, "auc": 0.5597775718257645}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5494125939849624, "accuracy_n": 292, "auc": 0.5494125939849624}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5048543689320388, "accuracy_n": 412, "auc": 0.5048543689320388}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5137617215145082, "accuracy_n": 1902, "auc": 0.5137617215145082}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5081989598244967, "accuracy_n": 2000, "auc": 0.5081989598244967}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5896358543417368, "accuracy_n": 59, "auc": 0.5896358543417368}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5200803212851406, "accuracy_n": 996, "auc": 0.5200803212851406}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5070564516129032, "accuracy_n": 992, "auc": 0.5070564516129032}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.2429718875502008, "accuracy_n": 498, "auc": 0.2429718875502008}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.23825503355704697, "accuracy_n": 298, "auc": 0.23825503355704697}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21303258145363407, "accuracy_n": 399, "auc": 0.21303258145363407}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5298013245033113, "accuracy_n": 302, "auc": 0.5298013245033113}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5619593759654, "accuracy_n": 322, "auc": 0.5619593759654}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5429981203007519, "accuracy_n": 292, "auc": 0.5429981203007519}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5072815533980582, "accuracy_n": 412, "auc": 0.5072815533980582}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5139021585279547, "accuracy_n": 1902, "auc": 0.5139021585279547}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5069940248429683, "accuracy_n": 2000, "auc": 0.5069940248429683}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5896358543417367, "accuracy_n": 59, "auc": 0.5896358543417367}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5180722891566265, "accuracy_n": 996, "auc": 0.5180722891566265}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5060483870967742, "accuracy_n": 992, "auc": 0.5060483870967742}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.24096385542168675, "accuracy_n": 498, "auc": 0.24096385542168675}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.23825503355704697, "accuracy_n": 298, "auc": 0.23825503355704697}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5619786839666358, "accuracy_n": 322, "auc": 0.5619786839666358}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5445488721804511, "accuracy_n": 292, "auc": 0.5445488721804511}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5048543689320388, "accuracy_n": 412, "auc": 0.5048543689320388}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5167285916489739, "accuracy_n": 1902, "auc": 0.5167285916489739}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5068864860214538, "accuracy_n": 2000, "auc": 0.5068864860214538}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.5896358543417367, "accuracy_n": 59, "auc": 0.5896358543417367}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h7", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
