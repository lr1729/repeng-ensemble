{"key": "result_0", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9165829145728643, "accuracy_n": 995, "auc": 0.9165829145728643}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8903420523138833, "accuracy_n": 994, "auc": 0.8903420523138833}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054545454545455, "accuracy_n": 275, "auc": 0.5054545454545455}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6169354838709677, "accuracy_n": 992, "auc": 0.6169354838709677}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.33867735470941884, "accuracy_n": 499, "auc": 0.33867735470941884}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3082706766917293, "accuracy_n": 399, "auc": 0.3082706766917293}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5951884460920606, "accuracy_n": 322, "auc": 0.5951884460920606}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5360432330827067, "accuracy_n": 292, "auc": 0.5360432330827067}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5401459854014599, "accuracy_n": 411, "auc": 0.5401459854014599}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7370079175513093, "accuracy_n": 1902, "auc": 0.7370079175513093}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5016791061573228, "accuracy_n": 2000, "auc": 0.5016791061573228}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5784313725490197, "accuracy_n": 59, "auc": 0.5784313725490197}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8653266331658291, "accuracy_n": 995, "auc": 0.8653266331658291}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9235412474849095, "accuracy_n": 994, "auc": 0.9235412474849095}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 275, "auc": 0.64}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6199596774193549, "accuracy_n": 992, "auc": 0.6199596774193549}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2709030100334448, "accuracy_n": 299, "auc": 0.2709030100334448}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3408521303258145, "accuracy_n": 399, "auc": 0.3408521303258145}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5445544554455446, "accuracy_n": 303, "auc": 0.5445544554455446}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5085727525486561, "accuracy_n": 322, "auc": 0.5085727525486561}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5729793233082707, "accuracy_n": 292, "auc": 0.5729793233082707}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5425790754257908, "accuracy_n": 411, "auc": 0.5425790754257908}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7399139685067233, "accuracy_n": 1902, "auc": 0.7399139685067233}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5123059424452228, "accuracy_n": 2000, "auc": 0.5123059424452228}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6162464985994398, "accuracy_n": 59, "auc": 0.6162464985994398}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7796780684104627, "accuracy_n": 994, "auc": 0.7796780684104627}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6254545454545455, "accuracy_n": 275, "auc": 0.6254545454545455}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5413306451612904, "accuracy_n": 992, "auc": 0.5413306451612904}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.342685370741483, "accuracy_n": 499, "auc": 0.342685370741483}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27424749163879597, "accuracy_n": 299, "auc": 0.27424749163879597}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.45614035087719296, "accuracy_n": 399, "auc": 0.45614035087719296}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7260726072607261, "accuracy_n": 303, "auc": 0.7260726072607261}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5274173617547111, "accuracy_n": 322, "auc": 0.5274173617547111}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6641917293233084, "accuracy_n": 292, "auc": 0.6641917293233084}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5912408759124088, "accuracy_n": 411, "auc": 0.5912408759124088}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5771872788393488, "accuracy_n": 1902, "auc": 0.5771872788393488}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5113265888985924, "accuracy_n": 2000, "auc": 0.5113265888985924}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5994397759103642, "accuracy_n": 59, "auc": 0.5994397759103642}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5175879396984925, "accuracy_n": 995, "auc": 0.5175879396984925}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054545454545455, "accuracy_n": 275, "auc": 0.5054545454545455}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6169354838709677, "accuracy_n": 992, "auc": 0.6169354838709677}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.41082164328657317, "accuracy_n": 499, "auc": 0.41082164328657317}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3466933867735471, "accuracy_n": 499, "auc": 0.3466933867735471}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27424749163879597, "accuracy_n": 299, "auc": 0.27424749163879597}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.44360902255639095, "accuracy_n": 399, "auc": 0.44360902255639095}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5643564356435643, "accuracy_n": 303, "auc": 0.5643564356435643}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5489650911337658, "accuracy_n": 322, "auc": 0.5489650911337658}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5483082706766917, "accuracy_n": 292, "auc": 0.5483082706766917}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5158150851581509, "accuracy_n": 411, "auc": 0.5158150851581509}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5845817852087756, "accuracy_n": 1902, "auc": 0.5845817852087756}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5106833566917657, "accuracy_n": 2000, "auc": 0.5106833566917657}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5728291316526611, "accuracy_n": 59, "auc": 0.5728291316526611}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5362173038229376, "accuracy_n": 994, "auc": 0.5362173038229376}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5272727272727272, "accuracy_n": 275, "auc": 0.5272727272727272}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6169354838709677, "accuracy_n": 992, "auc": 0.6169354838709677}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2965931863727455, "accuracy_n": 499, "auc": 0.2965931863727455}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2845691382765531, "accuracy_n": 499, "auc": 0.2845691382765531}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2976588628762542, "accuracy_n": 299, "auc": 0.2976588628762542}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3609022556390977, "accuracy_n": 399, "auc": 0.3609022556390977}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5522474513438369, "accuracy_n": 322, "auc": 0.5522474513438369}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5183270676691729, "accuracy_n": 292, "auc": 0.5183270676691729}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5352798053527981, "accuracy_n": 411, "auc": 0.5352798053527981}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5770158793347486, "accuracy_n": 1902, "auc": 0.5770158793347486}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5021762856391158, "accuracy_n": 2000, "auc": 0.5021762856391158}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.572829131652661, "accuracy_n": 59, "auc": 0.572829131652661}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.671356783919598, "accuracy_n": 995, "auc": 0.671356783919598}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5573440643863179, "accuracy_n": 994, "auc": 0.5573440643863179}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 275, "auc": 0.52}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6118951612903226, "accuracy_n": 992, "auc": 0.6118951612903226}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4789579158316633, "accuracy_n": 499, "auc": 0.4789579158316633}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.39478957915831664, "accuracy_n": 499, "auc": 0.39478957915831664}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.44611528822055135, "accuracy_n": 399, "auc": 0.44611528822055135}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6124497991967872, "accuracy_n": 322, "auc": 0.6124497991967872}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5468515037593985, "accuracy_n": 292, "auc": 0.5468515037593985}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5669099756690997, "accuracy_n": 411, "auc": 0.5669099756690997}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6163592533616419, "accuracy_n": 1902, "auc": 0.6163592533616419}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5019186926480459, "accuracy_n": 2000, "auc": 0.5019186926480459}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5224089635854341, "accuracy_n": 59, "auc": 0.5224089635854341}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5115577889447236, "accuracy_n": 995, "auc": 0.5115577889447236}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5573440643863179, "accuracy_n": 994, "auc": 0.5573440643863179}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054545454545455, "accuracy_n": 275, "auc": 0.5054545454545455}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5715725806451613, "accuracy_n": 992, "auc": 0.5715725806451613}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.46092184368737477, "accuracy_n": 499, "auc": 0.46092184368737477}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4789579158316633, "accuracy_n": 499, "auc": 0.4789579158316633}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.34782608695652173, "accuracy_n": 299, "auc": 0.34782608695652173}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5238095238095238, "accuracy_n": 399, "auc": 0.5238095238095238}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5072984244670992, "accuracy_n": 322, "auc": 0.5072984244670992}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.600093984962406, "accuracy_n": 292, "auc": 0.600093984962406}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 411, "auc": 0.6666666666666666}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5309580679405521, "accuracy_n": 1902, "auc": 0.5309580679405521}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5162668723409151, "accuracy_n": 2000, "auc": 0.5162668723409151}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5115577889447236, "accuracy_n": 995, "auc": 0.5115577889447236}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 994, "auc": 0.5392354124748491}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5272727272727272, "accuracy_n": 275, "auc": 0.5272727272727272}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6108870967741935, "accuracy_n": 992, "auc": 0.6108870967741935}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5110220440881763, "accuracy_n": 499, "auc": 0.5110220440881763}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.48096192384769537, "accuracy_n": 499, "auc": 0.48096192384769537}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3712374581939799, "accuracy_n": 299, "auc": 0.3712374581939799}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.556390977443609, "accuracy_n": 399, "auc": 0.556390977443609}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5469570590052517, "accuracy_n": 322, "auc": 0.5469570590052517}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6934680451127819, "accuracy_n": 292, "auc": 0.6934680451127819}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5961070559610706, "accuracy_n": 411, "auc": 0.5961070559610706}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6251492834394904, "accuracy_n": 1902, "auc": 0.6251492834394904}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5323231686638876, "accuracy_n": 2000, "auc": 0.5323231686638876}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.57703081232493, "accuracy_n": 59, "auc": 0.57703081232493}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7487437185929648, "accuracy_n": 995, "auc": 0.7487437185929648}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6740442655935613, "accuracy_n": 994, "auc": 0.6740442655935613}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5309090909090909, "accuracy_n": 275, "auc": 0.5309090909090909}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6169354838709677, "accuracy_n": 992, "auc": 0.6169354838709677}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.46292585170340683, "accuracy_n": 499, "auc": 0.46292585170340683}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4649298597194389, "accuracy_n": 499, "auc": 0.4649298597194389}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.36789297658862874, "accuracy_n": 299, "auc": 0.36789297658862874}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 399, "auc": 0.5714285714285714}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5100015446400988, "accuracy_n": 322, "auc": 0.5100015446400988}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6034304511278196, "accuracy_n": 292, "auc": 0.6034304511278196}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6204379562043796, "accuracy_n": 411, "auc": 0.6204379562043796}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5585544939844302, "accuracy_n": 1902, "auc": 0.5585544939844302}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5149068813841797, "accuracy_n": 2000, "auc": 0.5149068813841797}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5938375350140056, "accuracy_n": 59, "auc": 0.5938375350140056}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6231155778894473, "accuracy_n": 995, "auc": 0.6231155778894473}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5221327967806841, "accuracy_n": 994, "auc": 0.5221327967806841}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6218181818181818, "accuracy_n": 275, "auc": 0.6218181818181818}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6068548387096774, "accuracy_n": 992, "auc": 0.6068548387096774}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3106212424849699, "accuracy_n": 499, "auc": 0.3106212424849699}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3466933867735471, "accuracy_n": 499, "auc": 0.3466933867735471}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26421404682274247, "accuracy_n": 299, "auc": 0.26421404682274247}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6023710225517455, "accuracy_n": 322, "auc": 0.6023710225517455}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5428806390977443, "accuracy_n": 292, "auc": 0.5428806390977443}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7591240875912408, "accuracy_n": 411, "auc": 0.7591240875912408}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5735436571125265, "accuracy_n": 1902, "auc": 0.5735436571125265}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5024944004785727, "accuracy_n": 2000, "auc": 0.5024944004785727}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5854341736694677, "accuracy_n": 59, "auc": 0.5854341736694677}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7979899497487437, "accuracy_n": 995, "auc": 0.7979899497487437}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.540241448692153, "accuracy_n": 994, "auc": 0.540241448692153}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5272727272727272, "accuracy_n": 275, "auc": 0.5272727272727272}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6169354838709677, "accuracy_n": 992, "auc": 0.6169354838709677}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2608695652173913, "accuracy_n": 299, "auc": 0.2608695652173913}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5341558541859747, "accuracy_n": 322, "auc": 0.5341558541859747}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5197838345864662, "accuracy_n": 292, "auc": 0.5197838345864662}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5742092457420924, "accuracy_n": 411, "auc": 0.5742092457420924}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5071374071125265, "accuracy_n": 1902, "auc": 0.5071374071125265}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5004311556471887, "accuracy_n": 2000, "auc": 0.5004311556471887}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.57703081232493, "accuracy_n": 59, "auc": 0.57703081232493}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6703517587939698, "accuracy_n": 995, "auc": 0.6703517587939698}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5020120724346077, "accuracy_n": 994, "auc": 0.5020120724346077}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5236363636363637, "accuracy_n": 275, "auc": 0.5236363636363637}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6118951612903226, "accuracy_n": 992, "auc": 0.6118951612903226}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3687374749498998, "accuracy_n": 499, "auc": 0.3687374749498998}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4228456913827655, "accuracy_n": 499, "auc": 0.4228456913827655}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.34782608695652173, "accuracy_n": 299, "auc": 0.34782608695652173}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.46365914786967416, "accuracy_n": 399, "auc": 0.46365914786967416}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5907590759075908, "accuracy_n": 303, "auc": 0.5907590759075908}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5315106580166821, "accuracy_n": 322, "auc": 0.5315106580166821}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197603383458647, "accuracy_n": 292, "auc": 0.6197603383458647}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7128953771289538, "accuracy_n": 411, "auc": 0.7128953771289538}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6895247257607926, "accuracy_n": 1902, "auc": 0.6895247257607926}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.520601937299365, "accuracy_n": 2000, "auc": 0.520601937299365}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5938375350140056, "accuracy_n": 59, "auc": 0.5938375350140056}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6693467336683417, "accuracy_n": 995, "auc": 0.6693467336683417}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5163636363636364, "accuracy_n": 275, "auc": 0.5163636363636364}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6088709677419355, "accuracy_n": 992, "auc": 0.6088709677419355}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3226452905811623, "accuracy_n": 499, "auc": 0.3226452905811623}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24812030075187969, "accuracy_n": 399, "auc": 0.24812030075187969}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5082508250825083, "accuracy_n": 303, "auc": 0.5082508250825083}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6229533518690146, "accuracy_n": 322, "auc": 0.6229533518690146}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5281954887218046, "accuracy_n": 292, "auc": 0.5281954887218046}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5669099756690997, "accuracy_n": 411, "auc": 0.5669099756690997}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5822297416843595, "accuracy_n": 1902, "auc": 0.5822297416843595}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5077437955101791, "accuracy_n": 2000, "auc": 0.5077437955101791}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5840336134453781, "accuracy_n": 59, "auc": 0.5840336134453781}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5678391959798995, "accuracy_n": 995, "auc": 0.5678391959798995}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5221327967806841, "accuracy_n": 994, "auc": 0.5221327967806841}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5163636363636364, "accuracy_n": 275, "auc": 0.5163636363636364}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6118951612903226, "accuracy_n": 992, "auc": 0.6118951612903226}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.32064128256513025, "accuracy_n": 499, "auc": 0.32064128256513025}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2845691382765531, "accuracy_n": 499, "auc": 0.2845691382765531}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3110367892976589, "accuracy_n": 299, "auc": 0.3110367892976589}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2932330827067669, "accuracy_n": 399, "auc": 0.2932330827067669}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5799544331170837, "accuracy_n": 322, "auc": 0.5799544331170837}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5075657894736842, "accuracy_n": 292, "auc": 0.5075657894736842}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8150851581508516, "accuracy_n": 411, "auc": 0.8150851581508516}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6528662420382166, "accuracy_n": 1902, "auc": 0.6528662420382166}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5000400144452147, "accuracy_n": 2000, "auc": 0.5000400144452147}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.596638655462185, "accuracy_n": 59, "auc": 0.596638655462185}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7437185929648241, "accuracy_n": 995, "auc": 0.7437185929648241}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5271629778672032, "accuracy_n": 994, "auc": 0.5271629778672032}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054545454545455, "accuracy_n": 275, "auc": 0.5054545454545455}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5604838709677419, "accuracy_n": 992, "auc": 0.5604838709677419}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2965931863727455, "accuracy_n": 499, "auc": 0.2965931863727455}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2709030100334448, "accuracy_n": 299, "auc": 0.2709030100334448}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.19298245614035087, "accuracy_n": 399, "auc": 0.19298245614035087}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5733510966944702, "accuracy_n": 322, "auc": 0.5733510966944702}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5266212406015037, "accuracy_n": 292, "auc": 0.5266212406015037}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5401459854014599, "accuracy_n": 411, "auc": 0.5401459854014599}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.898834483368719, "accuracy_n": 1902, "auc": 0.898834483368719}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5030761104758817, "accuracy_n": 2000, "auc": 0.5030761104758817}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6100502512562814, "accuracy_n": 995, "auc": 0.6100502512562814}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5543259557344065, "accuracy_n": 994, "auc": 0.5543259557344065}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5127272727272727, "accuracy_n": 275, "auc": 0.5127272727272727}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5786290322580645, "accuracy_n": 992, "auc": 0.5786290322580645}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3186372745490982, "accuracy_n": 499, "auc": 0.3186372745490982}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24749163879598662, "accuracy_n": 299, "auc": 0.24749163879598662}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 399, "auc": 0.2857142857142857}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5342910101946247, "accuracy_n": 322, "auc": 0.5342910101946247}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5790413533834586, "accuracy_n": 292, "auc": 0.5790413533834586}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5328467153284672, "accuracy_n": 411, "auc": 0.5328467153284672}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5424042374380751, "accuracy_n": 1902, "auc": 0.5424042374380751}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5112345556745986, "accuracy_n": 2000, "auc": 0.5112345556745986}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7076923076923076, "accuracy_n": 23, "auc": 0.7076923076923076}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6190954773869347, "accuracy_n": 995, "auc": 0.6190954773869347}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5311871227364185, "accuracy_n": 994, "auc": 0.5311871227364185}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054545454545455, "accuracy_n": 275, "auc": 0.5054545454545455}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5907258064516129, "accuracy_n": 992, "auc": 0.5907258064516129}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27655310621242485, "accuracy_n": 499, "auc": 0.27655310621242485}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3046092184368738, "accuracy_n": 499, "auc": 0.3046092184368738}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2408026755852843, "accuracy_n": 299, "auc": 0.2408026755852843}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.551764751312944, "accuracy_n": 322, "auc": 0.551764751312944}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5648966165413534, "accuracy_n": 292, "auc": 0.5648966165413534}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036496350364964, "accuracy_n": 411, "auc": 0.5036496350364964}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5543093152866242, "accuracy_n": 1902, "auc": 0.5543093152866242}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5076172498271876, "accuracy_n": 2000, "auc": 0.5076172498271876}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5868347338935574, "accuracy_n": 59, "auc": 0.5868347338935574}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6311557788944724, "accuracy_n": 995, "auc": 0.6311557788944724}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.540241448692153, "accuracy_n": 994, "auc": 0.540241448692153}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5018181818181818, "accuracy_n": 275, "auc": 0.5018181818181818}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.59375, "accuracy_n": 992, "auc": 0.59375}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5551436515291936, "accuracy_n": 322, "auc": 0.5551436515291936}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5627114661654135, "accuracy_n": 292, "auc": 0.5627114661654135}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.49878345498783455, "accuracy_n": 411, "auc": 0.49878345498783455}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5522680024769993, "accuracy_n": 1902, "auc": 0.5522680024769993}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.507085557886397, "accuracy_n": 2000, "auc": 0.507085557886397}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
