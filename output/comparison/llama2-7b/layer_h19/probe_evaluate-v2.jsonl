{"key": "result_0", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_1", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8633165829145729, "accuracy_n": 995, "auc": 0.8633165829145729}}
{"key": "result_2", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6823104693140795, "accuracy_n": 277, "auc": 0.6823104693140795}}
{"key": "result_3", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_4", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5357502517623364, "accuracy_n": 993, "auc": 0.5357502517623364}}
{"key": "result_5", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.45271629778672035, "accuracy_n": 497, "auc": 0.45271629778672035}}
{"key": "result_6", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.466, "accuracy_n": 500, "auc": 0.466}}
{"key": "result_7", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4276094276094276, "accuracy_n": 297, "auc": 0.4276094276094276}}
{"key": "result_8", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5527638190954773, "accuracy_n": 398, "auc": 0.5527638190954773}}
{"key": "result_9", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5775577557755776, "accuracy_n": 303, "auc": 0.5775577557755776}}
{"key": "result_10", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_11", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5858433734939759, "accuracy_n": 322, "auc": 0.5858433734939759}}
{"key": "result_12", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5142857142857142, "accuracy_n": 292, "auc": 0.5142857142857142}}
{"key": "result_13", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_14", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6619681086341118, "accuracy_n": 1902, "auc": 0.6619681086341118}}
{"key": "result_15", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5046381743809515, "accuracy_n": 2000, "auc": 0.5046381743809515}}
{"key": "result_16", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5714285714285715, "accuracy_n": 59, "auc": 0.5714285714285715}}
{"key": "result_17", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_18", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9197592778335005, "accuracy_n": 997, "auc": 0.9197592778335005}}
{"key": "result_19", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7819095477386935, "accuracy_n": 995, "auc": 0.7819095477386935}}
{"key": "result_20", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6112789526686808, "accuracy_n": 993, "auc": 0.6112789526686808}}
{"key": "result_23", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.32193158953722334, "accuracy_n": 497, "auc": 0.32193158953722334}}
{"key": "result_24", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.424, "accuracy_n": 500, "auc": 0.424}}
{"key": "result_25", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4107744107744108, "accuracy_n": 297, "auc": 0.4107744107744108}}
{"key": "result_26", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.535175879396985, "accuracy_n": 398, "auc": 0.535175879396985}}
{"key": "result_27", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_28", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_29", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5184198331788693, "accuracy_n": 322, "auc": 0.5184198331788693}}
{"key": "result_30", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.550046992481203, "accuracy_n": 292, "auc": 0.550046992481203}}
{"key": "result_31", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5738498789346247, "accuracy_n": 413, "auc": 0.5738498789346247}}
{"key": "result_32", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6617867569002123, "accuracy_n": 1902, "auc": 0.6617867569002123}}
{"key": "result_33", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5158457203050301, "accuracy_n": 2000, "auc": 0.5158457203050301}}
{"key": "result_34", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.542016806722689, "accuracy_n": 59, "auc": 0.542016806722689}}
{"key": "result_35", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_36", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8906720160481444, "accuracy_n": 997, "auc": 0.8906720160481444}}
{"key": "result_37", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6693467336683417, "accuracy_n": 995, "auc": 0.6693467336683417}}
{"key": "result_38", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_39", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_41", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2595573440643863, "accuracy_n": 497, "auc": 0.2595573440643863}}
{"key": "result_42", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_43", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2727272727272727, "accuracy_n": 297, "auc": 0.2727272727272727}}
{"key": "result_44", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5125628140703518, "accuracy_n": 398, "auc": 0.5125628140703518}}
{"key": "result_45", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5907590759075908, "accuracy_n": 303, "auc": 0.5907590759075908}}
{"key": "result_46", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.554525795489651, "accuracy_n": 322, "auc": 0.554525795489651}}
{"key": "result_48", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5208646616541354, "accuracy_n": 292, "auc": 0.5208646616541354}}
{"key": "result_49", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_50", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6082691967445153, "accuracy_n": 1902, "auc": 0.6082691967445153}}
{"key": "result_51", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5203888603785967, "accuracy_n": 2000, "auc": 0.5203888603785967}}
{"key": "result_52", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.546218487394958, "accuracy_n": 59, "auc": 0.546218487394958}}
{"key": "result_53", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_54", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8525576730190572, "accuracy_n": 997, "auc": 0.8525576730190572}}
{"key": "result_55", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5195979899497487, "accuracy_n": 995, "auc": 0.5195979899497487}}
{"key": "result_56", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_57", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5307150050352467, "accuracy_n": 993, "auc": 0.5307150050352467}}
{"key": "result_59", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.38832997987927564, "accuracy_n": 497, "auc": 0.38832997987927564}}
{"key": "result_60", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_61", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.42424242424242425, "accuracy_n": 297, "auc": 0.42424242424242425}}
{"key": "result_62", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5804020100502513, "accuracy_n": 398, "auc": 0.5804020100502513}}
{"key": "result_63", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_64", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6261970960766141, "accuracy_n": 322, "auc": 0.6261970960766141}}
{"key": "result_66", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5555921052631578, "accuracy_n": 292, "auc": 0.5555921052631578}}
{"key": "result_67", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_68", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5723173213021939, "accuracy_n": 1902, "auc": 0.5723173213021939}}
{"key": "result_69", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5208065111505253, "accuracy_n": 2000, "auc": 0.5208065111505253}}
{"key": "result_70", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5490196078431373, "accuracy_n": 59, "auc": 0.5490196078431373}}
{"key": "result_71", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_72", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5145436308926781, "accuracy_n": 997, "auc": 0.5145436308926781}}
{"key": "result_73", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5336683417085427, "accuracy_n": 995, "auc": 0.5336683417085427}}
{"key": "result_74", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.45875251509054327, "accuracy_n": 497, "auc": 0.45875251509054327}}
{"key": "result_78", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.33, "accuracy_n": 500, "auc": 0.33}}
{"key": "result_79", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4107744107744108, "accuracy_n": 297, "auc": 0.4107744107744108}}
{"key": "result_80", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.49748743718592964, "accuracy_n": 398, "auc": 0.49748743718592964}}
{"key": "result_81", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_82", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5572675316651221, "accuracy_n": 322, "auc": 0.5572675316651221}}
{"key": "result_84", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.537593984962406, "accuracy_n": 292, "auc": 0.537593984962406}}
{"key": "result_85", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 413, "auc": 0.5714285714285714}}
{"key": "result_86", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689932767162067, "accuracy_n": 1902, "auc": 0.5689932767162067}}
{"key": "result_87", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5021117623462069, "accuracy_n": 2000, "auc": 0.5021117623462069}}
{"key": "result_88", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5840336134453782, "accuracy_n": 59, "auc": 0.5840336134453782}}
{"key": "result_89", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_90", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6088264794383149, "accuracy_n": 997, "auc": 0.6088264794383149}}
{"key": "result_91", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5256281407035176, "accuracy_n": 995, "auc": 0.5256281407035176}}
{"key": "result_92", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_94", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_95", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 497, "auc": 0.545271629778672}}
{"key": "result_96", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.514, "accuracy_n": 500, "auc": 0.514}}
{"key": "result_97", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5151515151515151, "accuracy_n": 297, "auc": 0.5151515151515151}}
{"key": "result_98", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.628140703517588, "accuracy_n": 398, "auc": 0.628140703517588}}
{"key": "result_99", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_100", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_101", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6079124189063947, "accuracy_n": 322, "auc": 0.6079124189063947}}
{"key": "result_102", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6379699248120301, "accuracy_n": 292, "auc": 0.6379699248120301}}
{"key": "result_103", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5641646489104116, "accuracy_n": 413, "auc": 0.5641646489104116}}
{"key": "result_104", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5079451963906583, "accuracy_n": 1902, "auc": 0.5079451963906583}}
{"key": "result_105", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.534128820504202, "accuracy_n": 2000, "auc": 0.534128820504202}}
{"key": "result_106", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6638655462184875, "accuracy_n": 59, "auc": 0.6638655462184875}}
{"key": "result_107", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_108", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.847542627883651, "accuracy_n": 997, "auc": 0.847542627883651}}
{"key": "result_109", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5618090452261306, "accuracy_n": 995, "auc": 0.5618090452261306}}
{"key": "result_110", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_111", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_112", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6263846928499497, "accuracy_n": 993, "auc": 0.6263846928499497}}
{"key": "result_113", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5513078470824949, "accuracy_n": 497, "auc": 0.5513078470824949}}
{"key": "result_114", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 500, "auc": 0.54}}
{"key": "result_115", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.494949494949495, "accuracy_n": 297, "auc": 0.494949494949495}}
{"key": "result_116", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6030150753768844, "accuracy_n": 398, "auc": 0.6030150753768844}}
{"key": "result_117", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_118", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6739264751312943, "accuracy_n": 322, "auc": 0.6739264751312943}}
{"key": "result_120", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5531954887218045, "accuracy_n": 292, "auc": 0.5531954887218045}}
{"key": "result_121", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_122", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5427802105449399, "accuracy_n": 1902, "auc": 0.5427802105449399}}
{"key": "result_123", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5127230930365863, "accuracy_n": 2000, "auc": 0.5127230930365863}}
{"key": "result_124", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5812324929971988, "accuracy_n": 59, "auc": 0.5812324929971988}}
{"key": "result_125", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_126", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8054162487462387, "accuracy_n": 997, "auc": 0.8054162487462387}}
{"key": "result_127", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.578894472361809, "accuracy_n": 995, "auc": 0.578894472361809}}
{"key": "result_128", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_129", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_130", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_131", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5492957746478874, "accuracy_n": 497, "auc": 0.5492957746478874}}
{"key": "result_132", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.552, "accuracy_n": 500, "auc": 0.552}}
{"key": "result_133", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.48484848484848486, "accuracy_n": 297, "auc": 0.48484848484848486}}
{"key": "result_134", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6180904522613065, "accuracy_n": 398, "auc": 0.6180904522613065}}
{"key": "result_135", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_136", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7406935434043869, "accuracy_n": 322, "auc": 0.7406935434043869}}
{"key": "result_138", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6667293233082707, "accuracy_n": 292, "auc": 0.6667293233082707}}
{"key": "result_139", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_140", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5308618630573249, "accuracy_n": 1902, "auc": 0.5308618630573249}}
{"key": "result_141", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5583130510114152, "accuracy_n": 2000, "auc": 0.5583130510114152}}
{"key": "result_142", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.546218487394958, "accuracy_n": 59, "auc": 0.546218487394958}}
{"key": "result_143", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_144", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.827482447342026, "accuracy_n": 997, "auc": 0.827482447342026}}
{"key": "result_145", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5678391959798995, "accuracy_n": 995, "auc": 0.5678391959798995}}
{"key": "result_146", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_148", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6263846928499497, "accuracy_n": 993, "auc": 0.6263846928499497}}
{"key": "result_149", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5311871227364185, "accuracy_n": 497, "auc": 0.5311871227364185}}
{"key": "result_150", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 500, "auc": 0.54}}
{"key": "result_151", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4882154882154882, "accuracy_n": 297, "auc": 0.4882154882154882}}
{"key": "result_152", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6206030150753769, "accuracy_n": 398, "auc": 0.6206030150753769}}
{"key": "result_153", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5841584158415841, "accuracy_n": 303, "auc": 0.5841584158415841}}
{"key": "result_154", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5787380290392339, "accuracy_n": 322, "auc": 0.5787380290392339}}
{"key": "result_156", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6116541353383459, "accuracy_n": 292, "auc": 0.6116541353383459}}
{"key": "result_157", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5569007263922519, "accuracy_n": 413, "auc": 0.5569007263922519}}
{"key": "result_158", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5328965410474169, "accuracy_n": 1902, "auc": 0.5328965410474169}}
{"key": "result_159", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5314913683839867, "accuracy_n": 2000, "auc": 0.5314913683839867}}
{"key": "result_160", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_161", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_162", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6519558676028084, "accuracy_n": 997, "auc": 0.6519558676028084}}
{"key": "result_163", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6673366834170854, "accuracy_n": 995, "auc": 0.6673366834170854}}
{"key": "result_164", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_166", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6112789526686808, "accuracy_n": 993, "auc": 0.6112789526686808}}
{"key": "result_167", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4909456740442656, "accuracy_n": 497, "auc": 0.4909456740442656}}
{"key": "result_168", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.37, "accuracy_n": 500, "auc": 0.37}}
{"key": "result_169", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4377104377104377, "accuracy_n": 297, "auc": 0.4377104377104377}}
{"key": "result_170", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5678391959798995, "accuracy_n": 398, "auc": 0.5678391959798995}}
{"key": "result_171", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6798679867986799, "accuracy_n": 303, "auc": 0.6798679867986799}}
{"key": "result_172", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_173", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6637318504788384, "accuracy_n": 322, "auc": 0.6637318504788384}}
{"key": "result_174", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5175751879699249, "accuracy_n": 292, "auc": 0.5175751879699249}}
{"key": "result_175", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8062953995157385, "accuracy_n": 413, "auc": 0.8062953995157385}}
{"key": "result_176", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5893765481245576, "accuracy_n": 1902, "auc": 0.5893765481245576}}
{"key": "result_177", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5074706969215886, "accuracy_n": 2000, "auc": 0.5074706969215886}}
{"key": "result_178", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5868347338935576, "accuracy_n": 59, "auc": 0.5868347338935576}}
{"key": "result_179", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6615384615384616, "accuracy_n": 23, "auc": 0.6615384615384616}}
{"key": "result_180", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7683049147442327, "accuracy_n": 997, "auc": 0.7683049147442327}}
{"key": "result_181", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5417085427135678, "accuracy_n": 995, "auc": 0.5417085427135678}}
{"key": "result_182", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_184", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_185", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.39436619718309857, "accuracy_n": 497, "auc": 0.39436619718309857}}
{"key": "result_186", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.364, "accuracy_n": 500, "auc": 0.364}}
{"key": "result_187", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4074074074074074, "accuracy_n": 297, "auc": 0.4074074074074074}}
{"key": "result_188", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.46984924623115576, "accuracy_n": 398, "auc": 0.46984924623115576}}
{"key": "result_189", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_190", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_191", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6406394810009267, "accuracy_n": 322, "auc": 0.6406394810009267}}
{"key": "result_192", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5586466165413534, "accuracy_n": 292, "auc": 0.5586466165413534}}
{"key": "result_193", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_194", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5545924009200283, "accuracy_n": 1902, "auc": 0.5545924009200283}}
{"key": "result_195", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5143986979299527, "accuracy_n": 2000, "auc": 0.5143986979299527}}
{"key": "result_196", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5322128851540616, "accuracy_n": 59, "auc": 0.5322128851540616}}
{"key": "result_197", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_198", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5075225677031093, "accuracy_n": 997, "auc": 0.5075225677031093}}
{"key": "result_199", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5115577889447236, "accuracy_n": 995, "auc": 0.5115577889447236}}
{"key": "result_200", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5699899295065458, "accuracy_n": 993, "auc": 0.5699899295065458}}
{"key": "result_203", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36217303822937624, "accuracy_n": 497, "auc": 0.36217303822937624}}
{"key": "result_204", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.328, "accuracy_n": 500, "auc": 0.328}}
{"key": "result_205", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.39730639730639733, "accuracy_n": 297, "auc": 0.39730639730639733}}
{"key": "result_206", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4321608040201005, "accuracy_n": 398, "auc": 0.4321608040201005}}
{"key": "result_207", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6171617161716172, "accuracy_n": 303, "auc": 0.6171617161716172}}
{"key": "result_208", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_209", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5602023478529503, "accuracy_n": 322, "auc": 0.5602023478529503}}
{"key": "result_210", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6051221804511278, "accuracy_n": 292, "auc": 0.6051221804511278}}
{"key": "result_211", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6416464891041163, "accuracy_n": 413, "auc": 0.6416464891041163}}
{"key": "result_212", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.604837889242746, "accuracy_n": 1902, "auc": 0.604837889242746}}
{"key": "result_213", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5152825169886329, "accuracy_n": 2000, "auc": 0.5152825169886329}}
{"key": "result_214", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6050420168067228, "accuracy_n": 59, "auc": 0.6050420168067228}}
{"key": "result_215", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_216", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5155466399197592, "accuracy_n": 997, "auc": 0.5155466399197592}}
{"key": "result_217", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5447236180904522, "accuracy_n": 995, "auc": 0.5447236180904522}}
{"key": "result_218", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_219", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_220", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_221", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.34004024144869216, "accuracy_n": 497, "auc": 0.34004024144869216}}
{"key": "result_222", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.328, "accuracy_n": 500, "auc": 0.328}}
{"key": "result_223", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3838383838383838, "accuracy_n": 297, "auc": 0.3838383838383838}}
{"key": "result_224", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2889447236180904, "accuracy_n": 398, "auc": 0.2889447236180904}}
{"key": "result_225", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_226", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_227", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6421261970960767, "accuracy_n": 322, "auc": 0.6421261970960767}}
{"key": "result_228", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5372180451127819, "accuracy_n": 292, "auc": 0.5372180451127819}}
{"key": "result_229", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7167070217917676, "accuracy_n": 413, "auc": 0.7167070217917676}}
{"key": "result_230", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5491253096249116, "accuracy_n": 1902, "auc": 0.5491253096249116}}
{"key": "result_231", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5072666232509937, "accuracy_n": 2000, "auc": 0.5072666232509937}}
{"key": "result_232", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5896358543417367, "accuracy_n": 59, "auc": 0.5896358543417367}}
{"key": "result_233", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_234", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5075225677031093, "accuracy_n": 997, "auc": 0.5075225677031093}}
{"key": "result_235", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.549748743718593, "accuracy_n": 995, "auc": 0.549748743718593}}
{"key": "result_236", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_239", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.42052313883299797, "accuracy_n": 497, "auc": 0.42052313883299797}}
{"key": "result_240", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.392, "accuracy_n": 500, "auc": 0.392}}
{"key": "result_241", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3434343434343434, "accuracy_n": 297, "auc": 0.3434343434343434}}
{"key": "result_242", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3693467336683417, "accuracy_n": 398, "auc": 0.3693467336683417}}
{"key": "result_243", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_244", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_245", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5950339820821748, "accuracy_n": 322, "auc": 0.5950339820821748}}
{"key": "result_246", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5220394736842106, "accuracy_n": 292, "auc": 0.5220394736842106}}
{"key": "result_247", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8523002421307506, "accuracy_n": 413, "auc": 0.8523002421307506}}
{"key": "result_248", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.512205856334041, "accuracy_n": 1902, "auc": 0.512205856334041}}
{"key": "result_249", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5030375965723627, "accuracy_n": 2000, "auc": 0.5030375965723627}}
{"key": "result_250", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.596638655462185, "accuracy_n": 59, "auc": 0.596638655462185}}
{"key": "result_251", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_252", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5325977933801405, "accuracy_n": 997, "auc": 0.5325977933801405}}
{"key": "result_253", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6824120603015076, "accuracy_n": 995, "auc": 0.6824120603015076}}
{"key": "result_254", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5568982880161127, "accuracy_n": 993, "auc": 0.5568982880161127}}
{"key": "result_257", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2917505030181087, "accuracy_n": 497, "auc": 0.2917505030181087}}
{"key": "result_258", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 500, "auc": 0.25}}
{"key": "result_259", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.29292929292929293, "accuracy_n": 297, "auc": 0.29292929292929293}}
{"key": "result_260", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.26884422110552764, "accuracy_n": 398, "auc": 0.26884422110552764}}
{"key": "result_261", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_262", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_263", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5982584182885388, "accuracy_n": 322, "auc": 0.5982584182885388}}
{"key": "result_264", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5259868421052631, "accuracy_n": 292, "auc": 0.5259868421052631}}
{"key": "result_265", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_266", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8894041932059448, "accuracy_n": 1902, "auc": 0.8894041932059448}}
{"key": "result_267", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5032546749376524, "accuracy_n": 2000, "auc": 0.5032546749376524}}
{"key": "result_268", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5980392156862745, "accuracy_n": 59, "auc": 0.5980392156862745}}
{"key": "result_269", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_270", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5336008024072216, "accuracy_n": 997, "auc": 0.5336008024072216}}
{"key": "result_271", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5185929648241207, "accuracy_n": 995, "auc": 0.5185929648241207}}
{"key": "result_272", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_273", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_274", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6143001007049346, "accuracy_n": 993, "auc": 0.6143001007049346}}
{"key": "result_275", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3118712273641851, "accuracy_n": 497, "auc": 0.3118712273641851}}
{"key": "result_276", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_277", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2996632996632997, "accuracy_n": 297, "auc": 0.2996632996632997}}
{"key": "result_278", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3693467336683417, "accuracy_n": 398, "auc": 0.3693467336683417}}
{"key": "result_279", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5544554455445545, "accuracy_n": 303, "auc": 0.5544554455445545}}
{"key": "result_280", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_281", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5022204201421069, "accuracy_n": 322, "auc": 0.5022204201421069}}
{"key": "result_282", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5715930451127819, "accuracy_n": 292, "auc": 0.5715930451127819}}
{"key": "result_283", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.612590799031477, "accuracy_n": 413, "auc": 0.612590799031477}}
{"key": "result_284", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5115667020523709, "accuracy_n": 1902, "auc": 0.5115667020523709}}
{"key": "result_285", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5144972335012938, "accuracy_n": 2000, "auc": 0.5144972335012938}}
{"key": "result_286", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6008403361344538, "accuracy_n": 59, "auc": 0.6008403361344538}}
{"key": "result_287", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_288", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.518555667001003, "accuracy_n": 997, "auc": 0.518555667001003}}
{"key": "result_289", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5487437185929648, "accuracy_n": 995, "auc": 0.5487437185929648}}
{"key": "result_290", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_291", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_292", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6102719033232629, "accuracy_n": 993, "auc": 0.6102719033232629}}
{"key": "result_293", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.289738430583501, "accuracy_n": 497, "auc": 0.289738430583501}}
{"key": "result_294", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_295", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.25925925925925924, "accuracy_n": 297, "auc": 0.25925925925925924}}
{"key": "result_296", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3015075376884422, "accuracy_n": 398, "auc": 0.3015075376884422}}
{"key": "result_297", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_298", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_299", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5206788693234476, "accuracy_n": 322, "auc": 0.5206788693234476}}
{"key": "result_300", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5537593984962406, "accuracy_n": 292, "auc": 0.5537593984962406}}
{"key": "result_301", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_302", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5008934890304317, "accuracy_n": 1902, "auc": 0.5008934890304317}}
{"key": "result_303", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5085165744833885, "accuracy_n": 2000, "auc": 0.5085165744833885}}
{"key": "result_304", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.596638655462185, "accuracy_n": 59, "auc": 0.596638655462185}}
{"key": "result_305", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_306", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.522567703109328, "accuracy_n": 997, "auc": 0.522567703109328}}
{"key": "result_307", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5577889447236181, "accuracy_n": 995, "auc": 0.5577889447236181}}
{"key": "result_308", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_309", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6143001007049346, "accuracy_n": 993, "auc": 0.6143001007049346}}
{"key": "result_311", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_312", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_313", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2727272727272727, "accuracy_n": 297, "auc": 0.2727272727272727}}
{"key": "result_314", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.24623115577889448, "accuracy_n": 398, "auc": 0.24623115577889448}}
{"key": "result_315", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_316", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_317", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5257182576459685, "accuracy_n": 322, "auc": 0.5257182576459685}}
{"key": "result_318", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5494595864661653, "accuracy_n": 292, "auc": 0.5494595864661653}}
{"key": "result_319", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_320", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5038813694267517, "accuracy_n": 1902, "auc": 0.5038813694267517}}
{"key": "result_321", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5081459406845872, "accuracy_n": 2000, "auc": 0.5081459406845872}}
{"key": "result_322", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5966386554621849, "accuracy_n": 59, "auc": 0.5966386554621849}}
{"key": "result_323", "value": {"llm_id": "Llama-2-7b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
