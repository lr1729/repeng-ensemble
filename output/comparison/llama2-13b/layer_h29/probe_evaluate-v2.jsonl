{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9114688128772636, "accuracy_n": 994, "auc": 0.9114688128772636}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9094567404426559, "accuracy_n": 994, "auc": 0.9094567404426559}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6118355065195586, "accuracy_n": 997, "auc": 0.6118355065195586}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.368, "accuracy_n": 500, "auc": 0.368}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.39, "accuracy_n": 500, "auc": 0.39}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.38127090301003347, "accuracy_n": 299, "auc": 0.38127090301003347}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.555, "accuracy_n": 400, "auc": 0.555}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5149501661129569, "accuracy_n": 301, "auc": 0.5149501661129569}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6171609514983009, "accuracy_n": 322, "auc": 0.6171609514983009}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5606203007518796, "accuracy_n": 292, "auc": 0.5606203007518796}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5561195152158528, "accuracy_n": 1902, "auc": 0.5561195152158528}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5286808537882176, "accuracy_n": 2000, "auc": 0.5286808537882176}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6568627450980393, "accuracy_n": 59, "auc": 0.6568627450980393}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5307692307692307, "accuracy_n": 23, "auc": 0.5307692307692307}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.89738430583501, "accuracy_n": 994, "auc": 0.89738430583501}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7967806841046278, "accuracy_n": 994, "auc": 0.7967806841046278}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.448, "accuracy_n": 500, "auc": 0.448}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.546, "accuracy_n": 500, "auc": 0.546}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5284280936454849, "accuracy_n": 299, "auc": 0.5284280936454849}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 400, "auc": 0.64}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8870431893687708, "accuracy_n": 301, "auc": 0.8870431893687708}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9626197096076614, "accuracy_n": 322, "auc": 0.9626197096076614}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8472274436090226, "accuracy_n": 292, "auc": 0.8472274436090226}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6029055690072639, "accuracy_n": 413, "auc": 0.6029055690072639}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7208886234961076, "accuracy_n": 1902, "auc": 0.7208886234961076}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6211982525691775, "accuracy_n": 2000, "auc": 0.6211982525691775}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6596638655462185, "accuracy_n": 59, "auc": 0.6596638655462185}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8692307692307693, "accuracy_n": 23, "auc": 0.8692307692307693}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8380281690140845, "accuracy_n": 994, "auc": 0.8380281690140845}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7655935613682092, "accuracy_n": 994, "auc": 0.7655935613682092}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.346, "accuracy_n": 500, "auc": 0.346}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.34, "accuracy_n": 400, "auc": 0.34}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.53156146179402, "accuracy_n": 301, "auc": 0.53156146179402}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7214627741736176, "accuracy_n": 322, "auc": 0.7214627741736176}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.649906015037594, "accuracy_n": 292, "auc": 0.649906015037594}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5578655785562634, "accuracy_n": 1902, "auc": 0.5578655785562634}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5326632914482128, "accuracy_n": 2000, "auc": 0.5326632914482128}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8199195171026157, "accuracy_n": 994, "auc": 0.8199195171026157}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6981891348088531, "accuracy_n": 994, "auc": 0.6981891348088531}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5740072202166066, "accuracy_n": 277, "auc": 0.5740072202166066}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5576730190571715, "accuracy_n": 997, "auc": 0.5576730190571715}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.472, "accuracy_n": 500, "auc": 0.472}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.478, "accuracy_n": 500, "auc": 0.478}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5083612040133779, "accuracy_n": 299, "auc": 0.5083612040133779}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 400, "auc": 0.64}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5681063122923588, "accuracy_n": 301, "auc": 0.5681063122923588}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5849938214396045, "accuracy_n": 322, "auc": 0.5849938214396045}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5939379699248121, "accuracy_n": 292, "auc": 0.5939379699248121}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5811138014527845, "accuracy_n": 413, "auc": 0.5811138014527845}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5041854653220099, "accuracy_n": 1902, "auc": 0.5041854653220099}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5732359381736807, "accuracy_n": 2000, "auc": 0.5732359381736807}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6582633053221287, "accuracy_n": 59, "auc": 0.6582633053221287}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8538461538461538, "accuracy_n": 23, "auc": 0.8538461538461538}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6569416498993964, "accuracy_n": 994, "auc": 0.6569416498993964}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.458, "accuracy_n": 500, "auc": 0.458}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.384, "accuracy_n": 500, "auc": 0.384}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4882943143812709, "accuracy_n": 299, "auc": 0.4882943143812709}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 400, "auc": 0.57}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6162341674389867, "accuracy_n": 322, "auc": 0.6162341674389867}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6332236842105263, "accuracy_n": 292, "auc": 0.6332236842105263}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7263922518159807, "accuracy_n": 413, "auc": 0.7263922518159807}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5089581121726823, "accuracy_n": 1902, "auc": 0.5089581121726823}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5487460973411402, "accuracy_n": 2000, "auc": 0.5487460973411402}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6344537815126051, "accuracy_n": 59, "auc": 0.6344537815126051}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6153846153846153, "accuracy_n": 23, "auc": 0.6153846153846153}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7012072434607646, "accuracy_n": 994, "auc": 0.7012072434607646}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 994, "auc": 0.545271629778672}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.588, "accuracy_n": 500, "auc": 0.588}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 500, "auc": 0.58}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5986622073578596, "accuracy_n": 299, "auc": 0.5986622073578596}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6525, "accuracy_n": 400, "auc": 0.6525}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5249169435215947, "accuracy_n": 301, "auc": 0.5249169435215947}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9664426938523324, "accuracy_n": 322, "auc": 0.9664426938523324}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8141447368421052, "accuracy_n": 292, "auc": 0.8141447368421052}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5786924939467313, "accuracy_n": 413, "auc": 0.5786924939467313}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5402158527954706, "accuracy_n": 1902, "auc": 0.5402158527954706}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.504596159213476, "accuracy_n": 2000, "auc": 0.504596159213476}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7086834733893557, "accuracy_n": 59, "auc": 0.7086834733893557}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7505030181086519, "accuracy_n": 994, "auc": 0.7505030181086519}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7062374245472837, "accuracy_n": 994, "auc": 0.7062374245472837}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.724172517552658, "accuracy_n": 997, "auc": 0.724172517552658}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.572, "accuracy_n": 500, "auc": 0.572}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.616, "accuracy_n": 500, "auc": 0.616}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6287625418060201, "accuracy_n": 299, "auc": 0.6287625418060201}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6625, "accuracy_n": 400, "auc": 0.6625}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6578073089700996, "accuracy_n": 301, "auc": 0.6578073089700996}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8492817423540315, "accuracy_n": 322, "auc": 0.8492817423540315}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5441729323308271, "accuracy_n": 292, "auc": 0.5441729323308271}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5112482307147912, "accuracy_n": 1902, "auc": 0.5112482307147912}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5655701708316703, "accuracy_n": 2000, "auc": 0.5655701708316703}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7829131652661064, "accuracy_n": 59, "auc": 0.7829131652661064}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7394366197183099, "accuracy_n": 994, "auc": 0.7394366197183099}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7364185110663984, "accuracy_n": 994, "auc": 0.7364185110663984}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7151454363089268, "accuracy_n": 997, "auc": 0.7151454363089268}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.544, "accuracy_n": 500, "auc": 0.544}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.608, "accuracy_n": 500, "auc": 0.608}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6675, "accuracy_n": 400, "auc": 0.6675}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6179401993355482, "accuracy_n": 301, "auc": 0.6179401993355482}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.989535063330244, "accuracy_n": 322, "auc": 0.989535063330244}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8793233082706767, "accuracy_n": 292, "auc": 0.8793233082706767}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5378129423213022, "accuracy_n": 1902, "auc": 0.5378129423213022}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5189383367395629, "accuracy_n": 2000, "auc": 0.5189383367395629}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8095238095238095, "accuracy_n": 59, "auc": 0.8095238095238095}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8158953722334004, "accuracy_n": 994, "auc": 0.8158953722334004}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7414486921529175, "accuracy_n": 994, "auc": 0.7414486921529175}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7001003009027081, "accuracy_n": 997, "auc": 0.7001003009027081}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 500, "auc": 0.55}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.646, "accuracy_n": 500, "auc": 0.646}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6825, "accuracy_n": 400, "auc": 0.6825}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5083056478405316, "accuracy_n": 301, "auc": 0.5083056478405316}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9323061476675935, "accuracy_n": 322, "auc": 0.9323061476675935}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.762687969924812, "accuracy_n": 292, "auc": 0.762687969924812}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5565574133050246, "accuracy_n": 1902, "auc": 0.5565574133050246}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.618316212152587, "accuracy_n": 2000, "auc": 0.618316212152587}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8515406162464987, "accuracy_n": 59, "auc": 0.8515406162464987}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7847082494969819, "accuracy_n": 994, "auc": 0.7847082494969819}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5015045135406219, "accuracy_n": 997, "auc": 0.5015045135406219}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.304, "accuracy_n": 500, "auc": 0.304}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.372, "accuracy_n": 500, "auc": 0.372}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2375, "accuracy_n": 400, "auc": 0.2375}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.946843853820598, "accuracy_n": 301, "auc": 0.946843853820598}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9728143342601174, "accuracy_n": 322, "auc": 0.9728143342601174}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7104323308270677, "accuracy_n": 292, "auc": 0.7104323308270677}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5966372523000708, "accuracy_n": 1902, "auc": 0.5966372523000708}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6463763418594113, "accuracy_n": 2000, "auc": 0.6463763418594113}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7310924369747899, "accuracy_n": 59, "auc": 0.7310924369747899}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7132796780684104, "accuracy_n": 994, "auc": 0.7132796780684104}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8480885311871227, "accuracy_n": 994, "auc": 0.8480885311871227}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5446339017051154, "accuracy_n": 997, "auc": 0.5446339017051154}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.378, "accuracy_n": 500, "auc": 0.378}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3311036789297659, "accuracy_n": 299, "auc": 0.3311036789297659}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3725, "accuracy_n": 400, "auc": 0.3725}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8438538205980066, "accuracy_n": 301, "auc": 0.8438538205980066}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9199876428792091, "accuracy_n": 322, "auc": 0.9199876428792091}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8148966165413534, "accuracy_n": 292, "auc": 0.8148966165413534}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9782082324455206, "accuracy_n": 413, "auc": 0.9782082324455206}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5306042108987968, "accuracy_n": 1902, "auc": 0.5306042108987968}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5905181770619194, "accuracy_n": 2000, "auc": 0.5905181770619194}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6638655462184874, "accuracy_n": 59, "auc": 0.6638655462184874}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7947686116700201, "accuracy_n": 994, "auc": 0.7947686116700201}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5925553319919518, "accuracy_n": 994, "auc": 0.5925553319919518}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5947843530591775, "accuracy_n": 997, "auc": 0.5947843530591775}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.392, "accuracy_n": 500, "auc": 0.392}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.354, "accuracy_n": 500, "auc": 0.354}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4214046822742475, "accuracy_n": 299, "auc": 0.4214046822742475}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 400, "auc": 0.61}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.654485049833887, "accuracy_n": 301, "auc": 0.654485049833887}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9928174235403151, "accuracy_n": 322, "auc": 0.9928174235403151}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8426221804511278, "accuracy_n": 292, "auc": 0.8426221804511278}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5583676132342534, "accuracy_n": 1902, "auc": 0.5583676132342534}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5349431144643216, "accuracy_n": 2000, "auc": 0.5349431144643216}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 59, "auc": 0.7142857142857143}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5352112676056338, "accuracy_n": 994, "auc": 0.5352112676056338}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5241448692152918, "accuracy_n": 994, "auc": 0.5241448692152918}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.364, "accuracy_n": 500, "auc": 0.364}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.334, "accuracy_n": 500, "auc": 0.334}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.39464882943143814, "accuracy_n": 299, "auc": 0.39464882943143814}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5575, "accuracy_n": 400, "auc": 0.5575}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5946843853820598, "accuracy_n": 301, "auc": 0.5946843853820598}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9432730923694779, "accuracy_n": 322, "auc": 0.9432730923694779}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8772086466165413, "accuracy_n": 292, "auc": 0.8772086466165413}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5883777239709443, "accuracy_n": 413, "auc": 0.5883777239709443}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5406426928520878, "accuracy_n": 1902, "auc": 0.5406426928520878}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5020377356225597, "accuracy_n": 2000, "auc": 0.5020377356225597}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6316526610644259, "accuracy_n": 59, "auc": 0.6316526610644259}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5754527162977867, "accuracy_n": 994, "auc": 0.5754527162977867}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5513078470824949, "accuracy_n": 994, "auc": 0.5513078470824949}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.366, "accuracy_n": 500, "auc": 0.366}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3377926421404682, "accuracy_n": 299, "auc": 0.3377926421404682}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.485, "accuracy_n": 400, "auc": 0.485}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5149501661129569, "accuracy_n": 301, "auc": 0.5149501661129569}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6427247451343836, "accuracy_n": 322, "auc": 0.6427247451343836}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5285244360902256, "accuracy_n": 292, "auc": 0.5285244360902256}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.801452784503632, "accuracy_n": 413, "auc": 0.801452784503632}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.502068957891012, "accuracy_n": 1902, "auc": 0.502068957891012}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5725176788820764, "accuracy_n": 2000, "auc": 0.5725176788820764}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6316526610644257, "accuracy_n": 59, "auc": 0.6316526610644257}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7692307692307692, "accuracy_n": 23, "auc": 0.7692307692307692}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8128772635814889, "accuracy_n": 994, "auc": 0.8128772635814889}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5593561368209256, "accuracy_n": 994, "auc": 0.5593561368209256}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6810431293881645, "accuracy_n": 997, "auc": 0.6810431293881645}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.322, "accuracy_n": 500, "auc": 0.322}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4775, "accuracy_n": 400, "auc": 0.4775}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6146179401993356, "accuracy_n": 301, "auc": 0.6146179401993356}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7806611059623109, "accuracy_n": 322, "auc": 0.7806611059623109}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7025845864661654, "accuracy_n": 292, "auc": 0.7025845864661654}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9308010438782732, "accuracy_n": 1902, "auc": 0.9308010438782732}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5145857654613315, "accuracy_n": 2000, "auc": 0.5145857654613315}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6302521008403361, "accuracy_n": 59, "auc": 0.6302521008403361}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6327967806841046, "accuracy_n": 994, "auc": 0.6327967806841046}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6116700201207244, "accuracy_n": 994, "auc": 0.6116700201207244}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.328, "accuracy_n": 500, "auc": 0.328}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 500, "auc": 0.28}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.46, "accuracy_n": 400, "auc": 0.46}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5808619091751622, "accuracy_n": 322, "auc": 0.5808619091751622}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5234022556390978, "accuracy_n": 292, "auc": 0.5234022556390978}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6900726392251816, "accuracy_n": 413, "auc": 0.6900726392251816}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5060830237084218, "accuracy_n": 1902, "auc": 0.5060830237084218}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5988071693881492, "accuracy_n": 2000, "auc": 0.5988071693881492}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6400560224089635, "accuracy_n": 59, "auc": 0.6400560224089635}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5171026156941649, "accuracy_n": 994, "auc": 0.5171026156941649}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6066398390342053, "accuracy_n": 994, "auc": 0.6066398390342053}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6639919759277834, "accuracy_n": 997, "auc": 0.6639919759277834}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.438, "accuracy_n": 500, "auc": 0.438}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.468, "accuracy_n": 500, "auc": 0.468}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5016722408026756, "accuracy_n": 299, "auc": 0.5016722408026756}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6375, "accuracy_n": 400, "auc": 0.6375}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5083056478405316, "accuracy_n": 301, "auc": 0.5083056478405316}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9849011430336732, "accuracy_n": 322, "auc": 0.9849011430336732}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.63078007518797, "accuracy_n": 292, "auc": 0.63078007518797}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.864406779661017, "accuracy_n": 413, "auc": 0.864406779661017}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.598330237084218, "accuracy_n": 1902, "auc": 0.598330237084218}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6234680719739826, "accuracy_n": 2000, "auc": 0.6234680719739826}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7100840336134454, "accuracy_n": 59, "auc": 0.7100840336134454}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5352112676056338, "accuracy_n": 994, "auc": 0.5352112676056338}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5744466800804829, "accuracy_n": 994, "auc": 0.5744466800804829}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2675, "accuracy_n": 400, "auc": 0.2675}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7501158480074143, "accuracy_n": 322, "auc": 0.7501158480074143}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5618421052631579, "accuracy_n": 292, "auc": 0.5618421052631579}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8619854721549637, "accuracy_n": 413, "auc": 0.8619854721549637}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5210589171974522, "accuracy_n": 1902, "auc": 0.5210589171974522}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.511412619955804, "accuracy_n": 2000, "auc": 0.511412619955804}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6120448179271709, "accuracy_n": 59, "auc": 0.6120448179271709}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
