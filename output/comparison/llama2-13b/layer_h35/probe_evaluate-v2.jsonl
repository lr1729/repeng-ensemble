{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9074446680080482, "accuracy_n": 994, "auc": 0.9074446680080482}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6128385155466399, "accuracy_n": 997, "auc": 0.6128385155466399}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.394, "accuracy_n": 500, "auc": 0.394}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.392, "accuracy_n": 500, "auc": 0.392}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4080267558528428, "accuracy_n": 299, "auc": 0.4080267558528428}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5575, "accuracy_n": 400, "auc": 0.5575}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5681063122923588, "accuracy_n": 301, "auc": 0.5681063122923588}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6211383997528577, "accuracy_n": 322, "auc": 0.6211383997528577}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5951597744360902, "accuracy_n": 292, "auc": 0.5951597744360902}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5626105803255484, "accuracy_n": 1902, "auc": 0.5626105803255484}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5168996007558728, "accuracy_n": 2000, "auc": 0.5168996007558728}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6750700280112045, "accuracy_n": 59, "auc": 0.6750700280112045}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9064386317907445, "accuracy_n": 994, "auc": 0.9064386317907445}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7877263581488934, "accuracy_n": 994, "auc": 0.7877263581488934}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.452, "accuracy_n": 500, "auc": 0.452}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.496, "accuracy_n": 500, "auc": 0.496}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5250836120401338, "accuracy_n": 299, "auc": 0.5250836120401338}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6325, "accuracy_n": 400, "auc": 0.6325}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6843853820598007, "accuracy_n": 301, "auc": 0.6843853820598007}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9090593141797961, "accuracy_n": 322, "auc": 0.9090593141797961}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7943609022556392, "accuracy_n": 292, "auc": 0.7943609022556392}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6738156847133757, "accuracy_n": 1902, "auc": 0.6738156847133757}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5498534971124576, "accuracy_n": 2000, "auc": 0.5498534971124576}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5406162464985995, "accuracy_n": 59, "auc": 0.5406162464985995}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9104627766599598, "accuracy_n": 994, "auc": 0.9104627766599598}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6458752515090543, "accuracy_n": 994, "auc": 0.6458752515090543}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5740072202166066, "accuracy_n": 277, "auc": 0.5740072202166066}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.326, "accuracy_n": 500, "auc": 0.326}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.33444816053511706, "accuracy_n": 299, "auc": 0.33444816053511706}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3725, "accuracy_n": 400, "auc": 0.3725}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5448504983388704, "accuracy_n": 301, "auc": 0.5448504983388704}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7007259808464628, "accuracy_n": 322, "auc": 0.7007259808464628}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6231672932330827, "accuracy_n": 292, "auc": 0.6231672932330827}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5792573425336165, "accuracy_n": 1902, "auc": 0.5792573425336165}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5248504710200382, "accuracy_n": 2000, "auc": 0.5248504710200382}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5742296918767508, "accuracy_n": 59, "auc": 0.5742296918767508}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7565392354124748, "accuracy_n": 994, "auc": 0.7565392354124748}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6941649899396378, "accuracy_n": 994, "auc": 0.6941649899396378}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6018054162487463, "accuracy_n": 997, "auc": 0.6018054162487463}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.402, "accuracy_n": 500, "auc": 0.402}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.45, "accuracy_n": 500, "auc": 0.45}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5016722408026756, "accuracy_n": 299, "auc": 0.5016722408026756}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6275, "accuracy_n": 400, "auc": 0.6275}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5016611295681063, "accuracy_n": 301, "auc": 0.5016611295681063}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5985480383070745, "accuracy_n": 322, "auc": 0.5985480383070745}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6174342105263158, "accuracy_n": 292, "auc": 0.6174342105263158}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5156261058032554, "accuracy_n": 1902, "auc": 0.5156261058032554}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5622549740456305, "accuracy_n": 2000, "auc": 0.5622549740456305}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6988795518207283, "accuracy_n": 59, "auc": 0.6988795518207283}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7923076923076924, "accuracy_n": 23, "auc": 0.7923076923076924}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5221327967806841, "accuracy_n": 994, "auc": 0.5221327967806841}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5975855130784709, "accuracy_n": 994, "auc": 0.5975855130784709}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.408, "accuracy_n": 500, "auc": 0.408}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.391304347826087, "accuracy_n": 299, "auc": 0.391304347826087}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5325, "accuracy_n": 400, "auc": 0.5325}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5095381526104417, "accuracy_n": 322, "auc": 0.5095381526104417}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6802631578947368, "accuracy_n": 292, "auc": 0.6802631578947368}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5375302663438256, "accuracy_n": 413, "auc": 0.5375302663438256}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5268577494692144, "accuracy_n": 1902, "auc": 0.5268577494692144}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5210531001691611, "accuracy_n": 2000, "auc": 0.5210531001691611}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8843058350100603, "accuracy_n": 994, "auc": 0.8843058350100603}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5362173038229376, "accuracy_n": 994, "auc": 0.5362173038229376}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6609829488465396, "accuracy_n": 997, "auc": 0.6609829488465396}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 500, "auc": 0.58}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.578, "accuracy_n": 500, "auc": 0.578}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5919732441471572, "accuracy_n": 299, "auc": 0.5919732441471572}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.521594684385382, "accuracy_n": 301, "auc": 0.521594684385382}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9626969416126043, "accuracy_n": 322, "auc": 0.9626969416126043}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8118890977443608, "accuracy_n": 292, "auc": 0.8118890977443608}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7651331719128329, "accuracy_n": 413, "auc": 0.7651331719128329}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5593716825902335, "accuracy_n": 1902, "auc": 0.5593716825902335}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5141896224537058, "accuracy_n": 2000, "auc": 0.5141896224537058}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7044817927170868, "accuracy_n": 59, "auc": 0.7044817927170868}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 994, "auc": 0.6237424547283702}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7766599597585513, "accuracy_n": 994, "auc": 0.7766599597585513}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.555956678700361, "accuracy_n": 277, "auc": 0.555956678700361}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7041123370110332, "accuracy_n": 997, "auc": 0.7041123370110332}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.556, "accuracy_n": 500, "auc": 0.556}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.604, "accuracy_n": 500, "auc": 0.604}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6187290969899666, "accuracy_n": 299, "auc": 0.6187290969899666}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 400, "auc": 0.67}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.574750830564784, "accuracy_n": 301, "auc": 0.574750830564784}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8251467408093914, "accuracy_n": 322, "auc": 0.8251467408093914}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5452067669172932, "accuracy_n": 292, "auc": 0.5452067669172932}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5397602618542109, "accuracy_n": 1902, "auc": 0.5397602618542109}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5532827350673593, "accuracy_n": 2000, "auc": 0.5532827350673593}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7549019607843137, "accuracy_n": 59, "auc": 0.7549019607843137}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8702213279678068, "accuracy_n": 994, "auc": 0.8702213279678068}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7394366197183099, "accuracy_n": 994, "auc": 0.7394366197183099}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6980942828485457, "accuracy_n": 997, "auc": 0.6980942828485457}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.542, "accuracy_n": 500, "auc": 0.542}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.606, "accuracy_n": 500, "auc": 0.606}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6675, "accuracy_n": 400, "auc": 0.6675}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5481727574750831, "accuracy_n": 301, "auc": 0.5481727574750831}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9828544949026877, "accuracy_n": 322, "auc": 0.9828544949026877}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8300281954887219, "accuracy_n": 292, "auc": 0.8300281954887219}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6271186440677966, "accuracy_n": 413, "auc": 0.6271186440677966}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5475771850672329, "accuracy_n": 1902, "auc": 0.5475771850672329}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5422457507160084, "accuracy_n": 2000, "auc": 0.5422457507160084}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.803921568627451, "accuracy_n": 59, "auc": 0.803921568627451}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.523076923076923, "accuracy_n": 23, "auc": 0.523076923076923}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8832997987927566, "accuracy_n": 994, "auc": 0.8832997987927566}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7676056338028169, "accuracy_n": 994, "auc": 0.7676056338028169}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6990972918756269, "accuracy_n": 997, "auc": 0.6990972918756269}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.556, "accuracy_n": 500, "auc": 0.556}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.638, "accuracy_n": 500, "auc": 0.638}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6187290969899666, "accuracy_n": 299, "auc": 0.6187290969899666}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6825, "accuracy_n": 400, "auc": 0.6825}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5282392026578073, "accuracy_n": 301, "auc": 0.5282392026578073}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9334260117392648, "accuracy_n": 322, "auc": 0.9334260117392648}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7393796992481203, "accuracy_n": 292, "auc": 0.7393796992481203}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5718473549186129, "accuracy_n": 1902, "auc": 0.5718473549186129}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.573609072875308, "accuracy_n": 2000, "auc": 0.573609072875308}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8081232492997199, "accuracy_n": 59, "auc": 0.8081232492997199}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6297786720321932, "accuracy_n": 994, "auc": 0.6297786720321932}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8561368209255533, "accuracy_n": 994, "auc": 0.8561368209255533}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.464, "accuracy_n": 500, "auc": 0.464}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.295, "accuracy_n": 400, "auc": 0.295}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9302325581395349, "accuracy_n": 301, "auc": 0.9302325581395349}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9213778189681805, "accuracy_n": 322, "auc": 0.9213778189681805}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.775093984962406, "accuracy_n": 292, "auc": 0.775093984962406}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 413, "auc": 0.7142857142857143}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6565728945506015, "accuracy_n": 1902, "auc": 0.6565728945506015}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6177390037803647, "accuracy_n": 2000, "auc": 0.6177390037803647}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7184873949579832, "accuracy_n": 59, "auc": 0.7184873949579832}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7203219315895373, "accuracy_n": 994, "auc": 0.7203219315895373}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7776659959758552, "accuracy_n": 994, "auc": 0.7776659959758552}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5255767301905717, "accuracy_n": 997, "auc": 0.5255767301905717}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3210702341137124, "accuracy_n": 299, "auc": 0.3210702341137124}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.295, "accuracy_n": 400, "auc": 0.295}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7475083056478405, "accuracy_n": 301, "auc": 0.7475083056478405}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8657707754093296, "accuracy_n": 322, "auc": 0.8657707754093296}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7015037593984961, "accuracy_n": 292, "auc": 0.7015037593984961}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8111380145278451, "accuracy_n": 413, "auc": 0.8111380145278451}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5115954529370135, "accuracy_n": 1902, "auc": 0.5115954529370135}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5830244718343323, "accuracy_n": 2000, "auc": 0.5830244718343323}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6582633053221288, "accuracy_n": 59, "auc": 0.6582633053221288}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.772635814889336, "accuracy_n": 994, "auc": 0.772635814889336}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6126760563380281, "accuracy_n": 994, "auc": 0.6126760563380281}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5667870036101083, "accuracy_n": 277, "auc": 0.5667870036101083}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5687061183550652, "accuracy_n": 997, "auc": 0.5687061183550652}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.414, "accuracy_n": 500, "auc": 0.414}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.346, "accuracy_n": 500, "auc": 0.346}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.431438127090301, "accuracy_n": 299, "auc": 0.431438127090301}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 400, "auc": 0.61}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6345514950166113, "accuracy_n": 301, "auc": 0.6345514950166113}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9904232313870869, "accuracy_n": 322, "auc": 0.9904232313870869}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8355733082706768, "accuracy_n": 292, "auc": 0.8355733082706768}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.57293546532201, "accuracy_n": 1902, "auc": 0.57293546532201}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5260869173771732, "accuracy_n": 2000, "auc": 0.5260869173771732}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7114845938375349, "accuracy_n": 59, "auc": 0.7114845938375349}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6388329979879276, "accuracy_n": 994, "auc": 0.6388329979879276}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5523138832997988, "accuracy_n": 994, "auc": 0.5523138832997988}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.386, "accuracy_n": 500, "auc": 0.386}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.40468227424749165, "accuracy_n": 299, "auc": 0.40468227424749165}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.575, "accuracy_n": 400, "auc": 0.575}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5382059800664452, "accuracy_n": 301, "auc": 0.5382059800664452}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9321902996601792, "accuracy_n": 322, "auc": 0.9321902996601792}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8746710526315788, "accuracy_n": 292, "auc": 0.8746710526315788}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5446855095541401, "accuracy_n": 1902, "auc": 0.5446855095541401}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5050183116104914, "accuracy_n": 2000, "auc": 0.5050183116104914}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6190476190476191, "accuracy_n": 59, "auc": 0.6190476190476191}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.635814889336016, "accuracy_n": 994, "auc": 0.635814889336016}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5462776659959758, "accuracy_n": 994, "auc": 0.5462776659959758}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6208625877632898, "accuracy_n": 997, "auc": 0.6208625877632898}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.368, "accuracy_n": 500, "auc": 0.368}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3879598662207358, "accuracy_n": 299, "auc": 0.3879598662207358}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.505, "accuracy_n": 400, "auc": 0.505}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5182724252491694, "accuracy_n": 301, "auc": 0.5182724252491694}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6565106580166821, "accuracy_n": 322, "auc": 0.6565106580166821}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5030075187969925, "accuracy_n": 292, "auc": 0.5030075187969925}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7675544794188862, "accuracy_n": 413, "auc": 0.7675544794188862}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.509190330856334, "accuracy_n": 1902, "auc": 0.509190330856334}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.562615104052563, "accuracy_n": 2000, "auc": 0.562615104052563}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6470588235294118, "accuracy_n": 59, "auc": 0.6470588235294118}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7826961770623743, "accuracy_n": 994, "auc": 0.7826961770623743}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5784708249496981, "accuracy_n": 994, "auc": 0.5784708249496981}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6068204613841525, "accuracy_n": 997, "auc": 0.6068204613841525}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4225, "accuracy_n": 400, "auc": 0.4225}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 301, "auc": 0.5714285714285714}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.740268767377201, "accuracy_n": 322, "auc": 0.740268767377201}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6666353383458647, "accuracy_n": 292, "auc": 0.6666353383458647}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8572529635527246, "accuracy_n": 1902, "auc": 0.8572529635527246}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5031516377412246, "accuracy_n": 2000, "auc": 0.5031516377412246}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5945674044265593, "accuracy_n": 994, "auc": 0.5945674044265593}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5915492957746479, "accuracy_n": 994, "auc": 0.5915492957746479}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5847542627883651, "accuracy_n": 997, "auc": 0.5847542627883651}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3511705685618729, "accuracy_n": 299, "auc": 0.3511705685618729}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4125, "accuracy_n": 400, "auc": 0.4125}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5016611295681063, "accuracy_n": 301, "auc": 0.5016611295681063}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5856502934816188, "accuracy_n": 322, "auc": 0.5856502934816188}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.537453007518797, "accuracy_n": 292, "auc": 0.537453007518797}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6464891041162227, "accuracy_n": 413, "auc": 0.6464891041162227}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5070395435244162, "accuracy_n": 1902, "auc": 0.5070395435244162}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5907232510936448, "accuracy_n": 2000, "auc": 0.5907232510936448}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6442577030812324, "accuracy_n": 59, "auc": 0.6442577030812324}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.653846153846154, "accuracy_n": 23, "auc": 0.653846153846154}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6841046277665996, "accuracy_n": 994, "auc": 0.6841046277665996}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6036217303822937, "accuracy_n": 994, "auc": 0.6036217303822937}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5566700100300903, "accuracy_n": 997, "auc": 0.5566700100300903}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.41, "accuracy_n": 500, "auc": 0.41}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.482, "accuracy_n": 500, "auc": 0.482}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4882943143812709, "accuracy_n": 299, "auc": 0.4882943143812709}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5875, "accuracy_n": 400, "auc": 0.5875}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5182724252491694, "accuracy_n": 301, "auc": 0.5182724252491694}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9888785912882299, "accuracy_n": 322, "auc": 0.9888785912882299}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6353383458646616, "accuracy_n": 292, "auc": 0.6353383458646616}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.927360774818402, "accuracy_n": 413, "auc": 0.927360774818402}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6350185774946921, "accuracy_n": 1902, "auc": 0.6350185774946921}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6189634458039353, "accuracy_n": 2000, "auc": 0.6189634458039353}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7016806722689075, "accuracy_n": 59, "auc": 0.7016806722689075}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5070422535211268, "accuracy_n": 994, "auc": 0.5070422535211268}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5050301810865191, "accuracy_n": 994, "auc": 0.5050301810865191}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.23411371237458195, "accuracy_n": 299, "auc": 0.23411371237458195}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.2525, "accuracy_n": 400, "auc": 0.2525}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7242045103490886, "accuracy_n": 322, "auc": 0.7242045103490886}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.574812030075188, "accuracy_n": 292, "auc": 0.574812030075188}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8353510895883777, "accuracy_n": 413, "auc": 0.8353510895883777}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5229332537154989, "accuracy_n": 1902, "auc": 0.5229332537154989}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.500994859144151, "accuracy_n": 2000, "auc": 0.500994859144151}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5868347338935573, "accuracy_n": 59, "auc": 0.5868347338935573}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
