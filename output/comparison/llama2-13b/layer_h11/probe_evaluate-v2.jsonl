{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8712273641851107, "accuracy_n": 994, "auc": 0.8712273641851107}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6248746238716149, "accuracy_n": 997, "auc": 0.6248746238716149}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.268, "accuracy_n": 500, "auc": 0.268}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2675, "accuracy_n": 400, "auc": 0.2675}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6710963455149501, "accuracy_n": 301, "auc": 0.6710963455149501}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9475208526413346, "accuracy_n": 322, "auc": 0.9475208526413346}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8369360902255639, "accuracy_n": 292, "auc": 0.8369360902255639}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7090465764331211, "accuracy_n": 1902, "auc": 0.7090465764331211}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7477839500059522, "accuracy_n": 2000, "auc": 0.7477839500059522}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7885154061624648, "accuracy_n": 59, "auc": 0.7885154061624648}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9265593561368209, "accuracy_n": 994, "auc": 0.9265593561368209}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8259557344064387, "accuracy_n": 994, "auc": 0.8259557344064387}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6228686058174524, "accuracy_n": 997, "auc": 0.6228686058174524}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27424749163879597, "accuracy_n": 299, "auc": 0.27424749163879597}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3125, "accuracy_n": 400, "auc": 0.3125}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9401993355481728, "accuracy_n": 301, "auc": 0.9401993355481728}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9270929873339512, "accuracy_n": 322, "auc": 0.9270929873339512}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8272556390977444, "accuracy_n": 292, "auc": 0.8272556390977444}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7580082271762207, "accuracy_n": 1902, "auc": 0.7580082271762207}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7687545203818579, "accuracy_n": 2000, "auc": 0.7687545203818579}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5840336134453782, "accuracy_n": 59, "auc": 0.5840336134453782}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8538461538461539, "accuracy_n": 23, "auc": 0.8538461538461539}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 994, "auc": 0.5855130784708249}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5727181544633901, "accuracy_n": 997, "auc": 0.5727181544633901}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2925, "accuracy_n": 400, "auc": 0.2925}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7973421926910299, "accuracy_n": 301, "auc": 0.7973421926910299}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.889017608897127, "accuracy_n": 322, "auc": 0.889017608897127}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7443609022556391, "accuracy_n": 292, "auc": 0.7443609022556391}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6850207891012031, "accuracy_n": 1902, "auc": 0.6850207891012031}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6916356804806535, "accuracy_n": 2000, "auc": 0.6916356804806535}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7170868347338936, "accuracy_n": 59, "auc": 0.7170868347338936}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5261569416498993, "accuracy_n": 994, "auc": 0.5261569416498993}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.326, "accuracy_n": 500, "auc": 0.326}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24749163879598662, "accuracy_n": 299, "auc": 0.24749163879598662}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2925, "accuracy_n": 400, "auc": 0.2925}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5880398671096345, "accuracy_n": 301, "auc": 0.5880398671096345}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8425239419215325, "accuracy_n": 322, "auc": 0.8425239419215325}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8328947368421052, "accuracy_n": 292, "auc": 0.8328947368421052}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6081486641896674, "accuracy_n": 1902, "auc": 0.6081486641896674}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6274745183011067, "accuracy_n": 2000, "auc": 0.6274745183011067}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6890756302521008, "accuracy_n": 59, "auc": 0.6890756302521008}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 23, "auc": 0.8461538461538461}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.528169014084507, "accuracy_n": 994, "auc": 0.528169014084507}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2508361204013378, "accuracy_n": 299, "auc": 0.2508361204013378}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2425, "accuracy_n": 400, "auc": 0.2425}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8292400370713624, "accuracy_n": 322, "auc": 0.8292400370713624}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7978383458646617, "accuracy_n": 292, "auc": 0.7978383458646617}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7435531670205238, "accuracy_n": 1902, "auc": 0.7435531670205238}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6548754100230183, "accuracy_n": 2000, "auc": 0.6548754100230183}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5100603621730382, "accuracy_n": 994, "auc": 0.5100603621730382}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6208625877632898, "accuracy_n": 997, "auc": 0.6208625877632898}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.494, "accuracy_n": 500, "auc": 0.494}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.34448160535117056, "accuracy_n": 299, "auc": 0.34448160535117056}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.42, "accuracy_n": 400, "auc": 0.42}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6212624584717608, "accuracy_n": 301, "auc": 0.6212624584717608}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7957213469261661, "accuracy_n": 322, "auc": 0.7957213469261661}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6976973684210526, "accuracy_n": 292, "auc": 0.6976973684210526}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6464891041162227, "accuracy_n": 413, "auc": 0.6464891041162227}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7067852087756548, "accuracy_n": 1902, "auc": 0.7067852087756548}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5142296368989205, "accuracy_n": 2000, "auc": 0.5142296368989205}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6106442577030812, "accuracy_n": 59, "auc": 0.6106442577030812}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.416, "accuracy_n": 500, "auc": 0.416}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.452, "accuracy_n": 500, "auc": 0.452}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.36454849498327757, "accuracy_n": 299, "auc": 0.36454849498327757}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4675, "accuracy_n": 400, "auc": 0.4675}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6411960132890365, "accuracy_n": 301, "auc": 0.6411960132890365}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7345535990114304, "accuracy_n": 322, "auc": 0.7345535990114304}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5393327067669172, "accuracy_n": 292, "auc": 0.5393327067669172}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5641646489104116, "accuracy_n": 413, "auc": 0.5641646489104116}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5179748319179052, "accuracy_n": 1902, "auc": 0.5179748319179052}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6457030988186734, "accuracy_n": 2000, "auc": 0.6457030988186734}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 59, "auc": 0.5294117647058824}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7846153846153847, "accuracy_n": 23, "auc": 0.7846153846153847}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5925553319919518, "accuracy_n": 994, "auc": 0.5925553319919518}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5995975855130785, "accuracy_n": 994, "auc": 0.5995975855130785}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.42, "accuracy_n": 500, "auc": 0.42}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.364, "accuracy_n": 500, "auc": 0.364}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.34782608695652173, "accuracy_n": 299, "auc": 0.34782608695652173}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4475, "accuracy_n": 400, "auc": 0.4475}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8006644518272426, "accuracy_n": 301, "auc": 0.8006644518272426}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7903923385851097, "accuracy_n": 322, "auc": 0.7903923385851097}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6919642857142857, "accuracy_n": 292, "auc": 0.6919642857142857}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.644655652866242, "accuracy_n": 1902, "auc": 0.644655652866242}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5298892900337021, "accuracy_n": 2000, "auc": 0.5298892900337021}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6316526610644259, "accuracy_n": 59, "auc": 0.6316526610644259}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6519114688128773, "accuracy_n": 994, "auc": 0.6519114688128773}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7424547283702213, "accuracy_n": 994, "auc": 0.7424547283702213}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6108324974924775, "accuracy_n": 997, "auc": 0.6108324974924775}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.426, "accuracy_n": 500, "auc": 0.426}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.462, "accuracy_n": 500, "auc": 0.462}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3979933110367893, "accuracy_n": 299, "auc": 0.3979933110367893}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.555, "accuracy_n": 400, "auc": 0.555}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8106312292358804, "accuracy_n": 301, "auc": 0.8106312292358804}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5716713005869631, "accuracy_n": 322, "auc": 0.5716713005869631}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6033834586466166, "accuracy_n": 292, "auc": 0.6033834586466166}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7263922518159807, "accuracy_n": 413, "auc": 0.7263922518159807}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5131126150035386, "accuracy_n": 1902, "auc": 0.5131126150035386}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5077943137472627, "accuracy_n": 2000, "auc": 0.5077943137472627}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5406162464985995, "accuracy_n": 59, "auc": 0.5406162464985995}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8702213279678068, "accuracy_n": 994, "auc": 0.8702213279678068}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8913480885311871, "accuracy_n": 994, "auc": 0.8913480885311871}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.629889669007021, "accuracy_n": 997, "auc": 0.629889669007021}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.312, "accuracy_n": 500, "auc": 0.312}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 400, "auc": 0.32}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9235880398671097, "accuracy_n": 301, "auc": 0.9235880398671097}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9475208526413346, "accuracy_n": 322, "auc": 0.9475208526413346}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8104793233082707, "accuracy_n": 292, "auc": 0.8104793233082707}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6973365617433414, "accuracy_n": 413, "auc": 0.6973365617433414}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7433596514508138, "accuracy_n": 1902, "auc": 0.7433596514508138}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7619700711957017, "accuracy_n": 2000, "auc": 0.7619700711957017}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7380952380952381, "accuracy_n": 59, "auc": 0.7380952380952381}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9275653923541247, "accuracy_n": 994, "auc": 0.9275653923541247}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 994, "auc": 0.5553319919517102}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.29431438127090304, "accuracy_n": 299, "auc": 0.29431438127090304}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3225, "accuracy_n": 400, "auc": 0.3225}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7408637873754153, "accuracy_n": 301, "auc": 0.7408637873754153}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9484476367006487, "accuracy_n": 322, "auc": 0.9484476367006487}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7949718045112782, "accuracy_n": 292, "auc": 0.7949718045112782}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8910411622276029, "accuracy_n": 413, "auc": 0.8910411622276029}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5703412508846426, "accuracy_n": 1902, "auc": 0.5703412508846426}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7067941526891207, "accuracy_n": 2000, "auc": 0.7067941526891207}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.707282913165266, "accuracy_n": 59, "auc": 0.707282913165266}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.903420523138833, "accuracy_n": 994, "auc": 0.903420523138833}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7937625754527163, "accuracy_n": 994, "auc": 0.7937625754527163}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.324, "accuracy_n": 500, "auc": 0.324}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.215, "accuracy_n": 400, "auc": 0.215}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9833887043189369, "accuracy_n": 301, "auc": 0.9833887043189369}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9751312944084028, "accuracy_n": 322, "auc": 0.9751312944084028}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9174342105263158, "accuracy_n": 292, "auc": 0.9174342105263158}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5399515738498789, "accuracy_n": 413, "auc": 0.5399515738498789}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6168900389242746, "accuracy_n": 1902, "auc": 0.6168900389242746}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7879584530015336, "accuracy_n": 2000, "auc": 0.7879584530015336}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7913165266106442, "accuracy_n": 59, "auc": 0.7913165266106442}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8993963782696177, "accuracy_n": 994, "auc": 0.8993963782696177}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.185, "accuracy_n": 400, "auc": 0.185}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8803986710963455, "accuracy_n": 301, "auc": 0.8803986710963455}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9548192771084337, "accuracy_n": 322, "auc": 0.9548192771084337}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9187500000000001, "accuracy_n": 292, "auc": 0.9187500000000001}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5520581113801453, "accuracy_n": 413, "auc": 0.5520581113801453}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5742557944090587, "accuracy_n": 1902, "auc": 0.5742557944090587}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.750476922168903, "accuracy_n": 2000, "auc": 0.750476922168903}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7408963585434174, "accuracy_n": 59, "auc": 0.7408963585434174}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5704225352112676, "accuracy_n": 994, "auc": 0.5704225352112676}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.33, "accuracy_n": 500, "auc": 0.33}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.255, "accuracy_n": 400, "auc": 0.255}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5382059800664452, "accuracy_n": 301, "auc": 0.5382059800664452}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8178483163422922, "accuracy_n": 322, "auc": 0.8178483163422922}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.600187969924812, "accuracy_n": 292, "auc": 0.600187969924812}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5665859564164649, "accuracy_n": 413, "auc": 0.5665859564164649}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5585401185421089, "accuracy_n": 1902, "auc": 0.5585401185421089}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6042231245479619, "accuracy_n": 2000, "auc": 0.6042231245479619}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7394957983193278, "accuracy_n": 59, "auc": 0.7394957983193278}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.576923076923077, "accuracy_n": 23, "auc": 0.576923076923077}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5271629778672032, "accuracy_n": 994, "auc": 0.5271629778672032}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6730382293762576, "accuracy_n": 994, "auc": 0.6730382293762576}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.338, "accuracy_n": 500, "auc": 0.338}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.1875, "accuracy_n": 400, "auc": 0.1875}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8870431893687708, "accuracy_n": 301, "auc": 0.8870431893687708}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7983858510966945, "accuracy_n": 322, "auc": 0.7983858510966945}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6625469924812031, "accuracy_n": 292, "auc": 0.6625469924812031}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9760317144373674, "accuracy_n": 1902, "auc": 0.9760317144373674}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5871159488575375, "accuracy_n": 2000, "auc": 0.5871159488575375}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5448179271708683, "accuracy_n": 59, "auc": 0.5448179271708683}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9175050301810865, "accuracy_n": 994, "auc": 0.9175050301810865}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8953722334004024, "accuracy_n": 994, "auc": 0.8953722334004024}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2, "accuracy_n": 400, "auc": 0.2}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9435215946843853, "accuracy_n": 301, "auc": 0.9435215946843853}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.990345999382144, "accuracy_n": 322, "auc": 0.990345999382144}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9532424812030076, "accuracy_n": 292, "auc": 0.9532424812030076}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8813559322033898, "accuracy_n": 413, "auc": 0.8813559322033898}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6880506900212314, "accuracy_n": 1902, "auc": 0.6880506900212314}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8521456245704699, "accuracy_n": 2000, "auc": 0.8521456245704699}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7801120448179272, "accuracy_n": 59, "auc": 0.7801120448179272}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8893360160965795, "accuracy_n": 994, "auc": 0.8893360160965795}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7676056338028169, "accuracy_n": 994, "auc": 0.7676056338028169}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6098294884653962, "accuracy_n": 997, "auc": 0.6098294884653962}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.328, "accuracy_n": 500, "auc": 0.328}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2976588628762542, "accuracy_n": 299, "auc": 0.2976588628762542}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2075, "accuracy_n": 400, "auc": 0.2075}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8538205980066446, "accuracy_n": 301, "auc": 0.8538205980066446}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9551282051282051, "accuracy_n": 322, "auc": 0.9551282051282051}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8337875939849624, "accuracy_n": 292, "auc": 0.8337875939849624}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8208232445520581, "accuracy_n": 413, "auc": 0.8208232445520581}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6997257607926397, "accuracy_n": 1902, "auc": 0.6997257607926397}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6981600357729141, "accuracy_n": 2000, "auc": 0.6981600357729141}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7128851540616247, "accuracy_n": 59, "auc": 0.7128851540616247}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7253521126760564, "accuracy_n": 994, "auc": 0.7253521126760564}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5080482897384306, "accuracy_n": 994, "auc": 0.5080482897384306}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2709030100334448, "accuracy_n": 299, "auc": 0.2709030100334448}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2625, "accuracy_n": 400, "auc": 0.2625}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7873754152823921, "accuracy_n": 301, "auc": 0.7873754152823921}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5347930182267532, "accuracy_n": 322, "auc": 0.5347930182267532}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5298872180451127, "accuracy_n": 292, "auc": 0.5298872180451127}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.774818401937046, "accuracy_n": 413, "auc": 0.774818401937046}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5968794232130219, "accuracy_n": 1902, "auc": 0.5968794232130219}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5300203373417804, "accuracy_n": 2000, "auc": 0.5300203373417804}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6918767507002801, "accuracy_n": 59, "auc": 0.6918767507002801}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
