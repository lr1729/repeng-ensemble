{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9275653923541247, "accuracy_n": 994, "auc": 0.9275653923541247}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9426559356136821, "accuracy_n": 994, "auc": 0.9426559356136821}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6750902527075813, "accuracy_n": 277, "auc": 0.6750902527075813}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6379137412236711, "accuracy_n": 997, "auc": 0.6379137412236711}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39, "accuracy_n": 500, "auc": 0.39}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4414715719063545, "accuracy_n": 299, "auc": 0.4414715719063545}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.565, "accuracy_n": 400, "auc": 0.565}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6511627906976745, "accuracy_n": 301, "auc": 0.6511627906976745}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9953274637009577, "accuracy_n": 322, "auc": 0.9953274637009577}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.549765037593985, "accuracy_n": 292, "auc": 0.549765037593985}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6787641542816703, "accuracy_n": 1902, "auc": 0.6787641542816703}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6985691834752346, "accuracy_n": 2000, "auc": 0.6985691834752346}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7450980392156863, "accuracy_n": 59, "auc": 0.7450980392156863}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.559679037111334, "accuracy_n": 997, "auc": 0.559679037111334}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.418, "accuracy_n": 500, "auc": 0.418}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.516, "accuracy_n": 500, "auc": 0.516}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4916387959866221, "accuracy_n": 299, "auc": 0.4916387959866221}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.605, "accuracy_n": 400, "auc": 0.605}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9568106312292359, "accuracy_n": 301, "auc": 0.9568106312292359}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9918906394810009, "accuracy_n": 322, "auc": 0.9918906394810009}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8234022556390977, "accuracy_n": 292, "auc": 0.8234022556390977}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9225181598062954, "accuracy_n": 413, "auc": 0.9225181598062954}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7519119338287332, "accuracy_n": 1902, "auc": 0.7519119338287332}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7231380528370741, "accuracy_n": 2000, "auc": 0.7231380528370741}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7170868347338935, "accuracy_n": 59, "auc": 0.7170868347338935}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8702213279678068, "accuracy_n": 994, "auc": 0.8702213279678068}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9295774647887324, "accuracy_n": 994, "auc": 0.9295774647887324}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5787362086258776, "accuracy_n": 997, "auc": 0.5787362086258776}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.426, "accuracy_n": 500, "auc": 0.426}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 500, "auc": 0.55}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5585284280936454, "accuracy_n": 299, "auc": 0.5585284280936454}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 400, "auc": 0.57}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9601328903654485, "accuracy_n": 301, "auc": 0.9601328903654485}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9934352795798579, "accuracy_n": 322, "auc": 0.9934352795798579}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9107612781954888, "accuracy_n": 292, "auc": 0.9107612781954888}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9830508474576272, "accuracy_n": 413, "auc": 0.9830508474576272}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7326399946921444, "accuracy_n": 1902, "auc": 0.7326399946921444}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7459883017769415, "accuracy_n": 2000, "auc": 0.7459883017769415}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7492997198879552, "accuracy_n": 59, "auc": 0.7492997198879552}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8501006036217303, "accuracy_n": 994, "auc": 0.8501006036217303}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9154929577464789, "accuracy_n": 994, "auc": 0.9154929577464789}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6642599277978339, "accuracy_n": 277, "auc": 0.6642599277978339}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6439317953861585, "accuracy_n": 997, "auc": 0.6439317953861585}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.528, "accuracy_n": 500, "auc": 0.528}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 500, "auc": 0.56}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6575, "accuracy_n": 400, "auc": 0.6575}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6677740863787376, "accuracy_n": 301, "auc": 0.6677740863787376}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9927015755329008, "accuracy_n": 322, "auc": 0.9927015755329008}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8904605263157894, "accuracy_n": 292, "auc": 0.8904605263157894}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8256658595641646, "accuracy_n": 413, "auc": 0.8256658595641646}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6403750884642604, "accuracy_n": 1902, "auc": 0.6403750884642604}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6480459445859956, "accuracy_n": 2000, "auc": 0.6480459445859956}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7338935574229692, "accuracy_n": 59, "auc": 0.7338935574229692}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 994, "auc": 0.5392354124748491}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.424, "accuracy_n": 500, "auc": 0.424}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.458, "accuracy_n": 500, "auc": 0.458}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4782608695652174, "accuracy_n": 299, "auc": 0.4782608695652174}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5775, "accuracy_n": 400, "auc": 0.5775}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9469029966017917, "accuracy_n": 322, "auc": 0.9469029966017917}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7931860902255639, "accuracy_n": 292, "auc": 0.7931860902255639}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6828087167070218, "accuracy_n": 413, "auc": 0.6828087167070218}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.626922991861288, "accuracy_n": 1902, "auc": 0.626922991861288}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6864728166868239, "accuracy_n": 2000, "auc": 0.6864728166868239}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6414565826330533, "accuracy_n": 59, "auc": 0.6414565826330533}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6106639839034205, "accuracy_n": 994, "auc": 0.6106639839034205}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7967806841046278, "accuracy_n": 994, "auc": 0.7967806841046278}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.592057761732852, "accuracy_n": 277, "auc": 0.592057761732852}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.634, "accuracy_n": 500, "auc": 0.634}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 500, "auc": 0.62}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6287625418060201, "accuracy_n": 299, "auc": 0.6287625418060201}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.695, "accuracy_n": 400, "auc": 0.695}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5116279069767442, "accuracy_n": 301, "auc": 0.5116279069767442}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9824683348779735, "accuracy_n": 322, "auc": 0.9824683348779735}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9202537593984963, "accuracy_n": 292, "auc": 0.9202537593984963}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9007263922518159, "accuracy_n": 413, "auc": 0.9007263922518159}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5782477441613587, "accuracy_n": 1902, "auc": 0.5782477441613587}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7007749797676961, "accuracy_n": 2000, "auc": 0.7007749797676961}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7507002801120448, "accuracy_n": 59, "auc": 0.7507002801120448}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5653923541247485, "accuracy_n": 994, "auc": 0.5653923541247485}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8812877263581489, "accuracy_n": 994, "auc": 0.8812877263581489}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7211634904714143, "accuracy_n": 997, "auc": 0.7211634904714143}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.576, "accuracy_n": 500, "auc": 0.576}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.638, "accuracy_n": 500, "auc": 0.638}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6421404682274248, "accuracy_n": 299, "auc": 0.6421404682274248}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6775, "accuracy_n": 400, "auc": 0.6775}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.521594684385382, "accuracy_n": 301, "auc": 0.521594684385382}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9917747914735867, "accuracy_n": 322, "auc": 0.9917747914735867}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8890037593984962, "accuracy_n": 292, "auc": 0.8890037593984962}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9612590799031477, "accuracy_n": 413, "auc": 0.9612590799031477}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5537553078556263, "accuracy_n": 1902, "auc": 0.5537553078556263}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7405863516729541, "accuracy_n": 2000, "auc": 0.7405863516729541}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6918767507002801, "accuracy_n": 59, "auc": 0.6918767507002801}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8400402414486922, "accuracy_n": 994, "auc": 0.8400402414486922}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6409227683049148, "accuracy_n": 997, "auc": 0.6409227683049148}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.564, "accuracy_n": 500, "auc": 0.564}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.606, "accuracy_n": 500, "auc": 0.606}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6321070234113713, "accuracy_n": 299, "auc": 0.6321070234113713}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7075, "accuracy_n": 400, "auc": 0.7075}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5116279069767442, "accuracy_n": 301, "auc": 0.5116279069767442}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9778730305838741, "accuracy_n": 322, "auc": 0.9778730305838741}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8924342105263158, "accuracy_n": 292, "auc": 0.8924342105263158}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5925098416489738, "accuracy_n": 1902, "auc": 0.5925098416489738}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6802195592608932, "accuracy_n": 2000, "auc": 0.6802195592608932}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7871148459383753, "accuracy_n": 59, "auc": 0.7871148459383753}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.653846153846154, "accuracy_n": 23, "auc": 0.653846153846154}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7645875251509054, "accuracy_n": 994, "auc": 0.7645875251509054}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.795774647887324, "accuracy_n": 994, "auc": 0.795774647887324}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6258776328986961, "accuracy_n": 997, "auc": 0.6258776328986961}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.588, "accuracy_n": 500, "auc": 0.588}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.616, "accuracy_n": 500, "auc": 0.616}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6975, "accuracy_n": 400, "auc": 0.6975}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5116279069767442, "accuracy_n": 301, "auc": 0.5116279069767442}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9845149830089589, "accuracy_n": 322, "auc": 0.9845149830089589}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8821898496240602, "accuracy_n": 292, "auc": 0.8821898496240602}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9418886198547215, "accuracy_n": 413, "auc": 0.9418886198547215}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6308928255484784, "accuracy_n": 1902, "auc": 0.6308928255484784}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6681291946392647, "accuracy_n": 2000, "auc": 0.6681291946392647}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7689075630252101, "accuracy_n": 59, "auc": 0.7689075630252101}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8501006036217303, "accuracy_n": 994, "auc": 0.8501006036217303}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.518555667001003, "accuracy_n": 997, "auc": 0.518555667001003}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.32441471571906355, "accuracy_n": 299, "auc": 0.32441471571906355}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.235, "accuracy_n": 400, "auc": 0.235}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9900332225913622, "accuracy_n": 301, "auc": 0.9900332225913622}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9472119246215632, "accuracy_n": 322, "auc": 0.9472119246215632}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7839285714285715, "accuracy_n": 292, "auc": 0.7839285714285715}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6658595641646489, "accuracy_n": 413, "auc": 0.6658595641646489}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.787609474522293, "accuracy_n": 1902, "auc": 0.787609474522293}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7059338421170043, "accuracy_n": 2000, "auc": 0.7059338421170043}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8277310924369747, "accuracy_n": 59, "auc": 0.8277310924369747}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.806841046277666, "accuracy_n": 994, "auc": 0.806841046277666}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6659979939819458, "accuracy_n": 997, "auc": 0.6659979939819458}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 500, "auc": 0.3}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.396, "accuracy_n": 500, "auc": 0.396}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.431438127090301, "accuracy_n": 299, "auc": 0.431438127090301}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2825, "accuracy_n": 400, "auc": 0.2825}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9667774086378738, "accuracy_n": 301, "auc": 0.9667774086378738}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9779116465863453, "accuracy_n": 322, "auc": 0.9779116465863453}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8390037593984963, "accuracy_n": 292, "auc": 0.8390037593984963}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.847457627118644, "accuracy_n": 413, "auc": 0.847457627118644}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.578955458244869, "accuracy_n": 1902, "auc": 0.578955458244869}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.687239093312686, "accuracy_n": 2000, "auc": 0.687239093312686}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.707282913165266, "accuracy_n": 59, "auc": 0.707282913165266}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8591549295774648, "accuracy_n": 994, "auc": 0.8591549295774648}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8148893360160966, "accuracy_n": 994, "auc": 0.8148893360160966}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6990972918756269, "accuracy_n": 997, "auc": 0.6990972918756269}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.508, "accuracy_n": 500, "auc": 0.508}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.624, "accuracy_n": 500, "auc": 0.624}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5953177257525084, "accuracy_n": 299, "auc": 0.5953177257525084}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.645, "accuracy_n": 400, "auc": 0.645}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6345514950166113, "accuracy_n": 301, "auc": 0.6345514950166113}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9996138399752857, "accuracy_n": 322, "auc": 0.9996138399752857}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9291823308270677, "accuracy_n": 292, "auc": 0.9291823308270677}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8426150121065376, "accuracy_n": 413, "auc": 0.8426150121065376}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6826576875442321, "accuracy_n": 1902, "auc": 0.6826576875442321}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7533129459734965, "accuracy_n": 2000, "auc": 0.7533129459734965}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8025210084033614, "accuracy_n": 59, "auc": 0.8025210084033614}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 994, "auc": 0.545271629778672}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7806841046277666, "accuracy_n": 994, "auc": 0.7806841046277666}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5703971119133574, "accuracy_n": 277, "auc": 0.5703971119133574}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.649949849548646, "accuracy_n": 997, "auc": 0.649949849548646}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.512, "accuracy_n": 500, "auc": 0.512}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.588, "accuracy_n": 500, "auc": 0.588}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5652173913043478, "accuracy_n": 299, "auc": 0.5652173913043478}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6475, "accuracy_n": 400, "auc": 0.6475}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7873754152823921, "accuracy_n": 301, "auc": 0.7873754152823921}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9913113994439297, "accuracy_n": 322, "auc": 0.9913113994439297}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9425751879699248, "accuracy_n": 292, "auc": 0.9425751879699248}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9685230024213075, "accuracy_n": 413, "auc": 0.9685230024213075}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5768853945506016, "accuracy_n": 1902, "auc": 0.5768853945506016}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7247026176449698, "accuracy_n": 2000, "auc": 0.7247026176449698}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7913165266106443, "accuracy_n": 59, "auc": 0.7913165266106443}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6730382293762576, "accuracy_n": 994, "auc": 0.6730382293762576}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.858148893360161, "accuracy_n": 994, "auc": 0.858148893360161}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6308926780341023, "accuracy_n": 997, "auc": 0.6308926780341023}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.452, "accuracy_n": 500, "auc": 0.452}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.468, "accuracy_n": 500, "auc": 0.468}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.568561872909699, "accuracy_n": 299, "auc": 0.568561872909699}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5375, "accuracy_n": 400, "auc": 0.5375}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5249169435215947, "accuracy_n": 301, "auc": 0.5249169435215947}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9955205437133149, "accuracy_n": 322, "auc": 0.9955205437133149}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8291823308270677, "accuracy_n": 292, "auc": 0.8291823308270677}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6072916666666666, "accuracy_n": 1902, "auc": 0.6072916666666666}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6777186564349731, "accuracy_n": 2000, "auc": 0.6777186564349731}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8235294117647058, "accuracy_n": 59, "auc": 0.8235294117647058}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7092555331991952, "accuracy_n": 994, "auc": 0.7092555331991952}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7152917505030181, "accuracy_n": 994, "auc": 0.7152917505030181}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5956678700361011, "accuracy_n": 277, "auc": 0.5956678700361011}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.325, "accuracy_n": 400, "auc": 0.325}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.770764119601329, "accuracy_n": 301, "auc": 0.770764119601329}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9104494902687673, "accuracy_n": 322, "auc": 0.9104494902687673}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6481203007518797, "accuracy_n": 292, "auc": 0.6481203007518797}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5520581113801453, "accuracy_n": 413, "auc": 0.5520581113801453}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9621063340410474, "accuracy_n": 1902, "auc": 0.9621063340410474}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5676639266775306, "accuracy_n": 2000, "auc": 0.5676639266775306}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6372549019607843, "accuracy_n": 59, "auc": 0.6372549019607843}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8943661971830986, "accuracy_n": 994, "auc": 0.8943661971830986}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6245487364620939, "accuracy_n": 277, "auc": 0.6245487364620939}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6780341023069207, "accuracy_n": 997, "auc": 0.6780341023069207}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.542, "accuracy_n": 500, "auc": 0.542}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.558, "accuracy_n": 500, "auc": 0.558}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5652173913043478, "accuracy_n": 299, "auc": 0.5652173913043478}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.635, "accuracy_n": 400, "auc": 0.635}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5913621262458472, "accuracy_n": 301, "auc": 0.5913621262458472}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9996910719802286, "accuracy_n": 322, "auc": 0.9996910719802286}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9114661654135339, "accuracy_n": 292, "auc": 0.9114661654135339}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7205889508138712, "accuracy_n": 1902, "auc": 0.7205889508138712}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7953571239217357, "accuracy_n": 2000, "auc": 0.7953571239217357}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7843137254901961, "accuracy_n": 59, "auc": 0.7843137254901961}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8551307847082495, "accuracy_n": 994, "auc": 0.8551307847082495}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7494969818913481, "accuracy_n": 994, "auc": 0.7494969818913481}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5817452357071213, "accuracy_n": 997, "auc": 0.5817452357071213}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.446, "accuracy_n": 500, "auc": 0.446}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.544, "accuracy_n": 500, "auc": 0.544}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5652173913043478, "accuracy_n": 299, "auc": 0.5652173913043478}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5825, "accuracy_n": 400, "auc": 0.5825}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5647840531561462, "accuracy_n": 301, "auc": 0.5647840531561462}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.996177015755329, "accuracy_n": 322, "auc": 0.996177015755329}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8676221804511278, "accuracy_n": 292, "auc": 0.8676221804511278}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9830508474576272, "accuracy_n": 413, "auc": 0.9830508474576272}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7309105184005662, "accuracy_n": 1902, "auc": 0.7309105184005662}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6798754350320465, "accuracy_n": 2000, "auc": 0.6798754350320465}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7478991596638656, "accuracy_n": 59, "auc": 0.7478991596638656}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8450704225352113, "accuracy_n": 994, "auc": 0.8450704225352113}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5674044265593562, "accuracy_n": 994, "auc": 0.5674044265593562}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.488, "accuracy_n": 500, "auc": 0.488}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3210702341137124, "accuracy_n": 299, "auc": 0.3210702341137124}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.18, "accuracy_n": 400, "auc": 0.18}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9700996677740864, "accuracy_n": 301, "auc": 0.9700996677740864}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.627818968180414, "accuracy_n": 322, "auc": 0.627818968180414}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5326597744360902, "accuracy_n": 292, "auc": 0.5326597744360902}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7312348668280871, "accuracy_n": 413, "auc": 0.7312348668280871}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6605460456475584, "accuracy_n": 1902, "auc": 0.6605460456475584}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5500505682551401, "accuracy_n": 2000, "auc": 0.5500505682551401}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6582633053221288, "accuracy_n": 59, "auc": 0.6582633053221288}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
