{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38, "accuracy_n": 50, "auc": 0.38}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.34, "accuracy_n": 50, "auc": 0.34}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.35294117647058826, "accuracy_n": 51, "auc": 0.35294117647058826}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28205128205128205, "accuracy_n": 39, "auc": 0.28205128205128205}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.621962196219622, "accuracy_n": 200, "auc": 0.621962196219622}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.582929292929293, "accuracy_n": 200, "auc": 0.582929292929293}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5656565656565656, "accuracy_n": 99, "auc": 0.5656565656565656}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4, "accuracy_n": 50, "auc": 0.4}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 50, "auc": 0.62}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5490196078431373, "accuracy_n": 51, "auc": 0.5490196078431373}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 50, "auc": 0.72}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5128205128205128, "accuracy_n": 39, "auc": 0.5128205128205128}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9682968296829683, "accuracy_n": 200, "auc": 0.9682968296829683}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.786969696969697, "accuracy_n": 200, "auc": 0.786969696969697}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5858585858585859, "accuracy_n": 99, "auc": 0.5858585858585859}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5098039215686274, "accuracy_n": 51, "auc": 0.5098039215686274}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 50, "auc": 0.66}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5128205128205128, "accuracy_n": 39, "auc": 0.5128205128205128}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9119911991199119, "accuracy_n": 200, "auc": 0.9119911991199119}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7266666666666667, "accuracy_n": 200, "auc": 0.7266666666666667}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5656565656565656, "accuracy_n": 99, "auc": 0.5656565656565656}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 50, "auc": 0.5}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.44, "accuracy_n": 50, "auc": 0.44}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.49019607843137253, "accuracy_n": 51, "auc": 0.49019607843137253}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 50, "auc": 0.66}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.46153846153846156, "accuracy_n": 39, "auc": 0.46153846153846156}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7536753675367538, "accuracy_n": 200, "auc": 0.7536753675367538}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6989898989898989, "accuracy_n": 200, "auc": 0.6989898989898989}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5858585858585859, "accuracy_n": 99, "auc": 0.5858585858585859}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 50, "auc": 0.28}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 50, "auc": 0.36}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.35294117647058826, "accuracy_n": 51, "auc": 0.35294117647058826}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 50, "auc": 0.58}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2564102564102564, "accuracy_n": 39, "auc": 0.2564102564102564}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7537753775377538, "accuracy_n": 200, "auc": 0.7537753775377538}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5494949494949495, "accuracy_n": 200, "auc": 0.5494949494949495}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5151515151515151, "accuracy_n": 99, "auc": 0.5151515151515151}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48, "accuracy_n": 50, "auc": 0.48}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 50, "auc": 0.6}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4117647058823529, "accuracy_n": 51, "auc": 0.4117647058823529}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 50, "auc": 0.76}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 39, "auc": 0.5384615384615384}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5058505850585058, "accuracy_n": 200, "auc": 0.5058505850585058}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.572929292929293, "accuracy_n": 200, "auc": 0.572929292929293}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5252525252525253, "accuracy_n": 99, "auc": 0.5252525252525253}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 50, "auc": 0.5}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48, "accuracy_n": 50, "auc": 0.48}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.39215686274509803, "accuracy_n": 51, "auc": 0.39215686274509803}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48, "accuracy_n": 50, "auc": 0.48}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.41025641025641024, "accuracy_n": 39, "auc": 0.41025641025641024}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5523552355235524, "accuracy_n": 200, "auc": 0.5523552355235524}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5891919191919193, "accuracy_n": 200, "auc": 0.5891919191919193}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 99, "auc": 0.5757575757575758}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 50, "auc": 0.6}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 50, "auc": 0.56}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 51, "auc": 0.5294117647058824}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 50, "auc": 0.76}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 39, "auc": 0.5384615384615384}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9723972397239724, "accuracy_n": 200, "auc": 0.9723972397239724}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7926262626262626, "accuracy_n": 200, "auc": 0.7926262626262626}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 99, "auc": 0.5757575757575758}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42, "accuracy_n": 50, "auc": 0.42}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 50, "auc": 0.5}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 51, "auc": 0.5882352941176471}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 50, "auc": 0.84}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48717948717948717, "accuracy_n": 39, "auc": 0.48717948717948717}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7320732073207321, "accuracy_n": 200, "auc": 0.7320732073207321}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5531313131313131, "accuracy_n": 200, "auc": 0.5531313131313131}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5858585858585859, "accuracy_n": 99, "auc": 0.5858585858585859}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 50, "auc": 0.68}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5490196078431373, "accuracy_n": 51, "auc": 0.5490196078431373}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 50, "auc": 0.78}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5641025641025641, "accuracy_n": 39, "auc": 0.5641025641025641}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9827982798279828, "accuracy_n": 200, "auc": 0.9827982798279828}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8704040404040405, "accuracy_n": 200, "auc": 0.8704040404040405}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5656565656565656, "accuracy_n": 99, "auc": 0.5656565656565656}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 50, "auc": 0.7}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 51, "auc": 0.5882352941176471}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 50, "auc": 0.78}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5897435897435898, "accuracy_n": 39, "auc": 0.5897435897435898}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9776977697769778, "accuracy_n": 200, "auc": 0.9776977697769778}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8866666666666666, "accuracy_n": 200, "auc": 0.8866666666666666}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_easy", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 99, "auc": 0.5757575757575758}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 50, "auc": 0.52}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 50, "auc": 0.72}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 51, "auc": 0.5882352941176471}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 50, "auc": 0.8}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 39, "auc": 0.6153846153846154}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.884988498849885, "accuracy_n": 200, "auc": 0.884988498849885}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7336363636363635, "accuracy_n": 200, "auc": 0.7336363636363635}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5858585858585859, "accuracy_n": 99, "auc": 0.5858585858585859}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 50, "auc": 0.3}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.46, "accuracy_n": 50, "auc": 0.46}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.23529411764705882, "accuracy_n": 51, "auc": 0.23529411764705882}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 50, "auc": 0.3}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.1794871794871795, "accuracy_n": 39, "auc": 0.1794871794871795}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.99, "accuracy_n": 100, "auc": 0.99}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9346934693469346, "accuracy_n": 200, "auc": 0.9346934693469346}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6112121212121213, "accuracy_n": 200, "auc": 0.6112121212121213}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9595959595959596, "accuracy_n": 99, "auc": 0.9595959595959596}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 50, "auc": 0.28}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.44, "accuracy_n": 50, "auc": 0.44}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 51, "auc": 0.3333333333333333}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 50, "auc": 0.32}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.23076923076923078, "accuracy_n": 39, "auc": 0.23076923076923078}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8945894589458945, "accuracy_n": 200, "auc": 0.8945894589458945}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7534343434343435, "accuracy_n": 200, "auc": 0.7534343434343435}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9797979797979798, "accuracy_n": 99, "auc": 0.9797979797979798}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38, "accuracy_n": 50, "auc": 0.38}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 50, "auc": 0.36}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.45098039215686275, "accuracy_n": 51, "auc": 0.45098039215686275}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 50, "auc": 0.66}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48717948717948717, "accuracy_n": 39, "auc": 0.48717948717948717}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9865986598659866, "accuracy_n": 200, "auc": 0.9865986598659866}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8227272727272729, "accuracy_n": 200, "auc": 0.8227272727272729}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 99, "auc": 0.5757575757575758}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 50, "auc": 0.36}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.34, "accuracy_n": 50, "auc": 0.34}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.39215686274509803, "accuracy_n": 51, "auc": 0.39215686274509803}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 50, "auc": 0.66}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 39, "auc": 0.6153846153846154}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9301930193019302, "accuracy_n": 200, "auc": 0.9301930193019302}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8482828282828283, "accuracy_n": 200, "auc": 0.8482828282828283}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 99, "auc": 0.5757575757575758}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38, "accuracy_n": 50, "auc": 0.38}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 50, "auc": 0.36}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3137254901960784, "accuracy_n": 51, "auc": 0.3137254901960784}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 50, "auc": 0.28}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38461538461538464, "accuracy_n": 39, "auc": 0.38461538461538464}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6821682168216822, "accuracy_n": 200, "auc": 0.6821682168216822}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5123232323232323, "accuracy_n": 200, "auc": 0.5123232323232323}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 99, "auc": 1.0}}
