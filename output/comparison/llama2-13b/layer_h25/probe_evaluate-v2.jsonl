{"key": "result_0", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9164989939637826, "accuracy_n": 994, "auc": 0.9164989939637826}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9144869215291751, "accuracy_n": 994, "auc": 0.9144869215291751}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6098294884653962, "accuracy_n": 997, "auc": 0.6098294884653962}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.368, "accuracy_n": 500, "auc": 0.368}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.412, "accuracy_n": 500, "auc": 0.412}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.391304347826087, "accuracy_n": 299, "auc": 0.391304347826087}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 400, "auc": 0.54}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.584717607973422, "accuracy_n": 301, "auc": 0.584717607973422}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6136855112758728, "accuracy_n": 322, "auc": 0.6136855112758728}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5461466165413533, "accuracy_n": 292, "auc": 0.5461466165413533}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5941005396319887, "accuracy_n": 1902, "auc": 0.5941005396319887}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5165144617206812, "accuracy_n": 2000, "auc": 0.5165144617206812}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6918767507002801, "accuracy_n": 59, "auc": 0.6918767507002801}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9235412474849095, "accuracy_n": 994, "auc": 0.9235412474849095}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 994, "auc": 0.8088531187122736}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.474, "accuracy_n": 500, "auc": 0.474}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.538, "accuracy_n": 500, "auc": 0.538}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5351170568561873, "accuracy_n": 299, "auc": 0.5351170568561873}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9269102990033222, "accuracy_n": 301, "auc": 0.9269102990033222}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8934584491813408, "accuracy_n": 322, "auc": 0.8934584491813408}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7875469924812031, "accuracy_n": 292, "auc": 0.7875469924812031}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7191283292978208, "accuracy_n": 413, "auc": 0.7191283292978208}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7165317586694975, "accuracy_n": 1902, "auc": 0.7165317586694975}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6047113007795814, "accuracy_n": 2000, "auc": 0.6047113007795814}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6680672268907563, "accuracy_n": 59, "auc": 0.6680672268907563}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8999999999999999, "accuracy_n": 23, "auc": 0.8999999999999999}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8843058350100603, "accuracy_n": 994, "auc": 0.8843058350100603}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8279678068410463, "accuracy_n": 994, "auc": 0.8279678068410463}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.592057761732852, "accuracy_n": 277, "auc": 0.592057761732852}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.322, "accuracy_n": 500, "auc": 0.322}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.374, "accuracy_n": 500, "auc": 0.374}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.35785953177257523, "accuracy_n": 299, "auc": 0.35785953177257523}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3575, "accuracy_n": 400, "auc": 0.3575}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7201112140871178, "accuracy_n": 322, "auc": 0.7201112140871178}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6208646616541353, "accuracy_n": 292, "auc": 0.6208646616541353}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5898000707714084, "accuracy_n": 1902, "auc": 0.5898000707714084}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5222065165524754, "accuracy_n": 2000, "auc": 0.5222065165524754}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6176470588235294, "accuracy_n": 59, "auc": 0.6176470588235294}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8259557344064387, "accuracy_n": 994, "auc": 0.8259557344064387}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7122736418511066, "accuracy_n": 994, "auc": 0.7122736418511066}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5025075225677031, "accuracy_n": 997, "auc": 0.5025075225677031}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.458, "accuracy_n": 500, "auc": 0.458}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.504, "accuracy_n": 500, "auc": 0.504}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5418060200668896, "accuracy_n": 299, "auc": 0.5418060200668896}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 400, "auc": 0.64}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5149501661129569, "accuracy_n": 301, "auc": 0.5149501661129569}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5421300586963238, "accuracy_n": 322, "auc": 0.5421300586963238}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5231203007518797, "accuracy_n": 292, "auc": 0.5231203007518797}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6537530266343826, "accuracy_n": 413, "auc": 0.6537530266343826}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.520027202760085, "accuracy_n": 1902, "auc": 0.520027202760085}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5770983324980318, "accuracy_n": 2000, "auc": 0.5770983324980318}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6470588235294118, "accuracy_n": 59, "auc": 0.6470588235294118}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8769230769230769, "accuracy_n": 23, "auc": 0.8769230769230769}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5824949698189135, "accuracy_n": 994, "auc": 0.5824949698189135}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.488, "accuracy_n": 500, "auc": 0.488}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.43, "accuracy_n": 500, "auc": 0.43}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 299, "auc": 0.5384615384615384}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 400, "auc": 0.65}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8091597157862218, "accuracy_n": 322, "auc": 0.8091597157862218}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5866071428571429, "accuracy_n": 292, "auc": 0.5866071428571429}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8426150121065376, "accuracy_n": 413, "auc": 0.8426150121065376}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5411967002830856, "accuracy_n": 1902, "auc": 0.5411967002830856}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5632303261477394, "accuracy_n": 2000, "auc": 0.5632303261477394}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 59, "auc": 0.7142857142857143}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7263581488933601, "accuracy_n": 994, "auc": 0.7263581488933601}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 994, "auc": 0.6237424547283702}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.578, "accuracy_n": 500, "auc": 0.578}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 500, "auc": 0.58}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6625, "accuracy_n": 400, "auc": 0.6625}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6411960132890365, "accuracy_n": 301, "auc": 0.6411960132890365}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9621949335804758, "accuracy_n": 322, "auc": 0.9621949335804758}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8781954887218046, "accuracy_n": 292, "auc": 0.8781954887218046}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5690072639225182, "accuracy_n": 413, "auc": 0.5690072639225182}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5526229653220098, "accuracy_n": 1902, "auc": 0.5526229653220098}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.523969152864184, "accuracy_n": 2000, "auc": 0.523969152864184}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7563025210084033, "accuracy_n": 59, "auc": 0.7563025210084033}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.789738430583501, "accuracy_n": 994, "auc": 0.789738430583501}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7867203219315896, "accuracy_n": 994, "auc": 0.7867203219315896}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6720160481444333, "accuracy_n": 997, "auc": 0.6720160481444333}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.578, "accuracy_n": 500, "auc": 0.578}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.622, "accuracy_n": 500, "auc": 0.622}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6321070234113713, "accuracy_n": 299, "auc": 0.6321070234113713}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6675, "accuracy_n": 400, "auc": 0.6675}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6378737541528239, "accuracy_n": 301, "auc": 0.6378737541528239}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9757491504479456, "accuracy_n": 322, "auc": 0.9757491504479456}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7740131578947369, "accuracy_n": 292, "auc": 0.7740131578947369}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5018389508138712, "accuracy_n": 1902, "auc": 0.5018389508138712}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5685092318326916, "accuracy_n": 2000, "auc": 0.5685092318326916}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8123249299719888, "accuracy_n": 59, "auc": 0.8123249299719888}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7917505030181087, "accuracy_n": 994, "auc": 0.7917505030181087}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8511066398390342, "accuracy_n": 994, "auc": 0.8511066398390342}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6700100300902708, "accuracy_n": 997, "auc": 0.6700100300902708}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.536, "accuracy_n": 500, "auc": 0.536}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.612, "accuracy_n": 500, "auc": 0.612}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6675, "accuracy_n": 400, "auc": 0.6675}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6411960132890365, "accuracy_n": 301, "auc": 0.6411960132890365}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9768690145196169, "accuracy_n": 322, "auc": 0.9768690145196169}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8595394736842105, "accuracy_n": 292, "auc": 0.8595394736842105}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5617845452937014, "accuracy_n": 1902, "auc": 0.5617845452937014}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5262129628795995, "accuracy_n": 2000, "auc": 0.5262129628795995}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8375350140056023, "accuracy_n": 59, "auc": 0.8375350140056023}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8913480885311871, "accuracy_n": 994, "auc": 0.8913480885311871}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.806841046277666, "accuracy_n": 994, "auc": 0.806841046277666}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6690070210631895, "accuracy_n": 997, "auc": 0.6690070210631895}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.556, "accuracy_n": 500, "auc": 0.556}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.638, "accuracy_n": 500, "auc": 0.638}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6086956521739131, "accuracy_n": 299, "auc": 0.6086956521739131}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 400, "auc": 0.68}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.521594684385382, "accuracy_n": 301, "auc": 0.521594684385382}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9534677170219338, "accuracy_n": 322, "auc": 0.9534677170219338}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7673402255639098, "accuracy_n": 292, "auc": 0.7673402255639098}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5600926663128096, "accuracy_n": 1902, "auc": 0.5600926663128096}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6052194842338084, "accuracy_n": 2000, "auc": 0.6052194842338084}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8487394957983193, "accuracy_n": 59, "auc": 0.8487394957983193}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7374245472837022, "accuracy_n": 994, "auc": 0.7374245472837022}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8601609657947686, "accuracy_n": 994, "auc": 0.8601609657947686}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6168505516549649, "accuracy_n": 997, "auc": 0.6168505516549649}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.322, "accuracy_n": 500, "auc": 0.322}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3010033444816054, "accuracy_n": 299, "auc": 0.3010033444816054}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.19, "accuracy_n": 400, "auc": 0.19}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9634551495016611, "accuracy_n": 301, "auc": 0.9634551495016611}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9754016064257028, "accuracy_n": 322, "auc": 0.9754016064257028}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7382518796992482, "accuracy_n": 292, "auc": 0.7382518796992482}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9830508474576272, "accuracy_n": 413, "auc": 0.9830508474576272}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5881170382165605, "accuracy_n": 1902, "auc": 0.5881170382165605}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6401470931006092, "accuracy_n": 2000, "auc": 0.6401470931006092}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7296918767507002, "accuracy_n": 59, "auc": 0.7296918767507002}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7545271629778671, "accuracy_n": 994, "auc": 0.7545271629778671}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8169014084507042, "accuracy_n": 994, "auc": 0.8169014084507042}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6439317953861585, "accuracy_n": 997, "auc": 0.6439317953861585}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 500, "auc": 0.3}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.448, "accuracy_n": 500, "auc": 0.448}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3311036789297659, "accuracy_n": 299, "auc": 0.3311036789297659}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.345, "accuracy_n": 400, "auc": 0.345}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9401993355481728, "accuracy_n": 301, "auc": 0.9401993355481728}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9533518690145196, "accuracy_n": 322, "auc": 0.9533518690145196}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8521146616541354, "accuracy_n": 292, "auc": 0.8521146616541354}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9612590799031477, "accuracy_n": 413, "auc": 0.9612590799031477}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5434879246284501, "accuracy_n": 1902, "auc": 0.5434879246284501}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5996834857383515, "accuracy_n": 2000, "auc": 0.5996834857383515}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6484593837535014, "accuracy_n": 59, "auc": 0.6484593837535014}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8038229376257545, "accuracy_n": 994, "auc": 0.8038229376257545}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6428571428571429, "accuracy_n": 994, "auc": 0.6428571428571429}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6629889669007021, "accuracy_n": 997, "auc": 0.6629889669007021}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.38, "accuracy_n": 500, "auc": 0.38}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.392, "accuracy_n": 500, "auc": 0.392}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4280936454849498, "accuracy_n": 299, "auc": 0.4280936454849498}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.595, "accuracy_n": 400, "auc": 0.595}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7740863787375415, "accuracy_n": 301, "auc": 0.7740863787375415}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9948254556688292, "accuracy_n": 322, "auc": 0.9948254556688292}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8408364661654135, "accuracy_n": 292, "auc": 0.8408364661654135}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6440677966101694, "accuracy_n": 413, "auc": 0.6440677966101694}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.597057457537155, "accuracy_n": 1902, "auc": 0.597057457537155}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5645958190906917, "accuracy_n": 2000, "auc": 0.5645958190906917}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7170868347338935, "accuracy_n": 59, "auc": 0.7170868347338935}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5020120724346077, "accuracy_n": 994, "auc": 0.5020120724346077}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5875251509054326, "accuracy_n": 994, "auc": 0.5875251509054326}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.555956678700361, "accuracy_n": 277, "auc": 0.555956678700361}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5376128385155466, "accuracy_n": 997, "auc": 0.5376128385155466}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.394, "accuracy_n": 500, "auc": 0.394}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.386, "accuracy_n": 500, "auc": 0.386}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4782608695652174, "accuracy_n": 299, "auc": 0.4782608695652174}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.575, "accuracy_n": 400, "auc": 0.575}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6212624584717608, "accuracy_n": 301, "auc": 0.6212624584717608}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9545875810936052, "accuracy_n": 322, "auc": 0.9545875810936052}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8793233082706767, "accuracy_n": 292, "auc": 0.8793233082706767}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9830508474576272, "accuracy_n": 413, "auc": 0.9830508474576272}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5804737261146496, "accuracy_n": 1902, "auc": 0.5804737261146496}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5555755627781629, "accuracy_n": 2000, "auc": 0.5555755627781629}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6456582633053222, "accuracy_n": 59, "auc": 0.6456582633053222}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5482897384305835, "accuracy_n": 994, "auc": 0.5482897384305835}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.37, "accuracy_n": 500, "auc": 0.37}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.39464882943143814, "accuracy_n": 299, "auc": 0.39464882943143814}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5625, "accuracy_n": 400, "auc": 0.5625}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5149501661129569, "accuracy_n": 301, "auc": 0.5149501661129569}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7555993203583564, "accuracy_n": 322, "auc": 0.7555993203583564}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6261748120300752, "accuracy_n": 292, "auc": 0.6261748120300752}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8886198547215496, "accuracy_n": 413, "auc": 0.8886198547215496}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.516099389596603, "accuracy_n": 1902, "auc": 0.516099389596603}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5804035256727678, "accuracy_n": 2000, "auc": 0.5804035256727678}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6638655462184875, "accuracy_n": 59, "auc": 0.6638655462184875}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8601609657947686, "accuracy_n": 994, "auc": 0.8601609657947686}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6056338028169014, "accuracy_n": 994, "auc": 0.6056338028169014}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6359077231695085, "accuracy_n": 997, "auc": 0.6359077231695085}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.324, "accuracy_n": 500, "auc": 0.324}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.30434782608695654, "accuracy_n": 299, "auc": 0.30434782608695654}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3675, "accuracy_n": 400, "auc": 0.3675}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6079734219269103, "accuracy_n": 301, "auc": 0.6079734219269103}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7167130058696325, "accuracy_n": 322, "auc": 0.7167130058696325}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6309680451127819, "accuracy_n": 292, "auc": 0.6309680451127819}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.639225181598063, "accuracy_n": 413, "auc": 0.639225181598063}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9427127565463552, "accuracy_n": 1902, "auc": 0.9427127565463552}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.505726567290792, "accuracy_n": 2000, "auc": 0.505726567290792}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6736694677871149, "accuracy_n": 59, "auc": 0.6736694677871149}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5845070422535211, "accuracy_n": 994, "auc": 0.5845070422535211}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6961770623742455, "accuracy_n": 994, "auc": 0.6961770623742455}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6218655967903711, "accuracy_n": 997, "auc": 0.6218655967903711}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3277591973244147, "accuracy_n": 299, "auc": 0.3277591973244147}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.45, "accuracy_n": 400, "auc": 0.45}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5016611295681063, "accuracy_n": 301, "auc": 0.5016611295681063}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6549274019153537, "accuracy_n": 322, "auc": 0.6549274019153537}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5523966165413533, "accuracy_n": 292, "auc": 0.5523966165413533}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7772397094430993, "accuracy_n": 413, "auc": 0.7772397094430993}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5060321567586695, "accuracy_n": 1902, "auc": 0.5060321567586695}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6131503472753663, "accuracy_n": 2000, "auc": 0.6131503472753663}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6680672268907564, "accuracy_n": 59, "auc": 0.6680672268907564}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7464788732394366, "accuracy_n": 994, "auc": 0.7464788732394366}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6016096579476862, "accuracy_n": 994, "auc": 0.6016096579476862}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6890672016048145, "accuracy_n": 997, "auc": 0.6890672016048145}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.46, "accuracy_n": 500, "auc": 0.46}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.47, "accuracy_n": 500, "auc": 0.47}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5050167224080268, "accuracy_n": 299, "auc": 0.5050167224080268}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6125, "accuracy_n": 400, "auc": 0.6125}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5083056478405316, "accuracy_n": 301, "auc": 0.5083056478405316}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9895736793327155, "accuracy_n": 322, "auc": 0.9895736793327155}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.737640977443609, "accuracy_n": 292, "auc": 0.737640977443609}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9539951573849879, "accuracy_n": 413, "auc": 0.9539951573849879}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6593086518046709, "accuracy_n": 1902, "auc": 0.6593086518046709}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6249716147529258, "accuracy_n": 2000, "auc": 0.6249716147529258}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7100840336134454, "accuracy_n": 59, "auc": 0.7100840336134454}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.528169014084507, "accuracy_n": 994, "auc": 0.528169014084507}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.727364185110664, "accuracy_n": 994, "auc": 0.727364185110664}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6198595787362087, "accuracy_n": 997, "auc": 0.6198595787362087}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.332, "accuracy_n": 500, "auc": 0.332}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.23745819397993312, "accuracy_n": 299, "auc": 0.23745819397993312}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.17, "accuracy_n": 400, "auc": 0.17}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5348837209302325, "accuracy_n": 301, "auc": 0.5348837209302325}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7563330244053136, "accuracy_n": 322, "auc": 0.7563330244053136}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5528195488721804, "accuracy_n": 292, "auc": 0.5528195488721804}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.87409200968523, "accuracy_n": 413, "auc": 0.87409200968523}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5159478945506015, "accuracy_n": 1902, "auc": 0.5159478945506015}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5030235915165374, "accuracy_n": 2000, "auc": 0.5030235915165374}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6330532212885154, "accuracy_n": 59, "auc": 0.6330532212885154}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
