{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8733668341708543, "accuracy_n": 995, "auc": 0.8733668341708543}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9254783484390735, "accuracy_n": 993, "auc": 0.9254783484390735}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6462093862815884, "accuracy_n": 277, "auc": 0.6462093862815884}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.751004016064257, "accuracy_n": 996, "auc": 0.751004016064257}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 497, "auc": 0.2857142857142857}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.37, "accuracy_n": 500, "auc": 0.37}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.26421404682274247, "accuracy_n": 299, "auc": 0.26421404682274247}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23558897243107768, "accuracy_n": 399, "auc": 0.23558897243107768}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9058928019771394, "accuracy_n": 322, "auc": 0.9058928019771394}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6451597744360903, "accuracy_n": 292, "auc": 0.6451597744360903}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7276108014861995, "accuracy_n": 1902, "auc": 0.7276108014861995}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7631585002185789, "accuracy_n": 2000, "auc": 0.7631585002185789}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9033613445378151, "accuracy_n": 59, "auc": 0.9033613445378151}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9105527638190954, "accuracy_n": 995, "auc": 0.9105527638190954}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9496475327291037, "accuracy_n": 993, "auc": 0.9496475327291037}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7689530685920578, "accuracy_n": 277, "auc": 0.7689530685920578}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5763052208835341, "accuracy_n": 996, "auc": 0.5763052208835341}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30784708249496984, "accuracy_n": 497, "auc": 0.30784708249496984}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.312, "accuracy_n": 500, "auc": 0.312}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2656641604010025, "accuracy_n": 399, "auc": 0.2656641604010025}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9070898980537535, "accuracy_n": 322, "auc": 0.9070898980537535}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6437499999999999, "accuracy_n": 292, "auc": 0.6437499999999999}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8379732837933476, "accuracy_n": 1902, "auc": 0.8379732837933476}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7697158674281417, "accuracy_n": 2000, "auc": 0.7697158674281417}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8529411764705882, "accuracy_n": 59, "auc": 0.8529411764705882}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.770854271356784, "accuracy_n": 995, "auc": 0.770854271356784}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.94662638469285, "accuracy_n": 993, "auc": 0.94662638469285}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7689530685920578, "accuracy_n": 277, "auc": 0.7689530685920578}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7319277108433735, "accuracy_n": 996, "auc": 0.7319277108433735}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.28169014084507044, "accuracy_n": 497, "auc": 0.28169014084507044}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.428, "accuracy_n": 500, "auc": 0.428}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.38461538461538464, "accuracy_n": 299, "auc": 0.38461538461538464}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.43107769423558895, "accuracy_n": 399, "auc": 0.43107769423558895}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9735973597359736, "accuracy_n": 303, "auc": 0.9735973597359736}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9437751004016064, "accuracy_n": 322, "auc": 0.9437751004016064}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.749671052631579, "accuracy_n": 292, "auc": 0.749671052631579}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7111863057324842, "accuracy_n": 1902, "auc": 0.7111863057324842}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7795624220343544, "accuracy_n": 2000, "auc": 0.7795624220343544}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8179271708683473, "accuracy_n": 59, "auc": 0.8179271708683473}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9035175879396985, "accuracy_n": 995, "auc": 0.9035175879396985}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9486404833836858, "accuracy_n": 993, "auc": 0.9486404833836858}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5884476534296029, "accuracy_n": 277, "auc": 0.5884476534296029}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.308, "accuracy_n": 500, "auc": 0.308}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.35451505016722407, "accuracy_n": 299, "auc": 0.35451505016722407}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2882205513784461, "accuracy_n": 399, "auc": 0.2882205513784461}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8745874587458746, "accuracy_n": 303, "auc": 0.8745874587458746}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8690531356194008, "accuracy_n": 322, "auc": 0.8690531356194008}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.593468045112782, "accuracy_n": 292, "auc": 0.593468045112782}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5146154016277424, "accuracy_n": 1902, "auc": 0.5146154016277424}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7418868211424323, "accuracy_n": 2000, "auc": 0.7418868211424323}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7885154061624651, "accuracy_n": 59, "auc": 0.7885154061624651}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2977867203219316, "accuracy_n": 497, "auc": 0.2977867203219316}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.35, "accuracy_n": 500, "auc": 0.35}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3110367892976589, "accuracy_n": 299, "auc": 0.3110367892976589}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3157894736842105, "accuracy_n": 399, "auc": 0.3157894736842105}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8675857275254866, "accuracy_n": 322, "auc": 0.8675857275254866}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5448778195488722, "accuracy_n": 292, "auc": 0.5448778195488722}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9612590799031477, "accuracy_n": 413, "auc": 0.9612590799031477}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6265204794762916, "accuracy_n": 1902, "auc": 0.6265204794762916}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6572682738468587, "accuracy_n": 2000, "auc": 0.6572682738468587}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6554621848739496, "accuracy_n": 59, "auc": 0.6554621848739496}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6555891238670695, "accuracy_n": 993, "auc": 0.6555891238670695}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5492957746478874, "accuracy_n": 497, "auc": 0.5492957746478874}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.516, "accuracy_n": 500, "auc": 0.516}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.43478260869565216, "accuracy_n": 299, "auc": 0.43478260869565216}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5789473684210527, "accuracy_n": 399, "auc": 0.5789473684210527}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.66996699669967, "accuracy_n": 303, "auc": 0.66996699669967}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9245829471733087, "accuracy_n": 322, "auc": 0.9245829471733087}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7534774436090226, "accuracy_n": 292, "auc": 0.7534774436090226}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6101694915254238, "accuracy_n": 413, "auc": 0.6101694915254238}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5749966825902335, "accuracy_n": 1902, "auc": 0.5749966825902335}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5232348877944939, "accuracy_n": 2000, "auc": 0.5232348877944939}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5672268907563025, "accuracy_n": 59, "auc": 0.5672268907563025}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6361809045226131, "accuracy_n": 995, "auc": 0.6361809045226131}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.756294058408862, "accuracy_n": 993, "auc": 0.756294058408862}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6285140562248996, "accuracy_n": 996, "auc": 0.6285140562248996}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5191146881287726, "accuracy_n": 497, "auc": 0.5191146881287726}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.596, "accuracy_n": 500, "auc": 0.596}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5518394648829431, "accuracy_n": 299, "auc": 0.5518394648829431}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6616541353383458, "accuracy_n": 399, "auc": 0.6616541353383458}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9174917491749175, "accuracy_n": 303, "auc": 0.9174917491749175}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7443234476367007, "accuracy_n": 322, "auc": 0.7443234476367007}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7356203007518797, "accuracy_n": 292, "auc": 0.7356203007518797}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9782082324455206, "accuracy_n": 413, "auc": 0.9782082324455206}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5333880705944798, "accuracy_n": 1902, "auc": 0.5333880705944798}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.526353013437851, "accuracy_n": 2000, "auc": 0.526353013437851}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5397784491440081, "accuracy_n": 993, "auc": 0.5397784491440081}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5703971119133574, "accuracy_n": 277, "auc": 0.5703971119133574}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6234939759036144, "accuracy_n": 996, "auc": 0.6234939759036144}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4949698189134809, "accuracy_n": 497, "auc": 0.4949698189134809}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 500, "auc": 0.59}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6791979949874687, "accuracy_n": 399, "auc": 0.6791979949874687}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7557755775577558, "accuracy_n": 303, "auc": 0.7557755775577558}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7659097930182268, "accuracy_n": 322, "auc": 0.7659097930182268}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7955827067669172, "accuracy_n": 292, "auc": 0.7955827067669172}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9249394673123487, "accuracy_n": 413, "auc": 0.9249394673123487}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.583138711960368, "accuracy_n": 1902, "auc": 0.583138711960368}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5443965271462998, "accuracy_n": 2000, "auc": 0.5443965271462998}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.592436974789916, "accuracy_n": 59, "auc": 0.592436974789916}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5507537688442211, "accuracy_n": 995, "auc": 0.5507537688442211}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8207452165156093, "accuracy_n": 993, "auc": 0.8207452165156093}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6214859437751004, "accuracy_n": 996, "auc": 0.6214859437751004}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.46680080482897385, "accuracy_n": 497, "auc": 0.46680080482897385}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 500, "auc": 0.56}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5317725752508361, "accuracy_n": 299, "auc": 0.5317725752508361}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6641604010025063, "accuracy_n": 399, "auc": 0.6641604010025063}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8118811881188119, "accuracy_n": 303, "auc": 0.8118811881188119}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8403614457831325, "accuracy_n": 322, "auc": 0.8403614457831325}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7384868421052633, "accuracy_n": 292, "auc": 0.7384868421052633}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7167070217917676, "accuracy_n": 413, "auc": 0.7167070217917676}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5490711252653928, "accuracy_n": 1902, "auc": 0.5490711252653928}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5173617675981029, "accuracy_n": 2000, "auc": 0.5173617675981029}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5364145658263306, "accuracy_n": 59, "auc": 0.5364145658263306}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9155778894472362, "accuracy_n": 995, "auc": 0.9155778894472362}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9415911379657603, "accuracy_n": 993, "auc": 0.9415911379657603}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6750902527075813, "accuracy_n": 277, "auc": 0.6750902527075813}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6676706827309237, "accuracy_n": 996, "auc": 0.6676706827309237}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 497, "auc": 0.2857142857142857}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.346, "accuracy_n": 500, "auc": 0.346}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2681704260651629, "accuracy_n": 399, "auc": 0.2681704260651629}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.963314797652147, "accuracy_n": 322, "auc": 0.963314797652147}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6833646616541353, "accuracy_n": 292, "auc": 0.6833646616541353}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7343871638358104, "accuracy_n": 1902, "auc": 0.7343871638358104}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7710613531484867, "accuracy_n": 2000, "auc": 0.7710613531484867}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.903361344537815, "accuracy_n": 59, "auc": 0.903361344537815}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5185929648241207, "accuracy_n": 995, "auc": 0.5185929648241207}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5891238670694864, "accuracy_n": 993, "auc": 0.5891238670694864}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.33601609657947684, "accuracy_n": 497, "auc": 0.33601609657947684}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3110367892976589, "accuracy_n": 299, "auc": 0.3110367892976589}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2957393483709273, "accuracy_n": 399, "auc": 0.2957393483709273}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9351251158480075, "accuracy_n": 322, "auc": 0.9351251158480075}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.606203007518797, "accuracy_n": 292, "auc": 0.606203007518797}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5498473991507431, "accuracy_n": 1902, "auc": 0.5498473991507431}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7361017327255139, "accuracy_n": 2000, "auc": 0.7361017327255139}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7997198879551821, "accuracy_n": 59, "auc": 0.7997198879551821}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7055276381909548, "accuracy_n": 995, "auc": 0.7055276381909548}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9405840886203424, "accuracy_n": 993, "auc": 0.9405840886203424}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.631768953068592, "accuracy_n": 277, "auc": 0.631768953068592}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6234939759036144, "accuracy_n": 996, "auc": 0.6234939759036144}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.448692152917505, "accuracy_n": 497, "auc": 0.448692152917505}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.32, "accuracy_n": 500, "auc": 0.32}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30434782608695654, "accuracy_n": 299, "auc": 0.30434782608695654}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.39097744360902253, "accuracy_n": 399, "auc": 0.39097744360902253}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.958487797343219, "accuracy_n": 322, "auc": 0.958487797343219}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7953947368421053, "accuracy_n": 292, "auc": 0.7953947368421053}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7132165605095542, "accuracy_n": 1902, "auc": 0.7132165605095542}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7969166869239797, "accuracy_n": 2000, "auc": 0.7969166869239797}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9145658263305322, "accuracy_n": 59, "auc": 0.9145658263305322}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5567839195979899, "accuracy_n": 995, "auc": 0.5567839195979899}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8590130916414904, "accuracy_n": 993, "auc": 0.8590130916414904}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5983935742971888, "accuracy_n": 996, "auc": 0.5983935742971888}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3903420523138833, "accuracy_n": 497, "auc": 0.3903420523138833}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.344, "accuracy_n": 500, "auc": 0.344}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.39464882943143814, "accuracy_n": 299, "auc": 0.39464882943143814}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.39598997493734334, "accuracy_n": 399, "auc": 0.39598997493734334}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9174917491749175, "accuracy_n": 303, "auc": 0.9174917491749175}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9311862835959221, "accuracy_n": 322, "auc": 0.9311862835959221}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9060620300751879, "accuracy_n": 292, "auc": 0.9060620300751879}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.545497169143666, "accuracy_n": 1902, "auc": 0.545497169143666}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6740673383091296, "accuracy_n": 2000, "auc": 0.6740673383091296}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7296918767507004, "accuracy_n": 59, "auc": 0.7296918767507004}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5236180904522613, "accuracy_n": 995, "auc": 0.5236180904522613}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9516616314199395, "accuracy_n": 993, "auc": 0.9516616314199395}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3199195171026157, "accuracy_n": 497, "auc": 0.3199195171026157}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.384, "accuracy_n": 500, "auc": 0.384}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3411371237458194, "accuracy_n": 299, "auc": 0.3411371237458194}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9479842446709916, "accuracy_n": 322, "auc": 0.9479842446709916}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6807330827067669, "accuracy_n": 292, "auc": 0.6807330827067669}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7269041932059448, "accuracy_n": 1902, "auc": 0.7269041932059448}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7690316204149699, "accuracy_n": 2000, "auc": 0.7690316204149699}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9019607843137255, "accuracy_n": 59, "auc": 0.9019607843137255}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.785929648241206, "accuracy_n": 995, "auc": 0.785929648241206}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8318227593152064, "accuracy_n": 993, "auc": 0.8318227593152064}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5140562248995983, "accuracy_n": 996, "auc": 0.5140562248995983}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.29431438127090304, "accuracy_n": 299, "auc": 0.29431438127090304}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.24060150375939848, "accuracy_n": 399, "auc": 0.24060150375939848}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8283828382838284, "accuracy_n": 303, "auc": 0.8283828382838284}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6541936978683967, "accuracy_n": 322, "auc": 0.6541936978683967}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5262218045112782, "accuracy_n": 292, "auc": 0.5262218045112782}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9539951573849879, "accuracy_n": 413, "auc": 0.9539951573849879}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9103481068648267, "accuracy_n": 1902, "auc": 0.9103481068648267}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5222185208860399, "accuracy_n": 2000, "auc": 0.5222185208860399}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6050420168067228, "accuracy_n": 59, "auc": 0.6050420168067228}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9155778894472362, "accuracy_n": 995, "auc": 0.9155778894472362}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9415911379657603, "accuracy_n": 993, "auc": 0.9415911379657603}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.592057761732852, "accuracy_n": 277, "auc": 0.592057761732852}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6275100401606426, "accuracy_n": 996, "auc": 0.6275100401606426}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2837022132796781, "accuracy_n": 497, "auc": 0.2837022132796781}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.18546365914786966, "accuracy_n": 399, "auc": 0.18546365914786966}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9380213160333642, "accuracy_n": 322, "auc": 0.9380213160333642}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.687077067669173, "accuracy_n": 292, "auc": 0.687077067669173}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6603293082094832, "accuracy_n": 1902, "auc": 0.6603293082094832}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8307348952972022, "accuracy_n": 2000, "auc": 0.8307348952972022}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8935574229691876, "accuracy_n": 59, "auc": 0.8935574229691876}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7919597989949749, "accuracy_n": 995, "auc": 0.7919597989949749}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9436052366565961, "accuracy_n": 993, "auc": 0.9436052366565961}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7653429602888087, "accuracy_n": 277, "auc": 0.7653429602888087}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6335341365461847, "accuracy_n": 996, "auc": 0.6335341365461847}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.323943661971831, "accuracy_n": 497, "auc": 0.323943661971831}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.22556390977443608, "accuracy_n": 399, "auc": 0.22556390977443608}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.956750077232005, "accuracy_n": 322, "auc": 0.956750077232005}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6717575187969925, "accuracy_n": 292, "auc": 0.6717575187969925}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7430997876857749, "accuracy_n": 1902, "auc": 0.7430997876857749}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7520214797541912, "accuracy_n": 2000, "auc": 0.7520214797541912}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8585434173669467, "accuracy_n": 59, "auc": 0.8585434173669467}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8482412060301507, "accuracy_n": 995, "auc": 0.8482412060301507}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7381671701913394, "accuracy_n": 993, "auc": 0.7381671701913394}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.289738430583501, "accuracy_n": 497, "auc": 0.289738430583501}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.308, "accuracy_n": 500, "auc": 0.308}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.38847117794486213, "accuracy_n": 399, "auc": 0.38847117794486213}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6633663366336634, "accuracy_n": 303, "auc": 0.6633663366336634}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7291087426629594, "accuracy_n": 322, "auc": 0.7291087426629594}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5271616541353383, "accuracy_n": 292, "auc": 0.5271616541353383}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7796610169491526, "accuracy_n": 413, "auc": 0.7796610169491526}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6419033085633403, "accuracy_n": 1902, "auc": 0.6419033085633403}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6347581476913167, "accuracy_n": 2000, "auc": 0.6347581476913167}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6876750700280112, "accuracy_n": 59, "auc": 0.6876750700280112}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
