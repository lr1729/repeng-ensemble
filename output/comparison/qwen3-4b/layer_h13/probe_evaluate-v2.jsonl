{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5778894472361809, "accuracy_n": 995, "auc": 0.5778894472361809}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.9083585095669687, "accuracy_n": 993, "auc": 0.9083585095669687}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5130522088353414, "accuracy_n": 996, "auc": 0.5130522088353414}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2977867203219316, "accuracy_n": 497, "auc": 0.2977867203219316}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24561403508771928, "accuracy_n": 399, "auc": 0.24561403508771928}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.603606734630831, "accuracy_n": 322, "auc": 0.603606734630831}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5039003759398496, "accuracy_n": 292, "auc": 0.5039003759398496}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7324995576786979, "accuracy_n": 1902, "auc": 0.7324995576786979}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5859855407802216, "accuracy_n": 2000, "auc": 0.5859855407802216}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5672268907563025, "accuracy_n": 59, "auc": 0.5672268907563025}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.842211055276382, "accuracy_n": 995, "auc": 0.842211055276382}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8046324269889225, "accuracy_n": 993, "auc": 0.8046324269889225}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24, "accuracy_n": 500, "auc": 0.24}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25418060200668896, "accuracy_n": 299, "auc": 0.25418060200668896}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2556390977443609, "accuracy_n": 399, "auc": 0.2556390977443609}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5892029657089899, "accuracy_n": 322, "auc": 0.5892029657089899}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5772556390977444, "accuracy_n": 292, "auc": 0.5772556390977444}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8252300070771408, "accuracy_n": 1902, "auc": 0.8252300070771408}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5782717561039535, "accuracy_n": 2000, "auc": 0.5782717561039535}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5378151260504203, "accuracy_n": 59, "auc": 0.5378151260504203}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5903614457831325, "accuracy_n": 996, "auc": 0.5903614457831325}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2776659959758551, "accuracy_n": 497, "auc": 0.2776659959758551}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.256, "accuracy_n": 500, "auc": 0.256}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.30434782608695654, "accuracy_n": 299, "auc": 0.30434782608695654}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5668443002780352, "accuracy_n": 322, "auc": 0.5668443002780352}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5919642857142857, "accuracy_n": 292, "auc": 0.5919642857142857}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5394108280254777, "accuracy_n": 1902, "auc": 0.5394108280254777}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5190683836865109, "accuracy_n": 2000, "auc": 0.5190683836865109}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5840336134453781, "accuracy_n": 59, "auc": 0.5840336134453781}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.526686807653575, "accuracy_n": 993, "auc": 0.526686807653575}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25418060200668896, "accuracy_n": 299, "auc": 0.25418060200668896}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.553714859437751, "accuracy_n": 322, "auc": 0.553714859437751}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5196898496240601, "accuracy_n": 292, "auc": 0.5196898496240601}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5399515738498789, "accuracy_n": 413, "auc": 0.5399515738498789}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6829363499646144, "accuracy_n": 1902, "auc": 0.6829363499646144}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5066158883356892, "accuracy_n": 2000, "auc": 0.5066158883356892}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5336134453781513, "accuracy_n": 59, "auc": 0.5336134453781513}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5045317220543807, "accuracy_n": 993, "auc": 0.5045317220543807}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24346076458752516, "accuracy_n": 497, "auc": 0.24346076458752516}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.234, "accuracy_n": 500, "auc": 0.234}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5256024096385542, "accuracy_n": 322, "auc": 0.5256024096385542}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5167293233082707, "accuracy_n": 292, "auc": 0.5167293233082707}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6612548655343241, "accuracy_n": 1902, "auc": 0.6612548655343241}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5417165596780438, "accuracy_n": 2000, "auc": 0.5417165596780438}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5518207282913166, "accuracy_n": 59, "auc": 0.5518207282913166}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5297079556898288, "accuracy_n": 993, "auc": 0.5297079556898288}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5983935742971888, "accuracy_n": 996, "auc": 0.5983935742971888}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.43661971830985913, "accuracy_n": 497, "auc": 0.43661971830985913}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26421404682274247, "accuracy_n": 299, "auc": 0.26421404682274247}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6368937287611987, "accuracy_n": 322, "auc": 0.6368937287611987}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5344454887218045, "accuracy_n": 292, "auc": 0.5344454887218045}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5979277246992216, "accuracy_n": 1902, "auc": 0.5979277246992216}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5309176612757205, "accuracy_n": 2000, "auc": 0.5309176612757205}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5042016806722689, "accuracy_n": 59, "auc": 0.5042016806722689}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6022155085599195, "accuracy_n": 993, "auc": 0.6022155085599195}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.36619718309859156, "accuracy_n": 497, "auc": 0.36619718309859156}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3110367892976589, "accuracy_n": 299, "auc": 0.3110367892976589}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3007518796992481, "accuracy_n": 399, "auc": 0.3007518796992481}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6114457831325301, "accuracy_n": 322, "auc": 0.6114457831325301}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5633458646616541, "accuracy_n": 292, "auc": 0.5633458646616541}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6296222576079264, "accuracy_n": 1902, "auc": 0.6296222576079264}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.545108784271122, "accuracy_n": 2000, "auc": 0.545108784271122}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5490196078431373, "accuracy_n": 59, "auc": 0.5490196078431373}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5953815261044176, "accuracy_n": 996, "auc": 0.5953815261044176}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2937625754527163, "accuracy_n": 497, "auc": 0.2937625754527163}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 500, "auc": 0.28}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5577557755775577, "accuracy_n": 303, "auc": 0.5577557755775577}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7094145814025332, "accuracy_n": 322, "auc": 0.7094145814025332}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6202537593984963, "accuracy_n": 292, "auc": 0.6202537593984963}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6561743341404358, "accuracy_n": 413, "auc": 0.6561743341404358}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6851556970983723, "accuracy_n": 1902, "auc": 0.6851556970983723}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.542981016146829, "accuracy_n": 2000, "auc": 0.542981016146829}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5644257703081234, "accuracy_n": 59, "auc": 0.5644257703081234}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6555891238670695, "accuracy_n": 993, "auc": 0.6555891238670695}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 399, "auc": 0.3333333333333333}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7128712871287128, "accuracy_n": 303, "auc": 0.7128712871287128}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6591365461847389, "accuracy_n": 322, "auc": 0.6591365461847389}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.611936090225564, "accuracy_n": 292, "auc": 0.611936090225564}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5519097222222221, "accuracy_n": 1902, "auc": 0.5519097222222221}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5107163686090679, "accuracy_n": 2000, "auc": 0.5107163686090679}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5336134453781513, "accuracy_n": 59, "auc": 0.5336134453781513}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6022155085599195, "accuracy_n": 993, "auc": 0.6022155085599195}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7821782178217822, "accuracy_n": 303, "auc": 0.7821782178217822}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5181881371640408, "accuracy_n": 322, "auc": 0.5181881371640408}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5384868421052631, "accuracy_n": 292, "auc": 0.5384868421052631}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.559322033898305, "accuracy_n": 413, "auc": 0.559322033898305}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6538393489030432, "accuracy_n": 1902, "auc": 0.6538393489030432}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5547142518449161, "accuracy_n": 2000, "auc": 0.5547142518449161}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.572829131652661, "accuracy_n": 59, "auc": 0.572829131652661}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.595166163141994, "accuracy_n": 993, "auc": 0.595166163141994}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.289738430583501, "accuracy_n": 497, "auc": 0.289738430583501}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.18796992481203006, "accuracy_n": 399, "auc": 0.18796992481203006}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6596771702193389, "accuracy_n": 322, "auc": 0.6596771702193389}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5104793233082707, "accuracy_n": 292, "auc": 0.5104793233082707}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5690072639225182, "accuracy_n": 413, "auc": 0.5690072639225182}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.585647779547063, "accuracy_n": 1902, "auc": 0.585647779547063}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5108414137503638, "accuracy_n": 2000, "auc": 0.5108414137503638}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.542016806722689, "accuracy_n": 59, "auc": 0.542016806722689}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5135678391959799, "accuracy_n": 995, "auc": 0.5135678391959799}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7240684793554885, "accuracy_n": 993, "auc": 0.7240684793554885}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6104417670682731, "accuracy_n": 996, "auc": 0.6104417670682731}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.32595573440643866, "accuracy_n": 497, "auc": 0.32595573440643866}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.252, "accuracy_n": 500, "auc": 0.252}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3076923076923077, "accuracy_n": 299, "auc": 0.3076923076923077}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5643564356435643, "accuracy_n": 303, "auc": 0.5643564356435643}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.759460920605499, "accuracy_n": 322, "auc": 0.759460920605499}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7866071428571428, "accuracy_n": 292, "auc": 0.7866071428571428}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.559322033898305, "accuracy_n": 413, "auc": 0.559322033898305}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5637783085633404, "accuracy_n": 1902, "auc": 0.5637783085633404}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5737311169332129, "accuracy_n": 2000, "auc": 0.5737311169332129}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5882352941176471, "accuracy_n": 59, "auc": 0.5882352941176471}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5881168177240684, "accuracy_n": 993, "auc": 0.5881168177240684}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5030120481927711, "accuracy_n": 996, "auc": 0.5030120481927711}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2608695652173913, "accuracy_n": 299, "auc": 0.2608695652173913}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2882205513784461, "accuracy_n": 399, "auc": 0.2882205513784461}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5082508250825083, "accuracy_n": 303, "auc": 0.5082508250825083}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.705205437133148, "accuracy_n": 322, "auc": 0.705205437133148}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7634398496240601, "accuracy_n": 292, "auc": 0.7634398496240601}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5877631811748054, "accuracy_n": 1902, "auc": 0.5877631811748054}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6445746914636185, "accuracy_n": 2000, "auc": 0.6445746914636185}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5168067226890756, "accuracy_n": 59, "auc": 0.5168067226890756}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5236656596173213, "accuracy_n": 993, "auc": 0.5236656596173213}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.34004024144869216, "accuracy_n": 497, "auc": 0.34004024144869216}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5196555452579549, "accuracy_n": 322, "auc": 0.5196555452579549}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.506296992481203, "accuracy_n": 292, "auc": 0.506296992481203}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5482262915782024, "accuracy_n": 1902, "auc": 0.5482262915782024}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5064008106926601, "accuracy_n": 2000, "auc": 0.5064008106926601}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5738693467336683, "accuracy_n": 995, "auc": 0.5738693467336683}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.675730110775428, "accuracy_n": 993, "auc": 0.675730110775428}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6194779116465864, "accuracy_n": 996, "auc": 0.6194779116465864}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.580205437133148, "accuracy_n": 322, "auc": 0.580205437133148}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6006343984962406, "accuracy_n": 292, "auc": 0.6006343984962406}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.9318017958244871, "accuracy_n": 1902, "auc": 0.9318017958244871}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5276174699066363, "accuracy_n": 2000, "auc": 0.5276174699066363}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5056022408963585, "accuracy_n": 59, "auc": 0.5056022408963585}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5306532663316583, "accuracy_n": 995, "auc": 0.5306532663316583}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8841893252769386, "accuracy_n": 993, "auc": 0.8841893252769386}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.18796992481203006, "accuracy_n": 399, "auc": 0.18796992481203006}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6567656765676567, "accuracy_n": 303, "auc": 0.6567656765676567}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6277803521779426, "accuracy_n": 322, "auc": 0.6277803521779426}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6355263157894737, "accuracy_n": 292, "auc": 0.6355263157894737}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6123385527246992, "accuracy_n": 1902, "auc": 0.6123385527246992}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7037130404075871, "accuracy_n": 2000, "auc": 0.7037130404075871}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5364145658263305, "accuracy_n": 59, "auc": 0.5364145658263305}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6773869346733669, "accuracy_n": 995, "auc": 0.6773869346733669}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8207452165156093, "accuracy_n": 993, "auc": 0.8207452165156093}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 497, "auc": 0.2857142857142857}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.308, "accuracy_n": 500, "auc": 0.308}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21303258145363407, "accuracy_n": 399, "auc": 0.21303258145363407}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6963696369636964, "accuracy_n": 303, "auc": 0.6963696369636964}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5809005251776336, "accuracy_n": 322, "auc": 0.5809005251776336}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5361372180451127, "accuracy_n": 292, "auc": 0.5361372180451127}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5883777239709443, "accuracy_n": 413, "auc": 0.5883777239709443}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7328921178343949, "accuracy_n": 1902, "auc": 0.7328921178343949}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5569070434426828, "accuracy_n": 2000, "auc": 0.5569070434426828}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6330532212885154, "accuracy_n": 59, "auc": 0.6330532212885154}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5287009063444109, "accuracy_n": 993, "auc": 0.5287009063444109}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3058350100603622, "accuracy_n": 497, "auc": 0.3058350100603622}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3177257525083612, "accuracy_n": 299, "auc": 0.3177257525083612}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6963696369636964, "accuracy_n": 303, "auc": 0.6963696369636964}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5035526722273711, "accuracy_n": 322, "auc": 0.5035526722273711}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5240131578947368, "accuracy_n": 292, "auc": 0.5240131578947368}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6707021791767555, "accuracy_n": 413, "auc": 0.6707021791767555}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6740047770700637, "accuracy_n": 1902, "auc": 0.6740047770700637}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5070695521083111, "accuracy_n": 2000, "auc": 0.5070695521083111}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6064425770308123, "accuracy_n": 59, "auc": 0.6064425770308123}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
