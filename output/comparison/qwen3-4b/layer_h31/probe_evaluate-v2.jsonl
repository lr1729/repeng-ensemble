{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9425981873111783, "accuracy_n": 993, "auc": 0.9425981873111783}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.631768953068592, "accuracy_n": 277, "auc": 0.631768953068592}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.677710843373494, "accuracy_n": 996, "auc": 0.677710843373494}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7062374245472837, "accuracy_n": 497, "auc": 0.7062374245472837}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.736, "accuracy_n": 500, "auc": 0.736}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7658862876254181, "accuracy_n": 299, "auc": 0.7658862876254181}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7694235588972431, "accuracy_n": 399, "auc": 0.7694235588972431}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9216867469879517, "accuracy_n": 322, "auc": 0.9216867469879517}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8646146616541353, "accuracy_n": 292, "auc": 0.8646146616541353}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6038227618542109, "accuracy_n": 1902, "auc": 0.6038227618542109}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6935753807124372, "accuracy_n": 2000, "auc": 0.6935753807124372}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8277310924369748, "accuracy_n": 59, "auc": 0.8277310924369748}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9153846153846155, "accuracy_n": 23, "auc": 0.9153846153846155}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9236180904522613, "accuracy_n": 995, "auc": 0.9236180904522613}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9436052366565961, "accuracy_n": 993, "auc": 0.9436052366565961}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6425992779783394, "accuracy_n": 277, "auc": 0.6425992779783394}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6716867469879518, "accuracy_n": 996, "auc": 0.6716867469879518}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5231388329979879, "accuracy_n": 497, "auc": 0.5231388329979879}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.664, "accuracy_n": 500, "auc": 0.664}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6722408026755853, "accuracy_n": 299, "auc": 0.6722408026755853}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6892230576441103, "accuracy_n": 399, "auc": 0.6892230576441103}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7458745874587459, "accuracy_n": 303, "auc": 0.7458745874587459}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8463083101637319, "accuracy_n": 322, "auc": 0.8463083101637319}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8049342105263158, "accuracy_n": 292, "auc": 0.8049342105263158}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5914056970983722, "accuracy_n": 1902, "auc": 0.5914056970983722}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6323692853119975, "accuracy_n": 2000, "auc": 0.6323692853119975}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6442577030812325, "accuracy_n": 59, "auc": 0.6442577030812325}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 23, "auc": 0.9}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9226130653266331, "accuracy_n": 995, "auc": 0.9226130653266331}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.945619335347432, "accuracy_n": 993, "auc": 0.945619335347432}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7906137184115524, "accuracy_n": 277, "auc": 0.7906137184115524}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7409638554216867, "accuracy_n": 996, "auc": 0.7409638554216867}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4869215291750503, "accuracy_n": 497, "auc": 0.4869215291750503}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 500, "auc": 0.7}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68561872909699, "accuracy_n": 299, "auc": 0.68561872909699}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7518796992481203, "accuracy_n": 399, "auc": 0.7518796992481203}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8781665122026568, "accuracy_n": 322, "auc": 0.8781665122026568}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8146146616541352, "accuracy_n": 292, "auc": 0.8146146616541352}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.619874601910828, "accuracy_n": 1902, "auc": 0.619874601910828}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6431731855199727, "accuracy_n": 2000, "auc": 0.6431731855199727}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6918767507002801, "accuracy_n": 59, "auc": 0.6918767507002801}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.928643216080402, "accuracy_n": 995, "auc": 0.928643216080402}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9385699899295066, "accuracy_n": 993, "auc": 0.9385699899295066}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8194945848375451, "accuracy_n": 277, "auc": 0.8194945848375451}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7680722891566265, "accuracy_n": 996, "auc": 0.7680722891566265}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.46680080482897385, "accuracy_n": 497, "auc": 0.46680080482897385}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.666, "accuracy_n": 500, "auc": 0.666}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6454849498327759, "accuracy_n": 299, "auc": 0.6454849498327759}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7243107769423559, "accuracy_n": 399, "auc": 0.7243107769423559}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6563175780043251, "accuracy_n": 322, "auc": 0.6563175780043251}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51875, "accuracy_n": 292, "auc": 0.51875}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7360774818401937, "accuracy_n": 413, "auc": 0.7360774818401937}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5083676132342534, "accuracy_n": 1902, "auc": 0.5083676132342534}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5423787987463474, "accuracy_n": 2000, "auc": 0.5423787987463474}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.757703081232493, "accuracy_n": 59, "auc": 0.757703081232493}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9165829145728643, "accuracy_n": 995, "auc": 0.9165829145728643}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9486404833836858, "accuracy_n": 993, "auc": 0.9486404833836858}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8534136546184738, "accuracy_n": 996, "auc": 0.8534136546184738}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2837022132796781, "accuracy_n": 497, "auc": 0.2837022132796781}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.658, "accuracy_n": 500, "auc": 0.658}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6488294314381271, "accuracy_n": 299, "auc": 0.6488294314381271}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6265664160401002, "accuracy_n": 399, "auc": 0.6265664160401002}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8938832252085265, "accuracy_n": 322, "auc": 0.8938832252085265}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8169642857142858, "accuracy_n": 292, "auc": 0.8169642857142858}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5770534766454352, "accuracy_n": 1902, "auc": 0.5770534766454352}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6671348356756789, "accuracy_n": 2000, "auc": 0.6671348356756789}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8711484593837534, "accuracy_n": 59, "auc": 0.8711484593837534}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8307692307692308, "accuracy_n": 23, "auc": 0.8307692307692308}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.91356783919598, "accuracy_n": 995, "auc": 0.91356783919598}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7321248741188319, "accuracy_n": 993, "auc": 0.7321248741188319}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7545126353790613, "accuracy_n": 277, "auc": 0.7545126353790613}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5863453815261044, "accuracy_n": 996, "auc": 0.5863453815261044}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7303822937625755, "accuracy_n": 497, "auc": 0.7303822937625755}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.764, "accuracy_n": 500, "auc": 0.764}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7859531772575251, "accuracy_n": 299, "auc": 0.7859531772575251}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7819548872180451, "accuracy_n": 399, "auc": 0.7819548872180451}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9306930693069307, "accuracy_n": 303, "auc": 0.9306930693069307}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9042323138708681, "accuracy_n": 322, "auc": 0.9042323138708681}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8071428571428572, "accuracy_n": 292, "auc": 0.8071428571428572}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5127255838641189, "accuracy_n": 1902, "auc": 0.5127255838641189}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6351602928657246, "accuracy_n": 2000, "auc": 0.6351602928657246}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7324929971988795, "accuracy_n": 59, "auc": 0.7324929971988795}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9316582914572864, "accuracy_n": 995, "auc": 0.9316582914572864}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9113796576032226, "accuracy_n": 993, "auc": 0.9113796576032226}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7941767068273092, "accuracy_n": 996, "auc": 0.7941767068273092}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7243460764587525, "accuracy_n": 497, "auc": 0.7243460764587525}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.762, "accuracy_n": 500, "auc": 0.762}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7892976588628763, "accuracy_n": 299, "auc": 0.7892976588628763}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7819548872180451, "accuracy_n": 399, "auc": 0.7819548872180451}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9405940594059405, "accuracy_n": 303, "auc": 0.9405940594059405}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8970497374111832, "accuracy_n": 322, "auc": 0.8970497374111832}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7757518796992481, "accuracy_n": 292, "auc": 0.7757518796992481}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5026284943382874, "accuracy_n": 1902, "auc": 0.5026284943382874}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6626372120335441, "accuracy_n": 2000, "auc": 0.6626372120335441}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7240896358543417, "accuracy_n": 59, "auc": 0.7240896358543417}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9296482412060302, "accuracy_n": 995, "auc": 0.9296482412060302}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8731117824773413, "accuracy_n": 993, "auc": 0.8731117824773413}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7269076305220884, "accuracy_n": 996, "auc": 0.7269076305220884}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7283702213279678, "accuracy_n": 497, "auc": 0.7283702213279678}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7892976588628763, "accuracy_n": 299, "auc": 0.7892976588628763}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7844611528822055, "accuracy_n": 399, "auc": 0.7844611528822055}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9141914191419142, "accuracy_n": 303, "auc": 0.9141914191419142}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9265909793018227, "accuracy_n": 322, "auc": 0.9265909793018227}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8046522556390977, "accuracy_n": 292, "auc": 0.8046522556390977}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5213740711252653, "accuracy_n": 1902, "auc": 0.5213740711252653}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6868379484994083, "accuracy_n": 2000, "auc": 0.6868379484994083}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7899159663865547, "accuracy_n": 59, "auc": 0.7899159663865547}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8539778449144008, "accuracy_n": 993, "auc": 0.8539778449144008}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8050541516245487, "accuracy_n": 277, "auc": 0.8050541516245487}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.713855421686747, "accuracy_n": 996, "auc": 0.713855421686747}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7283702213279678, "accuracy_n": 497, "auc": 0.7283702213279678}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.764, "accuracy_n": 500, "auc": 0.764}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7859531772575251, "accuracy_n": 299, "auc": 0.7859531772575251}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7794486215538847, "accuracy_n": 399, "auc": 0.7794486215538847}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9372937293729373, "accuracy_n": 303, "auc": 0.9372937293729373}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8710611677479146, "accuracy_n": 322, "auc": 0.8710611677479146}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7572368421052631, "accuracy_n": 292, "auc": 0.7572368421052631}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.513736288039632, "accuracy_n": 1902, "auc": 0.513736288039632}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6122480215357744, "accuracy_n": 2000, "auc": 0.6122480215357744}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6246498599439776, "accuracy_n": 59, "auc": 0.6246498599439776}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9115577889447236, "accuracy_n": 995, "auc": 0.9115577889447236}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6958710976837865, "accuracy_n": 993, "auc": 0.6958710976837865}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7617328519855595, "accuracy_n": 277, "auc": 0.7617328519855595}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7871485943775101, "accuracy_n": 996, "auc": 0.7871485943775101}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6458752515090543, "accuracy_n": 497, "auc": 0.6458752515090543}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 500, "auc": 0.75}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 299, "auc": 0.7692307692307693}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7568922305764411, "accuracy_n": 399, "auc": 0.7568922305764411}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9660565338276181, "accuracy_n": 322, "auc": 0.9660565338276181}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8008458646616541, "accuracy_n": 292, "auc": 0.8008458646616541}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54611088995046, "accuracy_n": 1902, "auc": 0.54611088995046}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7237172619315573, "accuracy_n": 2000, "auc": 0.7237172619315573}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9509803921568627, "accuracy_n": 59, "auc": 0.9509803921568627}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8230769230769232, "accuracy_n": 23, "auc": 0.8230769230769232}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9266331658291457, "accuracy_n": 995, "auc": 0.9266331658291457}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.648539778449144, "accuracy_n": 993, "auc": 0.648539778449144}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8267148014440433, "accuracy_n": 277, "auc": 0.8267148014440433}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8122489959839357, "accuracy_n": 996, "auc": 0.8122489959839357}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6317907444668008, "accuracy_n": 497, "auc": 0.6317907444668008}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.682, "accuracy_n": 500, "auc": 0.682}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7591973244147158, "accuracy_n": 299, "auc": 0.7591973244147158}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7443609022556391, "accuracy_n": 399, "auc": 0.7443609022556391}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9604958294717332, "accuracy_n": 322, "auc": 0.9604958294717332}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7158364661654135, "accuracy_n": 292, "auc": 0.7158364661654135}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5001835633404105, "accuracy_n": 1902, "auc": 0.5001835633404105}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7271755103592398, "accuracy_n": 2000, "auc": 0.7271755103592398}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8907563025210083, "accuracy_n": 59, "auc": 0.8907563025210083}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9216080402010051, "accuracy_n": 995, "auc": 0.9216080402010051}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8700906344410876, "accuracy_n": 993, "auc": 0.8700906344410876}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8158844765342961, "accuracy_n": 277, "auc": 0.8158844765342961}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8052208835341366, "accuracy_n": 996, "auc": 0.8052208835341366}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6921529175050302, "accuracy_n": 497, "auc": 0.6921529175050302}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 500, "auc": 0.77}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7591973244147158, "accuracy_n": 299, "auc": 0.7591973244147158}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7769423558897243, "accuracy_n": 399, "auc": 0.7769423558897243}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9697250540624034, "accuracy_n": 322, "auc": 0.9697250540624034}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8232612781954887, "accuracy_n": 292, "auc": 0.8232612781954887}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5564026008492569, "accuracy_n": 1902, "auc": 0.5564026008492569}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7304016750046767, "accuracy_n": 2000, "auc": 0.7304016750046767}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.92296918767507, "accuracy_n": 59, "auc": 0.92296918767507}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.928643216080402, "accuracy_n": 995, "auc": 0.928643216080402}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9486404833836858, "accuracy_n": 993, "auc": 0.9486404833836858}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8122743682310469, "accuracy_n": 277, "auc": 0.8122743682310469}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7620481927710844, "accuracy_n": 996, "auc": 0.7620481927710844}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6861167002012073, "accuracy_n": 497, "auc": 0.6861167002012073}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.758, "accuracy_n": 500, "auc": 0.758}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7792642140468228, "accuracy_n": 299, "auc": 0.7792642140468228}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7543859649122807, "accuracy_n": 399, "auc": 0.7543859649122807}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9570957095709571, "accuracy_n": 303, "auc": 0.9570957095709571}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.955475749150448, "accuracy_n": 322, "auc": 0.955475749150448}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9281015037593984, "accuracy_n": 292, "auc": 0.9281015037593984}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5131723283793348, "accuracy_n": 1902, "auc": 0.5131723283793348}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7003178147311179, "accuracy_n": 2000, "auc": 0.7003178147311179}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8389355742296919, "accuracy_n": 59, "auc": 0.8389355742296919}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9538461538461538, "accuracy_n": 23, "auc": 0.9538461538461538}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8862034239677744, "accuracy_n": 993, "auc": 0.8862034239677744}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8267148014440433, "accuracy_n": 277, "auc": 0.8267148014440433}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7730923694779116, "accuracy_n": 996, "auc": 0.7730923694779116}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6539235412474849, "accuracy_n": 497, "auc": 0.6539235412474849}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 299, "auc": 0.782608695652174}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7619047619047619, "accuracy_n": 399, "auc": 0.7619047619047619}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9571748532591906, "accuracy_n": 322, "auc": 0.9571748532591906}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8367011278195489, "accuracy_n": 292, "auc": 0.8367011278195489}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5210323779193207, "accuracy_n": 1902, "auc": 0.5210323779193207}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6813099528929945, "accuracy_n": 2000, "auc": 0.6813099528929945}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.938375350140056, "accuracy_n": 59, "auc": 0.938375350140056}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8846153846153847, "accuracy_n": 23, "auc": 0.8846153846153847}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9065326633165829, "accuracy_n": 995, "auc": 0.9065326633165829}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6988922457200403, "accuracy_n": 993, "auc": 0.6988922457200403}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7075812274368231, "accuracy_n": 277, "auc": 0.7075812274368231}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8483935742971888, "accuracy_n": 996, "auc": 0.8483935742971888}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3038229376257545, "accuracy_n": 497, "auc": 0.3038229376257545}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 500, "auc": 0.28}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3612040133779264, "accuracy_n": 299, "auc": 0.3612040133779264}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3132832080200501, "accuracy_n": 399, "auc": 0.3132832080200501}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5742574257425742, "accuracy_n": 303, "auc": 0.5742574257425742}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5214704973741119, "accuracy_n": 322, "auc": 0.5214704973741119}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5488251879699249, "accuracy_n": 292, "auc": 0.5488251879699249}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8184019370460048, "accuracy_n": 413, "auc": 0.8184019370460048}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7441480891719745, "accuracy_n": 1902, "auc": 0.7441480891719745}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5896378592671955, "accuracy_n": 2000, "auc": 0.5896378592671955}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7843137254901962, "accuracy_n": 59, "auc": 0.7843137254901962}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9035175879396985, "accuracy_n": 995, "auc": 0.9035175879396985}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8529707955689829, "accuracy_n": 993, "auc": 0.8529707955689829}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8231046931407943, "accuracy_n": 277, "auc": 0.8231046931407943}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8242971887550201, "accuracy_n": 996, "auc": 0.8242971887550201}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5211267605633803, "accuracy_n": 497, "auc": 0.5211267605633803}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.616, "accuracy_n": 500, "auc": 0.616}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7123745819397993, "accuracy_n": 299, "auc": 0.7123745819397993}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6165413533834586, "accuracy_n": 399, "auc": 0.6165413533834586}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9570957095709571, "accuracy_n": 303, "auc": 0.9570957095709571}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8959298733395119, "accuracy_n": 322, "auc": 0.8959298733395119}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7281484962406015, "accuracy_n": 292, "auc": 0.7281484962406015}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5380799938075018, "accuracy_n": 1902, "auc": 0.5380799938075018}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7253413482267098, "accuracy_n": 2000, "auc": 0.7253413482267098}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9341736694677871, "accuracy_n": 59, "auc": 0.9341736694677871}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8974874371859296, "accuracy_n": 995, "auc": 0.8974874371859296}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7190332326283988, "accuracy_n": 993, "auc": 0.7190332326283988}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8158844765342961, "accuracy_n": 277, "auc": 0.8158844765342961}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.52, "accuracy_n": 100, "auc": 0.52}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8232931726907631, "accuracy_n": 996, "auc": 0.8232931726907631}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5311871227364185, "accuracy_n": 497, "auc": 0.5311871227364185}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 500, "auc": 0.63}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7357859531772575, "accuracy_n": 299, "auc": 0.7357859531772575}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6416040100250626, "accuracy_n": 399, "auc": 0.6416040100250626}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9735973597359736, "accuracy_n": 303, "auc": 0.9735973597359736}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.918520234785295, "accuracy_n": 322, "auc": 0.918520234785295}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7220864661654136, "accuracy_n": 292, "auc": 0.7220864661654136}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5391045205237084, "accuracy_n": 1902, "auc": 0.5391045205237084}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7044072910320627, "accuracy_n": 2000, "auc": 0.7044072910320627}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9495798319327731, "accuracy_n": 59, "auc": 0.9495798319327731}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9175879396984925, "accuracy_n": 995, "auc": 0.9175879396984925}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8680765357502518, "accuracy_n": 993, "auc": 0.8680765357502518}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7319277108433735, "accuracy_n": 996, "auc": 0.7319277108433735}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.30985915492957744, "accuracy_n": 497, "auc": 0.30985915492957744}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.688, "accuracy_n": 500, "auc": 0.688}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5919732441471572, "accuracy_n": 299, "auc": 0.5919732441471572}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42606516290726815, "accuracy_n": 399, "auc": 0.42606516290726815}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9308001235712079, "accuracy_n": 322, "auc": 0.9308001235712079}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8857142857142857, "accuracy_n": 292, "auc": 0.8857142857142857}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5120222929936306, "accuracy_n": 1902, "auc": 0.5120222929936306}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6652831672233677, "accuracy_n": 2000, "auc": 0.6652831672233677}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8543417366946778, "accuracy_n": 59, "auc": 0.8543417366946778}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9384615384615385, "accuracy_n": 23, "auc": 0.9384615384615385}}
