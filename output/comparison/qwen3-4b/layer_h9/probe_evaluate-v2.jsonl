{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6898288016112789, "accuracy_n": 993, "auc": 0.6898288016112789}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6124497991967871, "accuracy_n": 996, "auc": 0.6124497991967871}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2796780684104628, "accuracy_n": 497, "auc": 0.2796780684104628}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22742474916387959, "accuracy_n": 299, "auc": 0.22742474916387959}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7062706270627063, "accuracy_n": 303, "auc": 0.7062706270627063}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.552710843373494, "accuracy_n": 322, "auc": 0.552710843373494}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5038063909774436, "accuracy_n": 292, "auc": 0.5038063909774436}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.783677238145789, "accuracy_n": 1902, "auc": 0.783677238145789}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5306340588952612, "accuracy_n": 2000, "auc": 0.5306340588952612}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6636455186304129, "accuracy_n": 993, "auc": 0.6636455186304129}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2776659959758551, "accuracy_n": 497, "auc": 0.2776659959758551}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2832080200501253, "accuracy_n": 399, "auc": 0.2832080200501253}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5739110287303059, "accuracy_n": 322, "auc": 0.5739110287303059}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5477913533834586, "accuracy_n": 292, "auc": 0.5477913533834586}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8127156316348195, "accuracy_n": 1902, "auc": 0.8127156316348195}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5181980695030906, "accuracy_n": 2000, "auc": 0.5181980695030906}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6498599439775911, "accuracy_n": 59, "auc": 0.6498599439775911}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145728643216081, "accuracy_n": 995, "auc": 0.5145728643216081}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5035246727089627, "accuracy_n": 993, "auc": 0.5035246727089627}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.30181086519114686, "accuracy_n": 497, "auc": 0.30181086519114686}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26755852842809363, "accuracy_n": 299, "auc": 0.26755852842809363}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5378822984244671, "accuracy_n": 322, "auc": 0.5378822984244671}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6042293233082707, "accuracy_n": 292, "auc": 0.6042293233082707}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5326876513317191, "accuracy_n": 413, "auc": 0.5326876513317191}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5507994957537155, "accuracy_n": 1902, "auc": 0.5507994957537155}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5212461698673221, "accuracy_n": 2000, "auc": 0.5212461698673221}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5658263305322129, "accuracy_n": 59, "auc": 0.5658263305322129}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5035246727089627, "accuracy_n": 993, "auc": 0.5035246727089627}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28169014084507044, "accuracy_n": 497, "auc": 0.28169014084507044}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.286, "accuracy_n": 500, "auc": 0.286}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5279579857893111, "accuracy_n": 322, "auc": 0.5279579857893111}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5114661654135338, "accuracy_n": 292, "auc": 0.5114661654135338}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6944245399858457, "accuracy_n": 1902, "auc": 0.6944245399858457}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5110114751425265, "accuracy_n": 2000, "auc": 0.5110114751425265}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6064425770308123, "accuracy_n": 59, "auc": 0.6064425770308123}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.526686807653575, "accuracy_n": 993, "auc": 0.526686807653575}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3199195171026157, "accuracy_n": 497, "auc": 0.3199195171026157}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.252, "accuracy_n": 500, "auc": 0.252}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5189218412109978, "accuracy_n": 322, "auc": 0.5189218412109978}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060620300751879, "accuracy_n": 292, "auc": 0.5060620300751879}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.658615313163482, "accuracy_n": 1902, "auc": 0.658615313163482}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5266461192490489, "accuracy_n": 2000, "auc": 0.5266461192490489}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5980392156862746, "accuracy_n": 59, "auc": 0.5980392156862746}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5045317220543807, "accuracy_n": 993, "auc": 0.5045317220543807}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.41851106639839036, "accuracy_n": 497, "auc": 0.41851106639839036}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.322, "accuracy_n": 500, "auc": 0.322}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27424749163879597, "accuracy_n": 299, "auc": 0.27424749163879597}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5834877973432191, "accuracy_n": 322, "auc": 0.5834877973432191}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5459116541353384, "accuracy_n": 292, "auc": 0.5459116541353384}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6931462314225052, "accuracy_n": 1902, "auc": 0.6931462314225052}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.526760660598476, "accuracy_n": 2000, "auc": 0.526760660598476}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5504201680672269, "accuracy_n": 59, "auc": 0.5504201680672269}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5508559919436052, "accuracy_n": 993, "auc": 0.5508559919436052}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3420523138832998, "accuracy_n": 497, "auc": 0.3420523138832998}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.412, "accuracy_n": 500, "auc": 0.412}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25752508361204013, "accuracy_n": 299, "auc": 0.25752508361204013}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2807017543859649, "accuracy_n": 399, "auc": 0.2807017543859649}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5530197713932654, "accuracy_n": 322, "auc": 0.5530197713932654}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140037593984962, "accuracy_n": 292, "auc": 0.5140037593984962}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6609872611464969, "accuracy_n": 1902, "auc": 0.6609872611464969}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5248814822150796, "accuracy_n": 2000, "auc": 0.5248814822150796}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5126050420168067, "accuracy_n": 59, "auc": 0.5126050420168067}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5005025125628141, "accuracy_n": 995, "auc": 0.5005025125628141}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5206445115810675, "accuracy_n": 993, "auc": 0.5206445115810675}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5963855421686747, "accuracy_n": 996, "auc": 0.5963855421686747}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3702213279678068, "accuracy_n": 497, "auc": 0.3702213279678068}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2631578947368421, "accuracy_n": 399, "auc": 0.2631578947368421}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5445544554455446, "accuracy_n": 303, "auc": 0.5445544554455446}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5798192771084337, "accuracy_n": 322, "auc": 0.5798192771084337}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5394266917293233, "accuracy_n": 292, "auc": 0.5394266917293233}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6309392692852087, "accuracy_n": 1902, "auc": 0.6309392692852087}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5454038908045805, "accuracy_n": 2000, "auc": 0.5454038908045805}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5154061624649859, "accuracy_n": 59, "auc": 0.5154061624649859}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6475327291037261, "accuracy_n": 993, "auc": 0.6475327291037261}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6174698795180723, "accuracy_n": 996, "auc": 0.6174698795180723}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2917505030181087, "accuracy_n": 497, "auc": 0.2917505030181087}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.344, "accuracy_n": 500, "auc": 0.344}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2508361204013378, "accuracy_n": 299, "auc": 0.2508361204013378}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3182957393483709, "accuracy_n": 399, "auc": 0.3182957393483709}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6039603960396039, "accuracy_n": 303, "auc": 0.6039603960396039}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5913268458449181, "accuracy_n": 322, "auc": 0.5913268458449181}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5995065789473684, "accuracy_n": 292, "auc": 0.5995065789473684}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5282488499646143, "accuracy_n": 1902, "auc": 0.5282488499646143}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.527065270562673, "accuracy_n": 2000, "auc": 0.527065270562673}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6316526610644257, "accuracy_n": 59, "auc": 0.6316526610644257}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.500503524672709, "accuracy_n": 993, "auc": 0.500503524672709}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28169014084507044, "accuracy_n": 497, "auc": 0.28169014084507044}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7821782178217822, "accuracy_n": 303, "auc": 0.7821782178217822}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5273015137472968, "accuracy_n": 322, "auc": 0.5273015137472968}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5218045112781955, "accuracy_n": 292, "auc": 0.5218045112781955}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5395203025477707, "accuracy_n": 1902, "auc": 0.5395203025477707}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5736420847926101, "accuracy_n": 2000, "auc": 0.5736420847926101}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6176470588235294, "accuracy_n": 59, "auc": 0.6176470588235294}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6384692849949648, "accuracy_n": 993, "auc": 0.6384692849949648}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3702213279678068, "accuracy_n": 497, "auc": 0.3702213279678068}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.248, "accuracy_n": 500, "auc": 0.248}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27759197324414714, "accuracy_n": 299, "auc": 0.27759197324414714}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2556390977443609, "accuracy_n": 399, "auc": 0.2556390977443609}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5027417361754711, "accuracy_n": 322, "auc": 0.5027417361754711}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5582236842105264, "accuracy_n": 292, "auc": 0.5582236842105264}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5815496726822365, "accuracy_n": 1902, "auc": 0.5815496726822365}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.542703916113717, "accuracy_n": 2000, "auc": 0.542703916113717}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5028011204481793, "accuracy_n": 59, "auc": 0.5028011204481793}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6004016064257028, "accuracy_n": 996, "auc": 0.6004016064257028}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.26961770623742454, "accuracy_n": 497, "auc": 0.26961770623742454}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.242, "accuracy_n": 500, "auc": 0.242}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7244748223663886, "accuracy_n": 322, "auc": 0.7244748223663886}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7219924812030076, "accuracy_n": 292, "auc": 0.7219924812030076}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5727817586694975, "accuracy_n": 1902, "auc": 0.5727817586694975}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5823632331271589, "accuracy_n": 2000, "auc": 0.5823632331271589}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5182072829131652, "accuracy_n": 59, "auc": 0.5182072829131652}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5065326633165829, "accuracy_n": 995, "auc": 0.5065326633165829}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5589123867069486, "accuracy_n": 993, "auc": 0.5589123867069486}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.607429718875502, "accuracy_n": 996, "auc": 0.607429718875502}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2937625754527163, "accuracy_n": 497, "auc": 0.2937625754527163}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24414715719063546, "accuracy_n": 299, "auc": 0.24414715719063546}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.717871485943775, "accuracy_n": 322, "auc": 0.717871485943775}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.749859022556391, "accuracy_n": 292, "auc": 0.749859022556391}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5569007263922519, "accuracy_n": 413, "auc": 0.5569007263922519}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6263148000707714, "accuracy_n": 1902, "auc": 0.6263148000707714}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6227478119601175, "accuracy_n": 2000, "auc": 0.6227478119601175}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5812324929971989, "accuracy_n": 59, "auc": 0.5812324929971989}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6112789526686808, "accuracy_n": 993, "auc": 0.6112789526686808}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3158953722334004, "accuracy_n": 497, "auc": 0.3158953722334004}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2508361204013378, "accuracy_n": 299, "auc": 0.2508361204013378}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5841584158415841, "accuracy_n": 303, "auc": 0.5841584158415841}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6360827927092988, "accuracy_n": 322, "auc": 0.6360827927092988}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6356672932330827, "accuracy_n": 292, "auc": 0.6356672932330827}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6977275743099789, "accuracy_n": 1902, "auc": 0.6977275743099789}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5314308465355994, "accuracy_n": 2000, "auc": 0.5314308465355994}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.308, "accuracy_n": 500, "auc": 0.308}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2809364548494983, "accuracy_n": 299, "auc": 0.2809364548494983}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.21052631578947367, "accuracy_n": 399, "auc": 0.21052631578947367}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5082508250825083, "accuracy_n": 303, "auc": 0.5082508250825083}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6120636391720728, "accuracy_n": 322, "auc": 0.6120636391720728}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5842105263157894, "accuracy_n": 292, "auc": 0.5842105263157894}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8894959748761501, "accuracy_n": 1902, "auc": 0.8894959748761501}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5093018579707275, "accuracy_n": 2000, "auc": 0.5093018579707275}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6176470588235294, "accuracy_n": 59, "auc": 0.6176470588235294}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.525679758308157, "accuracy_n": 993, "auc": 0.525679758308157}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6184738955823293, "accuracy_n": 996, "auc": 0.6184738955823293}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.33400402414486924, "accuracy_n": 497, "auc": 0.33400402414486924}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24749163879598662, "accuracy_n": 299, "auc": 0.24749163879598662}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2531328320802005, "accuracy_n": 399, "auc": 0.2531328320802005}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6819972196478221, "accuracy_n": 322, "auc": 0.6819972196478221}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6604323308270676, "accuracy_n": 292, "auc": 0.6604323308270676}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5953954352441614, "accuracy_n": 1902, "auc": 0.5953954352441614}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6559823096137706, "accuracy_n": 2000, "auc": 0.6559823096137706}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5322128851540615, "accuracy_n": 59, "auc": 0.5322128851540615}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7076923076923076, "accuracy_n": 23, "auc": 0.7076923076923076}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5155778894472361, "accuracy_n": 995, "auc": 0.5155778894472361}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6183282980866063, "accuracy_n": 993, "auc": 0.6183282980866063}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6164658634538153, "accuracy_n": 996, "auc": 0.6164658634538153}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2776659959758551, "accuracy_n": 497, "auc": 0.2776659959758551}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2508361204013378, "accuracy_n": 299, "auc": 0.2508361204013378}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2781954887218045, "accuracy_n": 399, "auc": 0.2781954887218045}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.549698795180723, "accuracy_n": 322, "auc": 0.549698795180723}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5031484962406015, "accuracy_n": 292, "auc": 0.5031484962406015}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7692896319886766, "accuracy_n": 1902, "auc": 0.7692896319886766}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5152700124745033, "accuracy_n": 2000, "auc": 0.5152700124745033}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.65406162464986, "accuracy_n": 59, "auc": 0.65406162464986}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5165829145728643, "accuracy_n": 995, "auc": 0.5165829145728643}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5125881168177241, "accuracy_n": 993, "auc": 0.5125881168177241}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5843373493975904, "accuracy_n": 996, "auc": 0.5843373493975904}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2635814889336016, "accuracy_n": 497, "auc": 0.2635814889336016}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3, "accuracy_n": 500, "auc": 0.3}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27424749163879597, "accuracy_n": 299, "auc": 0.27424749163879597}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6253475440222428, "accuracy_n": 322, "auc": 0.6253475440222428}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5044642857142857, "accuracy_n": 292, "auc": 0.5044642857142857}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5399515738498789, "accuracy_n": 413, "auc": 0.5399515738498789}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7517947186836518, "accuracy_n": 1902, "auc": 0.7517947186836518}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5144847289871644, "accuracy_n": 2000, "auc": 0.5144847289871644}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5910364145658265, "accuracy_n": 59, "auc": 0.5910364145658265}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
