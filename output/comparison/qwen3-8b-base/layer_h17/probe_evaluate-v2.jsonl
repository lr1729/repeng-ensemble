{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9094567404426559, "accuracy_n": 994, "auc": 0.9094567404426559}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5740072202166066, "accuracy_n": 277, "auc": 0.5740072202166066}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6878147029204431, "accuracy_n": 993, "auc": 0.6878147029204431}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3440643863179074, "accuracy_n": 497, "auc": 0.3440643863179074}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23154362416107382, "accuracy_n": 298, "auc": 0.23154362416107382}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9438943894389439, "accuracy_n": 303, "auc": 0.9438943894389439}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8585881989496447, "accuracy_n": 322, "auc": 0.8585881989496447}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6223684210526316, "accuracy_n": 292, "auc": 0.6223684210526316}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7801629953998584, "accuracy_n": 1902, "auc": 0.7801629953998584}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7867850293956118, "accuracy_n": 2000, "auc": 0.7867850293956118}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8375350140056022, "accuracy_n": 59, "auc": 0.8375350140056022}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6173285198555957, "accuracy_n": 277, "auc": 0.6173285198555957}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7331319234642497, "accuracy_n": 993, "auc": 0.7331319234642497}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.25553319919517103, "accuracy_n": 497, "auc": 0.25553319919517103}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23366834170854273, "accuracy_n": 398, "auc": 0.23366834170854273}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8789388322520852, "accuracy_n": 322, "auc": 0.8789388322520852}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7453947368421052, "accuracy_n": 292, "auc": 0.7453947368421052}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8556241153573957, "accuracy_n": 1902, "auc": 0.8556241153573957}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8071583841766878, "accuracy_n": 2000, "auc": 0.8071583841766878}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8081232492997199, "accuracy_n": 59, "auc": 0.8081232492997199}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7917505030181087, "accuracy_n": 994, "auc": 0.7917505030181087}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9338014042126379, "accuracy_n": 997, "auc": 0.9338014042126379}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7978339350180506, "accuracy_n": 277, "auc": 0.7978339350180506}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6958710976837865, "accuracy_n": 993, "auc": 0.6958710976837865}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.289738430583501, "accuracy_n": 497, "auc": 0.289738430583501}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.358, "accuracy_n": 500, "auc": 0.358}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27181208053691275, "accuracy_n": 298, "auc": 0.27181208053691275}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.18341708542713567, "accuracy_n": 398, "auc": 0.18341708542713567}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8798656163113996, "accuracy_n": 322, "auc": 0.8798656163113996}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7584116541353384, "accuracy_n": 292, "auc": 0.7584116541353384}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.829792551309271, "accuracy_n": 1902, "auc": 0.829792551309271}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8252209047466135, "accuracy_n": 2000, "auc": 0.8252209047466135}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8823529411764706, "accuracy_n": 59, "auc": 0.8823529411764706}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8742454728370221, "accuracy_n": 994, "auc": 0.8742454728370221}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7978339350180506, "accuracy_n": 277, "auc": 0.7978339350180506}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5981873111782477, "accuracy_n": 993, "auc": 0.5981873111782477}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.25553319919517103, "accuracy_n": 497, "auc": 0.25553319919517103}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.19597989949748743, "accuracy_n": 398, "auc": 0.19597989949748743}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7596926166203275, "accuracy_n": 322, "auc": 0.7596926166203275}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7279135338345865, "accuracy_n": 292, "auc": 0.7279135338345865}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6522337225760793, "accuracy_n": 1902, "auc": 0.6522337225760793}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7509210825107864, "accuracy_n": 2000, "auc": 0.7509210825107864}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8151260504201681, "accuracy_n": 59, "auc": 0.8151260504201681}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6288866599799399, "accuracy_n": 997, "auc": 0.6288866599799399}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.316, "accuracy_n": 500, "auc": 0.316}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31208053691275167, "accuracy_n": 298, "auc": 0.31208053691275167}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2562814070351759, "accuracy_n": 398, "auc": 0.2562814070351759}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8118242199567501, "accuracy_n": 322, "auc": 0.8118242199567501}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6166353383458646, "accuracy_n": 292, "auc": 0.6166353383458646}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6391365888181175, "accuracy_n": 1902, "auc": 0.6391365888181175}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5671567435844339, "accuracy_n": 2000, "auc": 0.5671567435844339}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6204481792717087, "accuracy_n": 59, "auc": 0.6204481792717087}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5321931589537223, "accuracy_n": 994, "auc": 0.5321931589537223}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5235707121364093, "accuracy_n": 997, "auc": 0.5235707121364093}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5211267605633803, "accuracy_n": 497, "auc": 0.5211267605633803}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.358, "accuracy_n": 500, "auc": 0.358}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.36577181208053694, "accuracy_n": 298, "auc": 0.36577181208053694}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.49246231155778897, "accuracy_n": 398, "auc": 0.49246231155778897}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.594059405940594, "accuracy_n": 303, "auc": 0.594059405940594}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6787148594377512, "accuracy_n": 322, "auc": 0.6787148594377512}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7085526315789473, "accuracy_n": 292, "auc": 0.7085526315789473}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5860657731776362, "accuracy_n": 1902, "auc": 0.5860657731776362}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5906167126332607, "accuracy_n": 2000, "auc": 0.5906167126332607}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6204481792717087, "accuracy_n": 59, "auc": 0.6204481792717087}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5885311871227364, "accuracy_n": 994, "auc": 0.5885311871227364}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5436308926780341, "accuracy_n": 997, "auc": 0.5436308926780341}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4507042253521127, "accuracy_n": 497, "auc": 0.4507042253521127}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.532, "accuracy_n": 500, "auc": 0.532}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4664429530201342, "accuracy_n": 298, "auc": 0.4664429530201342}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6457286432160804, "accuracy_n": 398, "auc": 0.6457286432160804}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7491749174917491, "accuracy_n": 303, "auc": 0.7491749174917491}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8838430645659562, "accuracy_n": 322, "auc": 0.8838430645659562}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7507988721804512, "accuracy_n": 292, "auc": 0.7507988721804512}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5165815198159943, "accuracy_n": 1902, "auc": 0.5165815198159943}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6059607518314112, "accuracy_n": 2000, "auc": 0.6059607518314112}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6204481792717087, "accuracy_n": 59, "auc": 0.6204481792717087}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5995975855130785, "accuracy_n": 994, "auc": 0.5995975855130785}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.555667001003009, "accuracy_n": 997, "auc": 0.555667001003009}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4909456740442656, "accuracy_n": 497, "auc": 0.4909456740442656}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.478, "accuracy_n": 500, "auc": 0.478}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4697986577181208, "accuracy_n": 298, "auc": 0.4697986577181208}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6080402010050251, "accuracy_n": 398, "auc": 0.6080402010050251}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8977834414581403, "accuracy_n": 322, "auc": 0.8977834414581403}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.838110902255639, "accuracy_n": 292, "auc": 0.838110902255639}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8983050847457628, "accuracy_n": 413, "auc": 0.8983050847457628}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7130230449398443, "accuracy_n": 1902, "auc": 0.7130230449398443}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6154171655967804, "accuracy_n": 2000, "auc": 0.6154171655967804}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5434173669467787, "accuracy_n": 59, "auc": 0.5434173669467787}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5987963891675026, "accuracy_n": 997, "auc": 0.5987963891675026}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 497, "auc": 0.5130784708249497}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.532, "accuracy_n": 500, "auc": 0.532}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.49328859060402686, "accuracy_n": 298, "auc": 0.49328859060402686}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6306532663316583, "accuracy_n": 398, "auc": 0.6306532663316583}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.900990099009901, "accuracy_n": 303, "auc": 0.900990099009901}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7304989187519308, "accuracy_n": 322, "auc": 0.7304989187519308}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7823308270676691, "accuracy_n": 292, "auc": 0.7823308270676691}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5482373496107573, "accuracy_n": 1902, "auc": 0.5482373496107573}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5121878998318393, "accuracy_n": 2000, "auc": 0.5121878998318393}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 59, "auc": 0.5294117647058824}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9197592778335005, "accuracy_n": 997, "auc": 0.9197592778335005}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.555956678700361, "accuracy_n": 277, "auc": 0.555956678700361}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6918429003021148, "accuracy_n": 993, "auc": 0.6918429003021148}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3319919517102616, "accuracy_n": 497, "auc": 0.3319919517102616}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3087248322147651, "accuracy_n": 298, "auc": 0.3087248322147651}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.29396984924623115, "accuracy_n": 398, "auc": 0.29396984924623115}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9119941303676242, "accuracy_n": 322, "auc": 0.9119941303676242}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7788063909774435, "accuracy_n": 292, "auc": 0.7788063909774435}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7458808828733192, "accuracy_n": 1902, "auc": 0.7458808828733192}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8083718222278242, "accuracy_n": 2000, "auc": 0.8083718222278242}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9257703081232493, "accuracy_n": 59, "auc": 0.9257703081232493}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9406438631790744, "accuracy_n": 994, "auc": 0.9406438631790744}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6183282980866063, "accuracy_n": 993, "auc": 0.6183282980866063}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30181086519114686, "accuracy_n": 497, "auc": 0.30181086519114686}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.33221476510067116, "accuracy_n": 298, "auc": 0.33221476510067116}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.19095477386934673, "accuracy_n": 398, "auc": 0.19095477386934673}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8822984244670992, "accuracy_n": 322, "auc": 0.8822984244670992}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7947368421052632, "accuracy_n": 292, "auc": 0.7947368421052632}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6655276893135174, "accuracy_n": 1902, "auc": 0.6655276893135174}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7595842098997738, "accuracy_n": 2000, "auc": 0.7595842098997738}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8739495798319328, "accuracy_n": 59, "auc": 0.8739495798319328}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7756539235412475, "accuracy_n": 994, "auc": 0.7756539235412475}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6273917421953675, "accuracy_n": 993, "auc": 0.6273917421953675}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.386, "accuracy_n": 500, "auc": 0.386}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3724832214765101, "accuracy_n": 298, "auc": 0.3724832214765101}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30402010050251255, "accuracy_n": 398, "auc": 0.30402010050251255}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9835109669447019, "accuracy_n": 322, "auc": 0.9835109669447019}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9148966165413533, "accuracy_n": 292, "auc": 0.9148966165413533}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6204308209483369, "accuracy_n": 1902, "auc": 0.6204308209483369}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8238964266100062, "accuracy_n": 2000, "auc": 0.8238964266100062}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8823529411764706, "accuracy_n": 59, "auc": 0.8823529411764706}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8194583751253761, "accuracy_n": 997, "auc": 0.8194583751253761}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6203423967774421, "accuracy_n": 993, "auc": 0.6203423967774421}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3983903420523139, "accuracy_n": 497, "auc": 0.3983903420523139}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.394, "accuracy_n": 500, "auc": 0.394}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4161073825503356, "accuracy_n": 298, "auc": 0.4161073825503356}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.36683417085427134, "accuracy_n": 398, "auc": 0.36683417085427134}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9241581711461229, "accuracy_n": 322, "auc": 0.9241581711461229}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9038063909774436, "accuracy_n": 292, "auc": 0.9038063909774436}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5678089614295824, "accuracy_n": 1902, "auc": 0.5678089614295824}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.79803208958434, "accuracy_n": 2000, "auc": 0.79803208958434}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7380952380952381, "accuracy_n": 59, "auc": 0.7380952380952381}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5311871227364185, "accuracy_n": 994, "auc": 0.5311871227364185}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3963782696177062, "accuracy_n": 497, "auc": 0.3963782696177062}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.332, "accuracy_n": 500, "auc": 0.332}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.21105527638190955, "accuracy_n": 398, "auc": 0.21105527638190955}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9801980198019802, "accuracy_n": 303, "auc": 0.9801980198019802}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8804448563484708, "accuracy_n": 322, "auc": 0.8804448563484708}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7716635338345865, "accuracy_n": 292, "auc": 0.7716635338345865}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5438528397027601, "accuracy_n": 1902, "auc": 0.5438528397027601}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8145770623194973, "accuracy_n": 2000, "auc": 0.8145770623194973}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.907563025210084, "accuracy_n": 59, "auc": 0.907563025210084}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5110663983903421, "accuracy_n": 994, "auc": 0.5110663983903421}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7432296890672017, "accuracy_n": 997, "auc": 0.7432296890672017}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3561368209255533, "accuracy_n": 497, "auc": 0.3561368209255533}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23154362416107382, "accuracy_n": 298, "auc": 0.23154362416107382}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23869346733668342, "accuracy_n": 398, "auc": 0.23869346733668342}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.735973597359736, "accuracy_n": 303, "auc": 0.735973597359736}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5739110287303059, "accuracy_n": 322, "auc": 0.5739110287303059}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5414943609022557, "accuracy_n": 292, "auc": 0.5414943609022557}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6174334140435835, "accuracy_n": 413, "auc": 0.6174334140435835}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8986829883227176, "accuracy_n": 1902, "auc": 0.8986829883227176}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5279285822181807, "accuracy_n": 2000, "auc": 0.5279285822181807}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.665266106442577, "accuracy_n": 59, "auc": 0.665266106442577}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8953722334004024, "accuracy_n": 994, "auc": 0.8953722334004024}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9458375125376128, "accuracy_n": 997, "auc": 0.9458375125376128}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6534296028880866, "accuracy_n": 277, "auc": 0.6534296028880866}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.743202416918429, "accuracy_n": 993, "auc": 0.743202416918429}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3501006036217304, "accuracy_n": 497, "auc": 0.3501006036217304}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.352, "accuracy_n": 500, "auc": 0.352}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.31543624161073824, "accuracy_n": 298, "auc": 0.31543624161073824}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2185929648241206, "accuracy_n": 398, "auc": 0.2185929648241206}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8987102255174544, "accuracy_n": 322, "auc": 0.8987102255174544}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7753289473684211, "accuracy_n": 292, "auc": 0.7753289473684211}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.725362703467799, "accuracy_n": 1902, "auc": 0.725362703467799}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8697079645752117, "accuracy_n": 2000, "auc": 0.8697079645752117}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9047619047619048, "accuracy_n": 59, "auc": 0.9047619047619048}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9287863590772317, "accuracy_n": 997, "auc": 0.9287863590772317}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6606498194945848, "accuracy_n": 277, "auc": 0.6606498194945848}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7039274924471299, "accuracy_n": 993, "auc": 0.7039274924471299}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8965863453815262, "accuracy_n": 322, "auc": 0.8965863453815262}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7199248120300751, "accuracy_n": 292, "auc": 0.7199248120300751}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7843440375088464, "accuracy_n": 1902, "auc": 0.7843440375088464}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7229679914449116, "accuracy_n": 2000, "auc": 0.7229679914449116}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8389355742296919, "accuracy_n": 59, "auc": 0.8389355742296919}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7706237424547284, "accuracy_n": 994, "auc": 0.7706237424547284}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5325977933801405, "accuracy_n": 997, "auc": 0.5325977933801405}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30985915492957744, "accuracy_n": 497, "auc": 0.30985915492957744}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.322, "accuracy_n": 500, "auc": 0.322}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27638190954773867, "accuracy_n": 398, "auc": 0.27638190954773867}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8448844884488449, "accuracy_n": 303, "auc": 0.8448844884488449}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.53755406240346, "accuracy_n": 322, "auc": 0.53755406240346}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5853853383458646, "accuracy_n": 292, "auc": 0.5853853383458646}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7094430992736077, "accuracy_n": 413, "auc": 0.7094430992736077}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6380053520877565, "accuracy_n": 1902, "auc": 0.6380053520877565}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5633093546770384, "accuracy_n": 2000, "auc": 0.5633093546770384}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7128851540616247, "accuracy_n": 59, "auc": 0.7128851540616247}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9615384615384615, "accuracy_n": 23, "auc": 0.9615384615384615}}
