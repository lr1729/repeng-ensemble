{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6960882647943831, "accuracy_n": 997, "auc": 0.6960882647943831}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3199195171026157, "accuracy_n": 497, "auc": 0.3199195171026157}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24, "accuracy_n": 500, "auc": 0.24}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.515291936978684, "accuracy_n": 322, "auc": 0.515291936978684}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.531390977443609, "accuracy_n": 292, "auc": 0.531390977443609}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5326876513317191, "accuracy_n": 413, "auc": 0.5326876513317191}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7971890481245576, "accuracy_n": 1902, "auc": 0.7971890481245576}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5308926522474613, "accuracy_n": 2000, "auc": 0.5308926522474613}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5532212885154062, "accuracy_n": 59, "auc": 0.5532212885154062}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5382293762575453, "accuracy_n": 994, "auc": 0.5382293762575453}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5927783350050151, "accuracy_n": 997, "auc": 0.5927783350050151}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5448136958710977, "accuracy_n": 993, "auc": 0.5448136958710977}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.34808853118712274, "accuracy_n": 497, "auc": 0.34808853118712274}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.248, "accuracy_n": 500, "auc": 0.248}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24623115577889448, "accuracy_n": 398, "auc": 0.24623115577889448}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5169910410874266, "accuracy_n": 322, "auc": 0.5169910410874266}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5367951127819548, "accuracy_n": 292, "auc": 0.5367951127819548}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5956416464891041, "accuracy_n": 413, "auc": 0.5956416464891041}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.840625, "accuracy_n": 1902, "auc": 0.840625}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5362845987401452, "accuracy_n": 2000, "auc": 0.5362845987401452}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.572829131652661, "accuracy_n": 59, "auc": 0.572829131652661}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7482447342026078, "accuracy_n": 997, "auc": 0.7482447342026078}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.38028169014084506, "accuracy_n": 497, "auc": 0.38028169014084506}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25838926174496646, "accuracy_n": 298, "auc": 0.25838926174496646}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20351758793969849, "accuracy_n": 398, "auc": 0.20351758793969849}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5654541241890639, "accuracy_n": 322, "auc": 0.5654541241890639}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.568327067669173, "accuracy_n": 292, "auc": 0.568327067669173}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8255418435951875, "accuracy_n": 1902, "auc": 0.8255418435951875}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5114916484851031, "accuracy_n": 2000, "auc": 0.5114916484851031}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.518555667001003, "accuracy_n": 997, "auc": 0.518555667001003}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.32193158953722334, "accuracy_n": 497, "auc": 0.32193158953722334}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3053691275167785, "accuracy_n": 298, "auc": 0.3053691275167785}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2185929648241206, "accuracy_n": 398, "auc": 0.2185929648241206}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5268381217176398, "accuracy_n": 322, "auc": 0.5268381217176398}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5294172932330827, "accuracy_n": 292, "auc": 0.5294172932330827}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5472154963680388, "accuracy_n": 413, "auc": 0.5472154963680388}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5693736730360934, "accuracy_n": 1902, "auc": 0.5693736730360934}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5204228726570292, "accuracy_n": 2000, "auc": 0.5204228726570292}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5028011204481793, "accuracy_n": 59, "auc": 0.5028011204481793}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5245737211634904, "accuracy_n": 997, "auc": 0.5245737211634904}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 500, "auc": 0.25}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2483221476510067, "accuracy_n": 298, "auc": 0.2483221476510067}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.20100502512562815, "accuracy_n": 398, "auc": 0.20100502512562815}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5613994439295644, "accuracy_n": 322, "auc": 0.5613994439295644}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6101503759398497, "accuracy_n": 292, "auc": 0.6101503759398497}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5201853326256193, "accuracy_n": 1902, "auc": 0.5201853326256193}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5233219192128359, "accuracy_n": 2000, "auc": 0.5233219192128359}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5867602808425276, "accuracy_n": 997, "auc": 0.5867602808425276}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.4909456740442656, "accuracy_n": 497, "auc": 0.4909456740442656}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.314, "accuracy_n": 500, "auc": 0.314}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25125628140703515, "accuracy_n": 398, "auc": 0.25125628140703515}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5445544554455446, "accuracy_n": 303, "auc": 0.5445544554455446}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5026258881680569, "accuracy_n": 322, "auc": 0.5026258881680569}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5388627819548872, "accuracy_n": 292, "auc": 0.5388627819548872}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7541180113234253, "accuracy_n": 1902, "auc": 0.7541180113234253}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5067859497278517, "accuracy_n": 2000, "auc": 0.5067859497278517}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5238095238095238, "accuracy_n": 59, "auc": 0.5238095238095238}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135406218655968, "accuracy_n": 997, "auc": 0.5135406218655968}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3561368209255533, "accuracy_n": 497, "auc": 0.3561368209255533}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.366, "accuracy_n": 500, "auc": 0.366}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2562814070351759, "accuracy_n": 398, "auc": 0.2562814070351759}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.594059405940594, "accuracy_n": 303, "auc": 0.594059405940594}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.597505406240346, "accuracy_n": 322, "auc": 0.597505406240346}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5551691729323308, "accuracy_n": 292, "auc": 0.5551691729323308}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5423728813559322, "accuracy_n": 413, "auc": 0.5423728813559322}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6107395612172681, "accuracy_n": 1902, "auc": 0.6107395612172681}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5049842993320589, "accuracy_n": 2000, "auc": 0.5049842993320589}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5938375350140056, "accuracy_n": 59, "auc": 0.5938375350140056}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.540241448692153, "accuracy_n": 994, "auc": 0.540241448692153}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5646940822467402, "accuracy_n": 997, "auc": 0.5646940822467402}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3983903420523139, "accuracy_n": 497, "auc": 0.3983903420523139}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.256, "accuracy_n": 500, "auc": 0.256}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3187919463087248, "accuracy_n": 298, "auc": 0.3187919463087248}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.23618090452261306, "accuracy_n": 398, "auc": 0.23618090452261306}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5036685202347853, "accuracy_n": 322, "auc": 0.5036685202347853}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.568186090225564, "accuracy_n": 292, "auc": 0.568186090225564}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5738498789346247, "accuracy_n": 413, "auc": 0.5738498789346247}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.657698602264685, "accuracy_n": 1902, "auc": 0.657698602264685}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5161273219632287, "accuracy_n": 2000, "auc": 0.5161273219632287}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5518207282913166, "accuracy_n": 59, "auc": 0.5518207282913166}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5301810865191147, "accuracy_n": 994, "auc": 0.5301810865191147}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5195586760280843, "accuracy_n": 997, "auc": 0.5195586760280843}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6143001007049346, "accuracy_n": 993, "auc": 0.6143001007049346}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.33601609657947684, "accuracy_n": 497, "auc": 0.33601609657947684}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.356, "accuracy_n": 500, "auc": 0.356}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.36180904522613067, "accuracy_n": 398, "auc": 0.36180904522613067}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5608974358974359, "accuracy_n": 322, "auc": 0.5608974358974359}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6230733082706768, "accuracy_n": 292, "auc": 0.6230733082706768}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6295399515738499, "accuracy_n": 413, "auc": 0.6295399515738499}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6236851999292286, "accuracy_n": 1902, "auc": 0.6236851999292286}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5185401930096765, "accuracy_n": 2000, "auc": 0.5185401930096765}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5448179271708684, "accuracy_n": 59, "auc": 0.5448179271708684}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5095285857572718, "accuracy_n": 997, "auc": 0.5095285857572718}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3420523138832998, "accuracy_n": 497, "auc": 0.3420523138832998}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 500, "auc": 0.25}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6957059005251777, "accuracy_n": 322, "auc": 0.6957059005251777}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6012687969924813, "accuracy_n": 292, "auc": 0.6012687969924813}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5690072639225182, "accuracy_n": 413, "auc": 0.5690072639225182}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5865036712668082, "accuracy_n": 1902, "auc": 0.5865036712668082}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5945306255558258, "accuracy_n": 2000, "auc": 0.5945306255558258}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6708683473389356, "accuracy_n": 59, "auc": 0.6708683473389356}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5456369107321966, "accuracy_n": 997, "auc": 0.5456369107321966}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2917505030181087, "accuracy_n": 497, "auc": 0.2917505030181087}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2135678391959799, "accuracy_n": 398, "auc": 0.2135678391959799}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5145196169292554, "accuracy_n": 322, "auc": 0.5145196169292554}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5591165413533834, "accuracy_n": 292, "auc": 0.5591165413533834}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5157384987893463, "accuracy_n": 413, "auc": 0.5157384987893463}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6308751326963907, "accuracy_n": 1902, "auc": 0.6308751326963907}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5095619518646232, "accuracy_n": 2000, "auc": 0.5095619518646232}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5644257703081234, "accuracy_n": 59, "auc": 0.5644257703081234}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5372233400402414, "accuracy_n": 994, "auc": 0.5372233400402414}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135406218655968, "accuracy_n": 997, "auc": 0.5135406218655968}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2937625754527163, "accuracy_n": 497, "auc": 0.2937625754527163}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 500, "auc": 0.29}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24120603015075376, "accuracy_n": 398, "auc": 0.24120603015075376}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7928637627432809, "accuracy_n": 322, "auc": 0.7928637627432809}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7251879699248119, "accuracy_n": 292, "auc": 0.7251879699248119}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.576271186440678, "accuracy_n": 413, "auc": 0.576271186440678}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5743055555555555, "accuracy_n": 1902, "auc": 0.5743055555555555}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5993663712600248, "accuracy_n": 2000, "auc": 0.5993663712600248}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5784313725490197, "accuracy_n": 59, "auc": 0.5784313725490197}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5606820461384152, "accuracy_n": 997, "auc": 0.5606820461384152}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.32796780684104626, "accuracy_n": 497, "auc": 0.32796780684104626}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.304, "accuracy_n": 500, "auc": 0.304}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2562814070351759, "accuracy_n": 398, "auc": 0.2562814070351759}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7714704973741119, "accuracy_n": 322, "auc": 0.7714704973741119}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7359022556390977, "accuracy_n": 292, "auc": 0.7359022556390977}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5665859564164649, "accuracy_n": 413, "auc": 0.5665859564164649}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.712976601203114, "accuracy_n": 1902, "auc": 0.712976601203114}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5923868516534468, "accuracy_n": 2000, "auc": 0.5923868516534468}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 994, "auc": 0.5130784708249497}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6489468405215647, "accuracy_n": 997, "auc": 0.6489468405215647}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.28772635814889336, "accuracy_n": 497, "auc": 0.28772635814889336}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.242, "accuracy_n": 500, "auc": 0.242}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2986577181208054, "accuracy_n": 298, "auc": 0.2986577181208054}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.1984924623115578, "accuracy_n": 398, "auc": 0.1984924623115578}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.517299969107198, "accuracy_n": 322, "auc": 0.517299969107198}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5293233082706765, "accuracy_n": 292, "auc": 0.5293233082706765}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.612590799031477, "accuracy_n": 413, "auc": 0.612590799031477}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6061349964614297, "accuracy_n": 1902, "auc": 0.6061349964614297}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.523354931130138, "accuracy_n": 2000, "auc": 0.523354931130138}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5140056022408963, "accuracy_n": 59, "auc": 0.5140056022408963}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5291750503018109, "accuracy_n": 994, "auc": 0.5291750503018109}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5797392176529589, "accuracy_n": 997, "auc": 0.5797392176529589}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5629405840886204, "accuracy_n": 993, "auc": 0.5629405840886204}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.3460764587525151, "accuracy_n": 497, "auc": 0.3460764587525151}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.18592964824120603, "accuracy_n": 398, "auc": 0.18592964824120603}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.53755406240346, "accuracy_n": 322, "auc": 0.53755406240346}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5612781954887217, "accuracy_n": 292, "auc": 0.5612781954887217}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5472154963680388, "accuracy_n": 413, "auc": 0.5472154963680388}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8913714171974523, "accuracy_n": 1902, "auc": 0.8913714171974523}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5104442703816078, "accuracy_n": 2000, "auc": 0.5104442703816078}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5686274509803921, "accuracy_n": 59, "auc": 0.5686274509803921}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5171026156941649, "accuracy_n": 994, "auc": 0.5171026156941649}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6188565697091274, "accuracy_n": 997, "auc": 0.6188565697091274}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.36619718309859156, "accuracy_n": 497, "auc": 0.36619718309859156}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.23154362416107382, "accuracy_n": 298, "auc": 0.23154362416107382}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22613065326633167, "accuracy_n": 398, "auc": 0.22613065326633167}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7539388322520852, "accuracy_n": 322, "auc": 0.7539388322520852}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7469924812030075, "accuracy_n": 292, "auc": 0.7469924812030075}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6101694915254238, "accuracy_n": 413, "auc": 0.6101694915254238}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.687632696390658, "accuracy_n": 1902, "auc": 0.687632696390658}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.658353665673308, "accuracy_n": 2000, "auc": 0.658353665673308}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5868347338935574, "accuracy_n": 59, "auc": 0.5868347338935574}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5135406218655968, "accuracy_n": 997, "auc": 0.5135406218655968}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.308, "accuracy_n": 500, "auc": 0.308}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.24496644295302014, "accuracy_n": 298, "auc": 0.24496644295302014}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.22110552763819097, "accuracy_n": 398, "auc": 0.22110552763819097}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5956132221192463, "accuracy_n": 322, "auc": 0.5956132221192463}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5733552631578948, "accuracy_n": 292, "auc": 0.5733552631578948}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7868630573248407, "accuracy_n": 1902, "auc": 0.7868630573248407}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5303004384582835, "accuracy_n": 2000, "auc": 0.5303004384582835}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6008403361344538, "accuracy_n": 59, "auc": 0.6008403361344538}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5366098294884654, "accuracy_n": 997, "auc": 0.5366098294884654}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.35412474849094566, "accuracy_n": 497, "auc": 0.35412474849094566}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.27, "accuracy_n": 500, "auc": 0.27}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.1934673366834171, "accuracy_n": 398, "auc": 0.1934673366834171}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.539697250540624, "accuracy_n": 322, "auc": 0.539697250540624}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.537734962406015, "accuracy_n": 292, "auc": 0.537734962406015}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6150121065375302, "accuracy_n": 413, "auc": 0.6150121065375302}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.7701046089879688, "accuracy_n": 1902, "auc": 0.7701046089879688}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.5120883638993677, "accuracy_n": 2000, "auc": 0.5120883638993677}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.6120448179271708, "accuracy_n": 59, "auc": 0.6120448179271708}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h9", "token_idx": -1, "accuracy": 0.8538461538461539, "accuracy_n": 23, "auc": 0.8538461538461539}}
