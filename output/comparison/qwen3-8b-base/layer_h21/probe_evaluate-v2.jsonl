{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9446680080482898, "accuracy_n": 994, "auc": 0.9446680080482898}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8158844765342961, "accuracy_n": 277, "auc": 0.8158844765342961}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8318227593152064, "accuracy_n": 993, "auc": 0.8318227593152064}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.35412474849094566, "accuracy_n": 497, "auc": 0.35412474849094566}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.656, "accuracy_n": 500, "auc": 0.656}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.42953020134228187, "accuracy_n": 298, "auc": 0.42953020134228187}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.36432160804020103, "accuracy_n": 398, "auc": 0.36432160804020103}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9377896200185357, "accuracy_n": 322, "auc": 0.9377896200185357}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7980263157894737, "accuracy_n": 292, "auc": 0.7980263157894737}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7931053167020523, "accuracy_n": 1902, "auc": 0.7931053167020523}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7488903494161393, "accuracy_n": 2000, "auc": 0.7488903494161393}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9817927170868347, "accuracy_n": 59, "auc": 0.9817927170868347}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9436619718309859, "accuracy_n": 994, "auc": 0.9436619718309859}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.950852557673019, "accuracy_n": 997, "auc": 0.950852557673019}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7978339350180506, "accuracy_n": 277, "auc": 0.7978339350180506}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8116817724068479, "accuracy_n": 993, "auc": 0.8116817724068479}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 500, "auc": 0.57}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5637583892617449, "accuracy_n": 298, "auc": 0.5637583892617449}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4321608040201005, "accuracy_n": 398, "auc": 0.4321608040201005}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8517531665122027, "accuracy_n": 322, "auc": 0.8517531665122027}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7555921052631578, "accuracy_n": 292, "auc": 0.7555921052631578}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8397082891012031, "accuracy_n": 1902, "auc": 0.8397082891012031}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6960772838994876, "accuracy_n": 2000, "auc": 0.6960772838994876}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9341736694677871, "accuracy_n": 59, "auc": 0.9341736694677871}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9466800804828974, "accuracy_n": 994, "auc": 0.9466800804828974}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9488465396188566, "accuracy_n": 997, "auc": 0.9488465396188566}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8700361010830325, "accuracy_n": 277, "auc": 0.8700361010830325}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8328298086606244, "accuracy_n": 993, "auc": 0.8328298086606244}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3460764587525151, "accuracy_n": 497, "auc": 0.3460764587525151}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.556, "accuracy_n": 500, "auc": 0.556}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3221476510067114, "accuracy_n": 298, "auc": 0.3221476510067114}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.29396984924623115, "accuracy_n": 398, "auc": 0.29396984924623115}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8969338894037688, "accuracy_n": 322, "auc": 0.8969338894037688}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7377819548872181, "accuracy_n": 292, "auc": 0.7377819548872181}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9081221249115357, "accuracy_n": 1902, "auc": 0.9081221249115357}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7022485117127283, "accuracy_n": 2000, "auc": 0.7022485117127283}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8865546218487395, "accuracy_n": 59, "auc": 0.8865546218487395}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9456740442655935, "accuracy_n": 994, "auc": 0.9456740442655935}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.950852557673019, "accuracy_n": 997, "auc": 0.950852557673019}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8122743682310469, "accuracy_n": 277, "auc": 0.8122743682310469}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8257804632426989, "accuracy_n": 993, "auc": 0.8257804632426989}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4024144869215292, "accuracy_n": 497, "auc": 0.4024144869215292}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.608, "accuracy_n": 500, "auc": 0.608}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6610738255033557, "accuracy_n": 298, "auc": 0.6610738255033557}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7286432160804021, "accuracy_n": 398, "auc": 0.7286432160804021}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7815492740191535, "accuracy_n": 322, "auc": 0.7815492740191535}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8757988721804512, "accuracy_n": 292, "auc": 0.8757988721804512}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8332371284501061, "accuracy_n": 1902, "auc": 0.8332371284501061}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6837298264673547, "accuracy_n": 2000, "auc": 0.6837298264673547}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8417366946778712, "accuracy_n": 59, "auc": 0.8417366946778712}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8461538461538463, "accuracy_n": 23, "auc": 0.8461538461538463}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8551307847082495, "accuracy_n": 994, "auc": 0.8551307847082495}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.555956678700361, "accuracy_n": 277, "auc": 0.555956678700361}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3440643863179074, "accuracy_n": 497, "auc": 0.3440643863179074}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 500, "auc": 0.66}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.45302013422818793, "accuracy_n": 298, "auc": 0.45302013422818793}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 398, "auc": 0.5}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7623762376237624, "accuracy_n": 303, "auc": 0.7623762376237624}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7839048501699104, "accuracy_n": 322, "auc": 0.7839048501699104}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7228383458646616, "accuracy_n": 292, "auc": 0.7228383458646616}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8229011854210899, "accuracy_n": 1902, "auc": 0.8229011854210899}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.653839035891957, "accuracy_n": 2000, "auc": 0.653839035891957}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8613445378151261, "accuracy_n": 59, "auc": 0.8613445378151261}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5191146881287726, "accuracy_n": 994, "auc": 0.5191146881287726}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5486459378134403, "accuracy_n": 997, "auc": 0.5486459378134403}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6112789526686808, "accuracy_n": 993, "auc": 0.6112789526686808}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7645875251509054, "accuracy_n": 497, "auc": 0.7645875251509054}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.714, "accuracy_n": 500, "auc": 0.714}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7818791946308725, "accuracy_n": 298, "auc": 0.7818791946308725}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7663316582914573, "accuracy_n": 398, "auc": 0.7663316582914573}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6105610561056105, "accuracy_n": 303, "auc": 0.6105610561056105}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7962233549582947, "accuracy_n": 322, "auc": 0.7962233549582947}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6748590225563911, "accuracy_n": 292, "auc": 0.6748590225563911}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6740158351026184, "accuracy_n": 1902, "auc": 0.6740158351026184}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5084670566074353, "accuracy_n": 2000, "auc": 0.5084670566074353}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7717086834733894, "accuracy_n": 59, "auc": 0.7717086834733894}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9225352112676056, "accuracy_n": 994, "auc": 0.9225352112676056}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9338014042126379, "accuracy_n": 997, "auc": 0.9338014042126379}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7003610108303249, "accuracy_n": 277, "auc": 0.7003610108303249}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7170191339375629, "accuracy_n": 993, "auc": 0.7170191339375629}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7183098591549296, "accuracy_n": 497, "auc": 0.7183098591549296}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.724, "accuracy_n": 500, "auc": 0.724}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8389261744966443, "accuracy_n": 298, "auc": 0.8389261744966443}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8140703517587939, "accuracy_n": 398, "auc": 0.8140703517587939}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8854263206672845, "accuracy_n": 322, "auc": 0.8854263206672845}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8627349624060151, "accuracy_n": 292, "auc": 0.8627349624060151}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9903147699757869, "accuracy_n": 413, "auc": 0.9903147699757869}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6168259023354564, "accuracy_n": 1902, "auc": 0.6168259023354564}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5903061005022814, "accuracy_n": 2000, "auc": 0.5903061005022814}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8067226890756303, "accuracy_n": 59, "auc": 0.8067226890756303}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.716297786720322, "accuracy_n": 994, "auc": 0.716297786720322}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.917753259779338, "accuracy_n": 997, "auc": 0.917753259779338}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6787003610108303, "accuracy_n": 277, "auc": 0.6787003610108303}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6445115810674723, "accuracy_n": 993, "auc": 0.6445115810674723}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7364185110663984, "accuracy_n": 497, "auc": 0.7364185110663984}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.716, "accuracy_n": 500, "auc": 0.716}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7864321608040201, "accuracy_n": 398, "auc": 0.7864321608040201}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5643564356435643, "accuracy_n": 303, "auc": 0.5643564356435643}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8725671918443002, "accuracy_n": 322, "auc": 0.8725671918443002}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8536654135338345, "accuracy_n": 292, "auc": 0.8536654135338345}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9418886198547215, "accuracy_n": 413, "auc": 0.9418886198547215}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5290516631280963, "accuracy_n": 1902, "auc": 0.5290516631280963}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5400519587571113, "accuracy_n": 2000, "auc": 0.5400519587571113}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7773109243697479, "accuracy_n": 59, "auc": 0.7773109243697479}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5251509054325956, "accuracy_n": 994, "auc": 0.5251509054325956}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7502507522567703, "accuracy_n": 997, "auc": 0.7502507522567703}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6253776435045317, "accuracy_n": 993, "auc": 0.6253776435045317}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.744466800804829, "accuracy_n": 497, "auc": 0.744466800804829}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.708, "accuracy_n": 500, "auc": 0.708}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7885906040268457, "accuracy_n": 298, "auc": 0.7885906040268457}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7989949748743719, "accuracy_n": 398, "auc": 0.7989949748743719}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5742574257425742, "accuracy_n": 303, "auc": 0.5742574257425742}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7246679023787459, "accuracy_n": 322, "auc": 0.7246679023787459}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8856203007518797, "accuracy_n": 292, "auc": 0.8856203007518797}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9443099273607748, "accuracy_n": 413, "auc": 0.9443099273607748}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6331497699929227, "accuracy_n": 1902, "auc": 0.6331497699929227}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5279886038860029, "accuracy_n": 2000, "auc": 0.5279886038860029}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6918767507002801, "accuracy_n": 59, "auc": 0.6918767507002801}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9396378269617707, "accuracy_n": 994, "auc": 0.9396378269617707}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8050541516245487, "accuracy_n": 277, "auc": 0.8050541516245487}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7915407854984894, "accuracy_n": 993, "auc": 0.7915407854984894}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.26961770623742454, "accuracy_n": 497, "auc": 0.26961770623742454}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.474, "accuracy_n": 500, "auc": 0.474}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3825503355704698, "accuracy_n": 298, "auc": 0.3825503355704698}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.32663316582914576, "accuracy_n": 398, "auc": 0.32663316582914576}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9585264133456904, "accuracy_n": 322, "auc": 0.9585264133456904}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8353383458646616, "accuracy_n": 292, "auc": 0.8353383458646616}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6093285562632695, "accuracy_n": 1902, "auc": 0.6093285562632695}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7621661419772537, "accuracy_n": 2000, "auc": 0.7621661419772537}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9705882352941176, "accuracy_n": 59, "auc": 0.9705882352941176}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8307692307692307, "accuracy_n": 23, "auc": 0.8307692307692307}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9245472837022133, "accuracy_n": 994, "auc": 0.9245472837022133}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.954864593781344, "accuracy_n": 997, "auc": 0.954864593781344}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8231046931407943, "accuracy_n": 277, "auc": 0.8231046931407943}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7744209466263847, "accuracy_n": 993, "auc": 0.7744209466263847}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2977867203219316, "accuracy_n": 497, "auc": 0.2977867203219316}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.414, "accuracy_n": 500, "auc": 0.414}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.33221476510067116, "accuracy_n": 298, "auc": 0.33221476510067116}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2814070351758794, "accuracy_n": 398, "auc": 0.2814070351758794}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9423849243126351, "accuracy_n": 322, "auc": 0.9423849243126351}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6819548872180452, "accuracy_n": 292, "auc": 0.6819548872180452}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6289244957537155, "accuracy_n": 1902, "auc": 0.6289244957537155}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7307067851494389, "accuracy_n": 2000, "auc": 0.7307067851494389}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.938375350140056, "accuracy_n": 59, "auc": 0.938375350140056}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8963782696177063, "accuracy_n": 994, "auc": 0.8963782696177063}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9518555667001003, "accuracy_n": 997, "auc": 0.9518555667001003}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8231046931407943, "accuracy_n": 277, "auc": 0.8231046931407943}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8167170191339376, "accuracy_n": 993, "auc": 0.8167170191339376}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.40040241448692154, "accuracy_n": 497, "auc": 0.40040241448692154}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.458, "accuracy_n": 500, "auc": 0.458}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5973154362416108, "accuracy_n": 298, "auc": 0.5973154362416108}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.34673366834170855, "accuracy_n": 398, "auc": 0.34673366834170855}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9754016064257028, "accuracy_n": 322, "auc": 0.9754016064257028}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8721804511278195, "accuracy_n": 292, "auc": 0.8721804511278195}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7248877609695682, "accuracy_n": 1902, "auc": 0.7248877609695682}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7314260448021737, "accuracy_n": 2000, "auc": 0.7314260448021737}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9831932773109243, "accuracy_n": 59, "auc": 0.9831932773109243}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9235412474849095, "accuracy_n": 994, "auc": 0.9235412474849095}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9277833500501504, "accuracy_n": 997, "auc": 0.9277833500501504}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7184115523465704, "accuracy_n": 277, "auc": 0.7184115523465704}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6334340382678751, "accuracy_n": 993, "auc": 0.6334340382678751}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4607645875251509, "accuracy_n": 497, "auc": 0.4607645875251509}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.47, "accuracy_n": 500, "auc": 0.47}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6442953020134228, "accuracy_n": 298, "auc": 0.6442953020134228}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7261306532663316, "accuracy_n": 398, "auc": 0.7261306532663316}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9603960396039604, "accuracy_n": 303, "auc": 0.9603960396039604}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9392184121099784, "accuracy_n": 322, "auc": 0.9392184121099784}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9421522556390978, "accuracy_n": 292, "auc": 0.9421522556390978}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9903147699757869, "accuracy_n": 413, "auc": 0.9903147699757869}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7266763977353149, "accuracy_n": 1902, "auc": 0.7266763977353149}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6778902183688311, "accuracy_n": 2000, "auc": 0.6778902183688311}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9257703081232492, "accuracy_n": 59, "auc": 0.9257703081232492}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9287863590772317, "accuracy_n": 997, "auc": 0.9287863590772317}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7472924187725631, "accuracy_n": 277, "auc": 0.7472924187725631}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7270896273917422, "accuracy_n": 993, "auc": 0.7270896273917422}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5694164989939637, "accuracy_n": 497, "auc": 0.5694164989939637}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.538, "accuracy_n": 500, "auc": 0.538}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5671140939597316, "accuracy_n": 298, "auc": 0.5671140939597316}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.550251256281407, "accuracy_n": 398, "auc": 0.550251256281407}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9676011739264752, "accuracy_n": 322, "auc": 0.9676011739264752}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8040413533834587, "accuracy_n": 292, "auc": 0.8040413533834587}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6280442763623497, "accuracy_n": 1902, "auc": 0.6280442763623497}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7508940727602665, "accuracy_n": 2000, "auc": 0.7508940727602665}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9579831932773109, "accuracy_n": 59, "auc": 0.9579831932773109}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5754527162977867, "accuracy_n": 994, "auc": 0.5754527162977867}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9227683049147443, "accuracy_n": 997, "auc": 0.9227683049147443}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8447653429602888, "accuracy_n": 277, "auc": 0.8447653429602888}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8056394763343404, "accuracy_n": 993, "auc": 0.8056394763343404}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.36016096579476864, "accuracy_n": 497, "auc": 0.36016096579476864}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3523489932885906, "accuracy_n": 298, "auc": 0.3523489932885906}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.24874371859296482, "accuracy_n": 398, "auc": 0.24874371859296482}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7524752475247525, "accuracy_n": 303, "auc": 0.7524752475247525}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6361600247142416, "accuracy_n": 322, "auc": 0.6361600247142416}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6481672932330828, "accuracy_n": 292, "auc": 0.6481672932330828}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6174334140435835, "accuracy_n": 413, "auc": 0.6174334140435835}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8893721249115357, "accuracy_n": 1902, "auc": 0.8893721249115357}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5369858518925332, "accuracy_n": 2000, "auc": 0.5369858518925332}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6974789915966386, "accuracy_n": 59, "auc": 0.6974789915966386}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9225352112676056, "accuracy_n": 994, "auc": 0.9225352112676056}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.950852557673019, "accuracy_n": 997, "auc": 0.950852557673019}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8447653429602888, "accuracy_n": 277, "auc": 0.8447653429602888}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8328298086606244, "accuracy_n": 993, "auc": 0.8328298086606244}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.436, "accuracy_n": 500, "auc": 0.436}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.25879396984924624, "accuracy_n": 398, "auc": 0.25879396984924624}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9695319740500463, "accuracy_n": 322, "auc": 0.9695319740500463}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7730733082706767, "accuracy_n": 292, "auc": 0.7730733082706767}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6948469568294409, "accuracy_n": 1902, "auc": 0.6948469568294409}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.808854996653792, "accuracy_n": 2000, "auc": 0.808854996653792}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9705882352941176, "accuracy_n": 59, "auc": 0.9705882352941176}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9245472837022133, "accuracy_n": 994, "auc": 0.9245472837022133}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9478435305917753, "accuracy_n": 997, "auc": 0.9478435305917753}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7725631768953068, "accuracy_n": 277, "auc": 0.7725631768953068}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7371601208459214, "accuracy_n": 993, "auc": 0.7371601208459214}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.39235412474849096, "accuracy_n": 497, "auc": 0.39235412474849096}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.462, "accuracy_n": 500, "auc": 0.462}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5939597315436241, "accuracy_n": 298, "auc": 0.5939597315436241}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4849246231155779, "accuracy_n": 398, "auc": 0.4849246231155779}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9148130985480383, "accuracy_n": 322, "auc": 0.9148130985480383}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8094924812030075, "accuracy_n": 292, "auc": 0.8094924812030075}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7613599168435952, "accuracy_n": 1902, "auc": 0.7613599168435952}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6761060742928198, "accuracy_n": 2000, "auc": 0.6761060742928198}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9663865546218486, "accuracy_n": 59, "auc": 0.9663865546218486}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.60481444332999, "accuracy_n": 997, "auc": 0.60481444332999}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8483754512635379, "accuracy_n": 277, "auc": 0.8483754512635379}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4788732394366197, "accuracy_n": 497, "auc": 0.4788732394366197}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.40604026845637586, "accuracy_n": 298, "auc": 0.40604026845637586}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5025125628140703, "accuracy_n": 398, "auc": 0.5025125628140703}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6662805066419525, "accuracy_n": 322, "auc": 0.6662805066419525}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6236842105263158, "accuracy_n": 292, "auc": 0.6236842105263158}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6150121065375302, "accuracy_n": 413, "auc": 0.6150121065375302}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6903286447275301, "accuracy_n": 1902, "auc": 0.6903286447275301}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6244084114365286, "accuracy_n": 2000, "auc": 0.6244084114365286}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9005602240896359, "accuracy_n": 59, "auc": 0.9005602240896359}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9769230769230769, "accuracy_n": 23, "auc": 0.9769230769230769}}
