{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.506036217303823, "accuracy_n": 994, "auc": 0.506036217303823}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.872617853560682, "accuracy_n": 997, "auc": 0.872617853560682}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.35412474849094566, "accuracy_n": 497, "auc": 0.35412474849094566}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24496644295302014, "accuracy_n": 298, "auc": 0.24496644295302014}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20351758793969849, "accuracy_n": 398, "auc": 0.20351758793969849}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5676567656765676, "accuracy_n": 303, "auc": 0.5676567656765676}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.556456595613222, "accuracy_n": 322, "auc": 0.556456595613222}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5319548872180451, "accuracy_n": 292, "auc": 0.5319548872180451}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5569007263922519, "accuracy_n": 413, "auc": 0.5569007263922519}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8018389508138712, "accuracy_n": 1902, "auc": 0.8018389508138712}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.582730365662004, "accuracy_n": 2000, "auc": 0.582730365662004}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7647058823529411, "accuracy_n": 59, "auc": 0.7647058823529411}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.886317907444668, "accuracy_n": 994, "auc": 0.886317907444668}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8014042126379137, "accuracy_n": 997, "auc": 0.8014042126379137}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6143001007049346, "accuracy_n": 993, "auc": 0.6143001007049346}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2776659959758551, "accuracy_n": 497, "auc": 0.2776659959758551}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.268, "accuracy_n": 500, "auc": 0.268}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3087248322147651, "accuracy_n": 298, "auc": 0.3087248322147651}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2236180904522613, "accuracy_n": 398, "auc": 0.2236180904522613}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5008495520543714, "accuracy_n": 322, "auc": 0.5008495520543714}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5135338345864662, "accuracy_n": 292, "auc": 0.5135338345864662}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5302663438256658, "accuracy_n": 413, "auc": 0.5302663438256658}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8710456475583864, "accuracy_n": 1902, "auc": 0.8710456475583864}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6089178193327791, "accuracy_n": 2000, "auc": 0.6089178193327791}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6596638655462185, "accuracy_n": 59, "auc": 0.6596638655462185}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.629889669007021, "accuracy_n": 997, "auc": 0.629889669007021}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5861027190332326, "accuracy_n": 993, "auc": 0.5861027190332326}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.323943661971831, "accuracy_n": 497, "auc": 0.323943661971831}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.278, "accuracy_n": 500, "auc": 0.278}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20351758793969849, "accuracy_n": 398, "auc": 0.20351758793969849}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5462233549582948, "accuracy_n": 322, "auc": 0.5462233549582948}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150375939849625, "accuracy_n": 292, "auc": 0.5150375939849625}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6287763181174806, "accuracy_n": 1902, "auc": 0.6287763181174806}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5422107380764456, "accuracy_n": 2000, "auc": 0.5422107380764456}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6176470588235294, "accuracy_n": 59, "auc": 0.6176470588235294}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 994, "auc": 0.5855130784708249}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.526579739217653, "accuracy_n": 997, "auc": 0.526579739217653}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.30985915492957744, "accuracy_n": 497, "auc": 0.30985915492957744}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27, "accuracy_n": 500, "auc": 0.27}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23115577889447236, "accuracy_n": 398, "auc": 0.23115577889447236}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5379981464318814, "accuracy_n": 322, "auc": 0.5379981464318814}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5110432330827067, "accuracy_n": 292, "auc": 0.5110432330827067}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6656316348195329, "accuracy_n": 1902, "auc": 0.6656316348195329}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.513263288046985, "accuracy_n": 2000, "auc": 0.513263288046985}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5742296918767507, "accuracy_n": 59, "auc": 0.5742296918767507}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5275827482447342, "accuracy_n": 997, "auc": 0.5275827482447342}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.32595573440643866, "accuracy_n": 497, "auc": 0.32595573440643866}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 500, "auc": 0.26}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2348993288590604, "accuracy_n": 298, "auc": 0.2348993288590604}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.19597989949748743, "accuracy_n": 398, "auc": 0.19597989949748743}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6078544949026876, "accuracy_n": 322, "auc": 0.6078544949026876}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6978853383458646, "accuracy_n": 292, "auc": 0.6978853383458646}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6475805024769992, "accuracy_n": 1902, "auc": 0.6475805024769992}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5145837647390709, "accuracy_n": 2000, "auc": 0.5145837647390709}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5602240896358543, "accuracy_n": 59, "auc": 0.5602240896358543}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5090543259557344, "accuracy_n": 994, "auc": 0.5090543259557344}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.526579739217653, "accuracy_n": 997, "auc": 0.526579739217653}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6112789526686808, "accuracy_n": 993, "auc": 0.6112789526686808}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.4708249496981891, "accuracy_n": 497, "auc": 0.4708249496981891}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.348, "accuracy_n": 500, "auc": 0.348}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26884422110552764, "accuracy_n": 398, "auc": 0.26884422110552764}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5941071980228607, "accuracy_n": 322, "auc": 0.5941071980228607}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5672932330827067, "accuracy_n": 292, "auc": 0.5672932330827067}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5218351910828025, "accuracy_n": 1902, "auc": 0.5218351910828025}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5109844653920065, "accuracy_n": 2000, "auc": 0.5109844653920065}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.665266106442577, "accuracy_n": 59, "auc": 0.665266106442577}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8538461538461538, "accuracy_n": 23, "auc": 0.8538461538461538}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160965794768612, "accuracy_n": 994, "auc": 0.5160965794768612}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5997993981945837, "accuracy_n": 997, "auc": 0.5997993981945837}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3782696177062374, "accuracy_n": 497, "auc": 0.3782696177062374}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.426, "accuracy_n": 500, "auc": 0.426}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2986577181208054, "accuracy_n": 298, "auc": 0.2986577181208054}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.314070351758794, "accuracy_n": 398, "auc": 0.314070351758794}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5149057769539698, "accuracy_n": 322, "auc": 0.5149057769539698}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5547462406015038, "accuracy_n": 292, "auc": 0.5547462406015038}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6158284677990092, "accuracy_n": 1902, "auc": 0.6158284677990092}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5396608175551374, "accuracy_n": 2000, "auc": 0.5396608175551374}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 59, "auc": 0.5714285714285714}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5975855130784709, "accuracy_n": 994, "auc": 0.5975855130784709}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5897693079237714, "accuracy_n": 997, "auc": 0.5897693079237714}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6153071500503524, "accuracy_n": 993, "auc": 0.6153071500503524}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.38028169014084506, "accuracy_n": 497, "auc": 0.38028169014084506}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.296, "accuracy_n": 500, "auc": 0.296}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2986577181208054, "accuracy_n": 298, "auc": 0.2986577181208054}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2613065326633166, "accuracy_n": 398, "auc": 0.2613065326633166}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6270627062706271, "accuracy_n": 303, "auc": 0.6270627062706271}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5884306456595614, "accuracy_n": 322, "auc": 0.5884306456595614}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6252114661654135, "accuracy_n": 292, "auc": 0.6252114661654135}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6308519108280255, "accuracy_n": 1902, "auc": 0.6308519108280255}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5043170584581034, "accuracy_n": 2000, "auc": 0.5043170584581034}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5938375350140056, "accuracy_n": 59, "auc": 0.5938375350140056}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7153846153846155, "accuracy_n": 23, "auc": 0.7153846153846155}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.506036217303823, "accuracy_n": 994, "auc": 0.506036217303823}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5456369107321966, "accuracy_n": 997, "auc": 0.5456369107321966}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5780463242698892, "accuracy_n": 993, "auc": 0.5780463242698892}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3903420523138833, "accuracy_n": 497, "auc": 0.3903420523138833}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.364, "accuracy_n": 500, "auc": 0.364}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.33221476510067116, "accuracy_n": 298, "auc": 0.33221476510067116}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.4020100502512563, "accuracy_n": 398, "auc": 0.4020100502512563}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7095709570957096, "accuracy_n": 303, "auc": 0.7095709570957096}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5888168056842756, "accuracy_n": 322, "auc": 0.5888168056842756}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6091635338345864, "accuracy_n": 292, "auc": 0.6091635338345864}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6537530266343826, "accuracy_n": 413, "auc": 0.6537530266343826}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6148288216560509, "accuracy_n": 1902, "auc": 0.6148288216560509}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5019652094406081, "accuracy_n": 2000, "auc": 0.5019652094406081}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5896358543417366, "accuracy_n": 59, "auc": 0.5896358543417366}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5396188565697091, "accuracy_n": 997, "auc": 0.5396188565697091}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3641851106639839, "accuracy_n": 497, "auc": 0.3641851106639839}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2663316582914573, "accuracy_n": 398, "auc": 0.2663316582914573}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8877887788778878, "accuracy_n": 303, "auc": 0.8877887788778878}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5398903305529812, "accuracy_n": 322, "auc": 0.5398903305529812}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5915413533834586, "accuracy_n": 292, "auc": 0.5915413533834586}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6733877388535032, "accuracy_n": 1902, "auc": 0.6733877388535032}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5496254147747337, "accuracy_n": 2000, "auc": 0.5496254147747337}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5386158475426279, "accuracy_n": 997, "auc": 0.5386158475426279}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.30784708249496984, "accuracy_n": 497, "auc": 0.30784708249496984}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.252, "accuracy_n": 500, "auc": 0.252}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3221476510067114, "accuracy_n": 298, "auc": 0.3221476510067114}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.228643216080402, "accuracy_n": 398, "auc": 0.228643216080402}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5351791782514674, "accuracy_n": 322, "auc": 0.5351791782514674}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5573308270676692, "accuracy_n": 292, "auc": 0.5573308270676692}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.521555422859165, "accuracy_n": 1902, "auc": 0.521555422859165}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5108304097779298, "accuracy_n": 2000, "auc": 0.5108304097779298}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5994397759103641, "accuracy_n": 59, "auc": 0.5994397759103641}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6153846153846155, "accuracy_n": 23, "auc": 0.6153846153846155}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5175526579739218, "accuracy_n": 997, "auc": 0.5175526579739218}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.30181086519114686, "accuracy_n": 497, "auc": 0.30181086519114686}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 500, "auc": 0.25}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21608040201005024, "accuracy_n": 398, "auc": 0.21608040201005024}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7537843682421995, "accuracy_n": 322, "auc": 0.7537843682421995}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6643796992481202, "accuracy_n": 292, "auc": 0.6643796992481202}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5472154963680388, "accuracy_n": 413, "auc": 0.5472154963680388}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.567151008492569, "accuracy_n": 1902, "auc": 0.567151008492569}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6194856343139873, "accuracy_n": 2000, "auc": 0.6194856343139873}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5742296918767508, "accuracy_n": 59, "auc": 0.5742296918767508}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6846153846153847, "accuracy_n": 23, "auc": 0.6846153846153847}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5737211634904714, "accuracy_n": 997, "auc": 0.5737211634904714}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.289738430583501, "accuracy_n": 497, "auc": 0.289738430583501}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.30402010050251255, "accuracy_n": 398, "auc": 0.30402010050251255}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6896045721346926, "accuracy_n": 322, "auc": 0.6896045721346926}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7125939849624059, "accuracy_n": 292, "auc": 0.7125939849624059}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5569007263922519, "accuracy_n": 413, "auc": 0.5569007263922519}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5388059536447275, "accuracy_n": 1902, "auc": 0.5388059536447275}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6217964685251376, "accuracy_n": 2000, "auc": 0.6217964685251376}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5896358543417367, "accuracy_n": 59, "auc": 0.5896358543417367}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5687061183550652, "accuracy_n": 997, "auc": 0.5687061183550652}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.323943661971831, "accuracy_n": 497, "auc": 0.323943661971831}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20603015075376885, "accuracy_n": 398, "auc": 0.20603015075376885}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5122412727834416, "accuracy_n": 322, "auc": 0.5122412727834416}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5461466165413534, "accuracy_n": 292, "auc": 0.5461466165413534}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6464891041162227, "accuracy_n": 413, "auc": 0.6464891041162227}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5439866418966737, "accuracy_n": 1902, "auc": 0.5439866418966737}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5129091602068347, "accuracy_n": 2000, "auc": 0.5129091602068347}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5434173669467787, "accuracy_n": 59, "auc": 0.5434173669467787}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.693158953722334, "accuracy_n": 994, "auc": 0.693158953722334}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5847542627883651, "accuracy_n": 997, "auc": 0.5847542627883651}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.29577464788732394, "accuracy_n": 497, "auc": 0.29577464788732394}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23154362416107382, "accuracy_n": 298, "auc": 0.23154362416107382}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24120603015075376, "accuracy_n": 398, "auc": 0.24120603015075376}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7491749174917491, "accuracy_n": 303, "auc": 0.7491749174917491}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172227371022553, "accuracy_n": 322, "auc": 0.5172227371022553}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5301691729323308, "accuracy_n": 292, "auc": 0.5301691729323308}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.549636803874092, "accuracy_n": 413, "auc": 0.549636803874092}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.9294840322009906, "accuracy_n": 1902, "auc": 0.9294840322009906}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5295251585822481, "accuracy_n": 2000, "auc": 0.5295251585822481}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6848739495798319, "accuracy_n": 59, "auc": 0.6848739495798319}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8230769230769232, "accuracy_n": 23, "auc": 0.8230769230769232}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6368209255533199, "accuracy_n": 994, "auc": 0.6368209255533199}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.839518555667001, "accuracy_n": 997, "auc": 0.839518555667001}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6163141993957704, "accuracy_n": 993, "auc": 0.6163141993957704}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25553319919517103, "accuracy_n": 497, "auc": 0.25553319919517103}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24623115577889448, "accuracy_n": 398, "auc": 0.24623115577889448}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7108047574915044, "accuracy_n": 322, "auc": 0.7108047574915044}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7273966165413532, "accuracy_n": 292, "auc": 0.7273966165413532}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5544794188861986, "accuracy_n": 413, "auc": 0.5544794188861986}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7353547416843595, "accuracy_n": 1902, "auc": 0.7353547416843595}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.715690364221484, "accuracy_n": 2000, "auc": 0.715690364221484}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7128851540616247, "accuracy_n": 59, "auc": 0.7128851540616247}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5382293762575453, "accuracy_n": 994, "auc": 0.5382293762575453}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5325977933801405, "accuracy_n": 997, "auc": 0.5325977933801405}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2716297786720322, "accuracy_n": 497, "auc": 0.2716297786720322}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24496644295302014, "accuracy_n": 298, "auc": 0.24496644295302014}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24874371859296482, "accuracy_n": 398, "auc": 0.24874371859296482}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6243435279579859, "accuracy_n": 322, "auc": 0.6243435279579859}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5469454887218045, "accuracy_n": 292, "auc": 0.5469454887218045}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7758824309978769, "accuracy_n": 1902, "auc": 0.7758824309978769}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5533387552906599, "accuracy_n": 2000, "auc": 0.5533387552906599}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6428571428571428, "accuracy_n": 59, "auc": 0.6428571428571428}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5140845070422535, "accuracy_n": 994, "auc": 0.5140845070422535}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5075225677031093, "accuracy_n": 997, "auc": 0.5075225677031093}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3420523138832998, "accuracy_n": 497, "auc": 0.3420523138832998}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23366834170854273, "accuracy_n": 398, "auc": 0.23366834170854273}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.516759345072598, "accuracy_n": 322, "auc": 0.516759345072598}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5263627819548872, "accuracy_n": 292, "auc": 0.5263627819548872}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5824221514508139, "accuracy_n": 1902, "auc": 0.5824221514508139}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.502576430091263, "accuracy_n": 2000, "auc": 0.502576430091263}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6428571428571429, "accuracy_n": 59, "auc": 0.6428571428571429}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7923076923076923, "accuracy_n": 23, "auc": 0.7923076923076923}}
