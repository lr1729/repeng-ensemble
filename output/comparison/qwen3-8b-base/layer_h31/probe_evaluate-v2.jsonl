{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9356136820925554, "accuracy_n": 994, "auc": 0.9356136820925554}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8267148014440433, "accuracy_n": 277, "auc": 0.8267148014440433}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6243705941591138, "accuracy_n": 993, "auc": 0.6243705941591138}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6056338028169014, "accuracy_n": 497, "auc": 0.6056338028169014}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 500, "auc": 0.68}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7684563758389261, "accuracy_n": 298, "auc": 0.7684563758389261}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7613065326633166, "accuracy_n": 398, "auc": 0.7613065326633166}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7948331788693235, "accuracy_n": 322, "auc": 0.7948331788693235}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6691259398496241, "accuracy_n": 292, "auc": 0.6691259398496241}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5527495797947629, "accuracy_n": 1902, "auc": 0.5527495797947629}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5844149738055437, "accuracy_n": 2000, "auc": 0.5844149738055437}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8291316526610644, "accuracy_n": 59, "auc": 0.8291316526610644}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9378134403209629, "accuracy_n": 997, "auc": 0.9378134403209629}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7472924187725631, "accuracy_n": 277, "auc": 0.7472924187725631}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7421953675730111, "accuracy_n": 993, "auc": 0.7421953675730111}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4024144869215292, "accuracy_n": 497, "auc": 0.4024144869215292}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.438, "accuracy_n": 500, "auc": 0.438}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6006711409395973, "accuracy_n": 298, "auc": 0.6006711409395973}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.46733668341708545, "accuracy_n": 398, "auc": 0.46733668341708545}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.900990099009901, "accuracy_n": 303, "auc": 0.900990099009901}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6192075996292863, "accuracy_n": 322, "auc": 0.6192075996292863}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6215695488721804, "accuracy_n": 292, "auc": 0.6215695488721804}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7021791767554479, "accuracy_n": 413, "auc": 0.7021791767554479}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5668380661712668, "accuracy_n": 1902, "auc": 0.5668380661712668}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5469129355697406, "accuracy_n": 2000, "auc": 0.5469129355697406}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7198879551820727, "accuracy_n": 59, "auc": 0.7198879551820727}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9104627766599598, "accuracy_n": 994, "auc": 0.9104627766599598}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8335005015045135, "accuracy_n": 997, "auc": 0.8335005015045135}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8483754512635379, "accuracy_n": 277, "auc": 0.8483754512635379}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5498489425981873, "accuracy_n": 993, "auc": 0.5498489425981873}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.317907444668008, "accuracy_n": 497, "auc": 0.317907444668008}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 500, "auc": 0.26}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3691275167785235, "accuracy_n": 298, "auc": 0.3691275167785235}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.457286432160804, "accuracy_n": 398, "auc": 0.457286432160804}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6335727525486561, "accuracy_n": 322, "auc": 0.6335727525486561}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5640977443609022, "accuracy_n": 292, "auc": 0.5640977443609022}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8329297820823245, "accuracy_n": 413, "auc": 0.8329297820823245}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5209184801840057, "accuracy_n": 1902, "auc": 0.5209184801840057}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.536126541681547, "accuracy_n": 2000, "auc": 0.536126541681547}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5630252100840336, "accuracy_n": 59, "auc": 0.5630252100840336}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8692307692307693, "accuracy_n": 23, "auc": 0.8692307692307693}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5603621730382293, "accuracy_n": 994, "auc": 0.5603621730382293}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7220216606498195, "accuracy_n": 277, "auc": 0.7220216606498195}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8328298086606244, "accuracy_n": 993, "auc": 0.8328298086606244}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42655935613682094, "accuracy_n": 497, "auc": 0.42655935613682094}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.594, "accuracy_n": 500, "auc": 0.594}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7919463087248322, "accuracy_n": 298, "auc": 0.7919463087248322}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7135678391959799, "accuracy_n": 398, "auc": 0.7135678391959799}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5973597359735974, "accuracy_n": 303, "auc": 0.5973597359735974}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.733240654927402, "accuracy_n": 322, "auc": 0.733240654927402}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5577067669172933, "accuracy_n": 292, "auc": 0.5577067669172933}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6997578692493946, "accuracy_n": 413, "auc": 0.6997578692493946}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6065728945506015, "accuracy_n": 1902, "auc": 0.6065728945506015}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.559350925684172, "accuracy_n": 2000, "auc": 0.559350925684172}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5798319327731092, "accuracy_n": 59, "auc": 0.5798319327731092}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5472837022132797, "accuracy_n": 994, "auc": 0.5472837022132797}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9017051153460381, "accuracy_n": 997, "auc": 0.9017051153460381}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6173212487411883, "accuracy_n": 993, "auc": 0.6173212487411883}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5090543259557344, "accuracy_n": 497, "auc": 0.5090543259557344}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.662, "accuracy_n": 500, "auc": 0.662}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7550335570469798, "accuracy_n": 298, "auc": 0.7550335570469798}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7989949748743719, "accuracy_n": 398, "auc": 0.7989949748743719}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8745874587458746, "accuracy_n": 303, "auc": 0.8745874587458746}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.512936360827927, "accuracy_n": 322, "auc": 0.512936360827927}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5771616541353383, "accuracy_n": 292, "auc": 0.5771616541353383}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.648910411622276, "accuracy_n": 413, "auc": 0.648910411622276}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6058441702052371, "accuracy_n": 1902, "auc": 0.6058441702052371}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5243152778152913, "accuracy_n": 2000, "auc": 0.5243152778152913}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.546218487394958, "accuracy_n": 59, "auc": 0.546218487394958}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5110663983903421, "accuracy_n": 994, "auc": 0.5110663983903421}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8655967903711134, "accuracy_n": 997, "auc": 0.8655967903711134}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7009063444108762, "accuracy_n": 993, "auc": 0.7009063444108762}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7887323943661971, "accuracy_n": 497, "auc": 0.7887323943661971}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.762, "accuracy_n": 500, "auc": 0.762}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8456375838926175, "accuracy_n": 298, "auc": 0.8456375838926175}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8291457286432161, "accuracy_n": 398, "auc": 0.8291457286432161}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6707985789311092, "accuracy_n": 322, "auc": 0.6707985789311092}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6818139097744361, "accuracy_n": 292, "auc": 0.6818139097744361}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7409200968523002, "accuracy_n": 413, "auc": 0.7409200968523002}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5420138888888889, "accuracy_n": 1902, "auc": 0.5420138888888889}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5342613683539758, "accuracy_n": 2000, "auc": 0.5342613683539758}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8011204481792716, "accuracy_n": 59, "auc": 0.8011204481792716}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5241448692152918, "accuracy_n": 994, "auc": 0.5241448692152918}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7623363544813696, "accuracy_n": 993, "auc": 0.7623363544813696}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7605633802816901, "accuracy_n": 497, "auc": 0.7605633802816901}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.758, "accuracy_n": 500, "auc": 0.758}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8657718120805369, "accuracy_n": 298, "auc": 0.8657718120805369}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.821608040201005, "accuracy_n": 398, "auc": 0.821608040201005}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8778877887788779, "accuracy_n": 303, "auc": 0.8778877887788779}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6282051282051282, "accuracy_n": 322, "auc": 0.6282051282051282}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7628994360902256, "accuracy_n": 292, "auc": 0.7628994360902256}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.910411622276029, "accuracy_n": 413, "auc": 0.910411622276029}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5420365578556263, "accuracy_n": 1902, "auc": 0.5420365578556263}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5573882171463899, "accuracy_n": 2000, "auc": 0.5573882171463899}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6400560224089635, "accuracy_n": 59, "auc": 0.6400560224089635}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5090543259557344, "accuracy_n": 994, "auc": 0.5090543259557344}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8906720160481444, "accuracy_n": 997, "auc": 0.8906720160481444}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.798590130916415, "accuracy_n": 993, "auc": 0.798590130916415}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7545271629778671, "accuracy_n": 497, "auc": 0.7545271629778671}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 500, "auc": 0.75}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8523489932885906, "accuracy_n": 298, "auc": 0.8523489932885906}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8140703517587939, "accuracy_n": 398, "auc": 0.8140703517587939}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6216404077849861, "accuracy_n": 322, "auc": 0.6216404077849861}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6833646616541355, "accuracy_n": 292, "auc": 0.6833646616541355}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7699757869249395, "accuracy_n": 413, "auc": 0.7699757869249395}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5443360757254069, "accuracy_n": 1902, "auc": 0.5443360757254069}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.533628639939018, "accuracy_n": 2000, "auc": 0.533628639939018}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6638655462184874, "accuracy_n": 59, "auc": 0.6638655462184874}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6981891348088531, "accuracy_n": 994, "auc": 0.6981891348088531}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6137184115523465, "accuracy_n": 277, "auc": 0.6137184115523465}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7895266868076536, "accuracy_n": 993, "auc": 0.7895266868076536}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7746478873239436, "accuracy_n": 497, "auc": 0.7746478873239436}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.776, "accuracy_n": 500, "auc": 0.776}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8389261744966443, "accuracy_n": 298, "auc": 0.8389261744966443}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8366834170854272, "accuracy_n": 398, "auc": 0.8366834170854272}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7095709570957096, "accuracy_n": 303, "auc": 0.7095709570957096}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6465091133765832, "accuracy_n": 322, "auc": 0.6465091133765832}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7338345864661655, "accuracy_n": 292, "auc": 0.7338345864661655}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9588377723970944, "accuracy_n": 413, "auc": 0.9588377723970944}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5362139508138712, "accuracy_n": 1902, "auc": 0.5362139508138712}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.523790088221848, "accuracy_n": 2000, "auc": 0.523790088221848}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5140056022408964, "accuracy_n": 59, "auc": 0.5140056022408964}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.704225352112676, "accuracy_n": 994, "auc": 0.704225352112676}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5991943605236657, "accuracy_n": 993, "auc": 0.5991943605236657}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42655935613682094, "accuracy_n": 497, "auc": 0.42655935613682094}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.356, "accuracy_n": 500, "auc": 0.356}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5503355704697986, "accuracy_n": 298, "auc": 0.5503355704697986}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.25125628140703515, "accuracy_n": 398, "auc": 0.25125628140703515}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6975208526413346, "accuracy_n": 322, "auc": 0.6975208526413346}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6271616541353383, "accuracy_n": 292, "auc": 0.6271616541353383}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5575327317763623, "accuracy_n": 1902, "auc": 0.5575327317763623}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5821056401360891, "accuracy_n": 2000, "auc": 0.5821056401360891}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8809523809523809, "accuracy_n": 59, "auc": 0.8809523809523809}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8098591549295775, "accuracy_n": 994, "auc": 0.8098591549295775}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9107321965897693, "accuracy_n": 997, "auc": 0.9107321965897693}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8303249097472925, "accuracy_n": 277, "auc": 0.8303249097472925}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56797583081571, "accuracy_n": 993, "auc": 0.56797583081571}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4949698189134809, "accuracy_n": 497, "auc": 0.4949698189134809}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.274, "accuracy_n": 500, "auc": 0.274}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42953020134228187, "accuracy_n": 298, "auc": 0.42953020134228187}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4798994974874372, "accuracy_n": 398, "auc": 0.4798994974874372}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.944470188446092, "accuracy_n": 322, "auc": 0.944470188446092}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5428101503759398, "accuracy_n": 292, "auc": 0.5428101503759398}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5983921620665251, "accuracy_n": 1902, "auc": 0.5983921620665251}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6518553197704371, "accuracy_n": 2000, "auc": 0.6518553197704371}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9761904761904762, "accuracy_n": 59, "auc": 0.9761904761904762}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8309859154929577, "accuracy_n": 994, "auc": 0.8309859154929577}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8956870611835507, "accuracy_n": 997, "auc": 0.8956870611835507}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7653429602888087, "accuracy_n": 277, "auc": 0.7653429602888087}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5176233635448136, "accuracy_n": 993, "auc": 0.5176233635448136}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4869215291750503, "accuracy_n": 497, "auc": 0.4869215291750503}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.382, "accuracy_n": 500, "auc": 0.382}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6040268456375839, "accuracy_n": 298, "auc": 0.6040268456375839}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5100502512562815, "accuracy_n": 398, "auc": 0.5100502512562815}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8720651838121718, "accuracy_n": 322, "auc": 0.8720651838121718}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6643327067669174, "accuracy_n": 292, "auc": 0.6643327067669174}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5481024416135881, "accuracy_n": 1902, "auc": 0.5481024416135881}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5839468047965316, "accuracy_n": 2000, "auc": 0.5839468047965316}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9117647058823529, "accuracy_n": 59, "auc": 0.9117647058823529}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.613682092555332, "accuracy_n": 994, "auc": 0.613682092555332}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7632898696088265, "accuracy_n": 997, "auc": 0.7632898696088265}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6823104693140795, "accuracy_n": 277, "auc": 0.6823104693140795}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5327291037260826, "accuracy_n": 993, "auc": 0.5327291037260826}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5331991951710262, "accuracy_n": 497, "auc": 0.5331991951710262}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.544, "accuracy_n": 500, "auc": 0.544}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.714765100671141, "accuracy_n": 298, "auc": 0.714765100671141}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6206030150753769, "accuracy_n": 398, "auc": 0.6206030150753769}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9405940594059405, "accuracy_n": 303, "auc": 0.9405940594059405}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7338585109669447, "accuracy_n": 322, "auc": 0.7338585109669447}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.900093984962406, "accuracy_n": 292, "auc": 0.900093984962406}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8280871670702179, "accuracy_n": 413, "auc": 0.8280871670702179}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5365843949044586, "accuracy_n": 1902, "auc": 0.5365843949044586}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5692214889575137, "accuracy_n": 2000, "auc": 0.5692214889575137}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7871148459383752, "accuracy_n": 59, "auc": 0.7871148459383752}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8340040241448692, "accuracy_n": 994, "auc": 0.8340040241448692}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6990972918756269, "accuracy_n": 997, "auc": 0.6990972918756269}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7364620938628159, "accuracy_n": 277, "auc": 0.7364620938628159}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5901309164149043, "accuracy_n": 993, "auc": 0.5901309164149043}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4104627766599598, "accuracy_n": 497, "auc": 0.4104627766599598}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.502, "accuracy_n": 500, "auc": 0.502}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.540268456375839, "accuracy_n": 298, "auc": 0.540268456375839}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6532663316582915, "accuracy_n": 398, "auc": 0.6532663316582915}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9735973597359736, "accuracy_n": 303, "auc": 0.9735973597359736}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8431031819586037, "accuracy_n": 322, "auc": 0.8431031819586037}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.505874060150376, "accuracy_n": 292, "auc": 0.505874060150376}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5409567409766455, "accuracy_n": 1902, "auc": 0.5409567409766455}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.640858349864301, "accuracy_n": 2000, "auc": 0.640858349864301}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9565826330532212, "accuracy_n": 59, "auc": 0.9565826330532212}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7193158953722334, "accuracy_n": 994, "auc": 0.7193158953722334}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7713139418254764, "accuracy_n": 997, "auc": 0.7713139418254764}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6928499496475328, "accuracy_n": 993, "auc": 0.6928499496475328}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3420523138832998, "accuracy_n": 497, "auc": 0.3420523138832998}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.312, "accuracy_n": 500, "auc": 0.312}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4228187919463087, "accuracy_n": 298, "auc": 0.4228187919463087}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.22110552763819097, "accuracy_n": 398, "auc": 0.22110552763819097}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8448844884488449, "accuracy_n": 303, "auc": 0.8448844884488449}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5538307074451653, "accuracy_n": 322, "auc": 0.5538307074451653}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6044642857142857, "accuracy_n": 292, "auc": 0.6044642857142857}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6295399515738499, "accuracy_n": 413, "auc": 0.6295399515738499}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7169265304317056, "accuracy_n": 1902, "auc": 0.7169265304317056}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5352932408599503, "accuracy_n": 2000, "auc": 0.5352932408599503}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6750700280112044, "accuracy_n": 59, "auc": 0.6750700280112044}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7153846153846155, "accuracy_n": 23, "auc": 0.7153846153846155}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.682092555331992, "accuracy_n": 994, "auc": 0.682092555331992}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.776173285198556, "accuracy_n": 277, "auc": 0.776173285198556}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5055387713997986, "accuracy_n": 993, "auc": 0.5055387713997986}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5573440643863179, "accuracy_n": 497, "auc": 0.5573440643863179}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.452, "accuracy_n": 500, "auc": 0.452}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6610738255033557, "accuracy_n": 298, "auc": 0.6610738255033557}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.35175879396984927, "accuracy_n": 398, "auc": 0.35175879396984927}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9115693543404387, "accuracy_n": 322, "auc": 0.9115693543404387}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.650516917293233, "accuracy_n": 292, "auc": 0.650516917293233}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6151627742392074, "accuracy_n": 1902, "auc": 0.6151627742392074}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.685089317243525, "accuracy_n": 2000, "auc": 0.685089317243525}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9481792717086834, "accuracy_n": 59, "auc": 0.9481792717086834}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7374245472837022, "accuracy_n": 994, "auc": 0.7374245472837022}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56797583081571, "accuracy_n": 993, "auc": 0.56797583081571}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4507042253521127, "accuracy_n": 497, "auc": 0.4507042253521127}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.348, "accuracy_n": 500, "auc": 0.348}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5302013422818792, "accuracy_n": 298, "auc": 0.5302013422818792}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.20351758793969849, "accuracy_n": 398, "auc": 0.20351758793969849}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7744053135619401, "accuracy_n": 322, "auc": 0.7744053135619401}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6354323308270676, "accuracy_n": 292, "auc": 0.6354323308270676}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.561512517692852, "accuracy_n": 1902, "auc": 0.561512517692852}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5878997318031809, "accuracy_n": 2000, "auc": 0.5878997318031809}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9397759103641457, "accuracy_n": 59, "auc": 0.9397759103641457}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8682092555331992, "accuracy_n": 994, "auc": 0.8682092555331992}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.708124373119358, "accuracy_n": 997, "auc": 0.708124373119358}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7833935018050542, "accuracy_n": 277, "auc": 0.7833935018050542}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5105740181268882, "accuracy_n": 993, "auc": 0.5105740181268882}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4969818913480885, "accuracy_n": 497, "auc": 0.4969818913480885}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.37919463087248323, "accuracy_n": 298, "auc": 0.37919463087248323}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5527638190954773, "accuracy_n": 398, "auc": 0.5527638190954773}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5533286994130369, "accuracy_n": 322, "auc": 0.5533286994130369}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5534304511278196, "accuracy_n": 292, "auc": 0.5534304511278196}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8353510895883777, "accuracy_n": 413, "auc": 0.8353510895883777}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5521264596602973, "accuracy_n": 1902, "auc": 0.5521264596602973}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5002395864907232, "accuracy_n": 2000, "auc": 0.5002395864907232}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6568627450980392, "accuracy_n": 59, "auc": 0.6568627450980392}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-8B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
