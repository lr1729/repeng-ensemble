{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9428857715430862, "accuracy_n": 998, "auc": 0.9428857715430862}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9435483870967742, "accuracy_n": 992, "auc": 0.9435483870967742}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 276, "auc": 0.8333333333333334}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7877263581488934, "accuracy_n": 994, "auc": 0.7877263581488934}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3152610441767068, "accuracy_n": 498, "auc": 0.3152610441767068}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5863453815261044, "accuracy_n": 498, "auc": 0.5863453815261044}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.531986531986532, "accuracy_n": 297, "auc": 0.531986531986532}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.39348370927318294, "accuracy_n": 399, "auc": 0.39348370927318294}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9493358047574916, "accuracy_n": 322, "auc": 0.9493358047574916}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6991541353383459, "accuracy_n": 292, "auc": 0.6991541353383459}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8599688163481953, "accuracy_n": 1902, "auc": 0.8599688163481953}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7058057958923172, "accuracy_n": 2000, "auc": 0.7058057958923172}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9495798319327731, "accuracy_n": 59, "auc": 0.9495798319327731}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9153846153846155, "accuracy_n": 23, "auc": 0.9153846153846155}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9418837675350702, "accuracy_n": 998, "auc": 0.9418837675350702}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9495967741935484, "accuracy_n": 992, "auc": 0.9495967741935484}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7789855072463768, "accuracy_n": 276, "auc": 0.7789855072463768}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8048289738430584, "accuracy_n": 994, "auc": 0.8048289738430584}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3072289156626506, "accuracy_n": 498, "auc": 0.3072289156626506}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5401606425702812, "accuracy_n": 498, "auc": 0.5401606425702812}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5084175084175084, "accuracy_n": 297, "auc": 0.5084175084175084}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.37593984962406013, "accuracy_n": 399, "auc": 0.37593984962406013}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9801980198019802, "accuracy_n": 303, "auc": 0.9801980198019802}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9036530738337968, "accuracy_n": 322, "auc": 0.9036530738337968}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6822838345864662, "accuracy_n": 292, "auc": 0.6822838345864662}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8729432059447981, "accuracy_n": 1902, "auc": 0.8729432059447981}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.68046914936292, "accuracy_n": 2000, "auc": 0.68046914936292}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8711484593837534, "accuracy_n": 59, "auc": 0.8711484593837534}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9448897795591182, "accuracy_n": 998, "auc": 0.9448897795591182}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9445564516129032, "accuracy_n": 992, "auc": 0.9445564516129032}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7072434607645876, "accuracy_n": 994, "auc": 0.7072434607645876}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4598393574297189, "accuracy_n": 498, "auc": 0.4598393574297189}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4779116465863454, "accuracy_n": 498, "auc": 0.4779116465863454}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3602693602693603, "accuracy_n": 297, "auc": 0.3602693602693603}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3583959899749373, "accuracy_n": 399, "auc": 0.3583959899749373}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9540083410565338, "accuracy_n": 322, "auc": 0.9540083410565338}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7423872180451128, "accuracy_n": 292, "auc": 0.7423872180451128}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9060896585279548, "accuracy_n": 1902, "auc": 0.9060896585279548}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6541621525370659, "accuracy_n": 2000, "auc": 0.6541621525370659}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9159663865546218, "accuracy_n": 59, "auc": 0.9159663865546218}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8307692307692308, "accuracy_n": 23, "auc": 0.8307692307692308}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9318637274549099, "accuracy_n": 998, "auc": 0.9318637274549099}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9455645161290323, "accuracy_n": 992, "auc": 0.9455645161290323}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7536231884057971, "accuracy_n": 276, "auc": 0.7536231884057971}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 994, "auc": 0.8088531187122736}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3433734939759036, "accuracy_n": 498, "auc": 0.3433734939759036}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5281124497991968, "accuracy_n": 498, "auc": 0.5281124497991968}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.569023569023569, "accuracy_n": 297, "auc": 0.569023569023569}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5263157894736842, "accuracy_n": 399, "auc": 0.5263157894736842}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9801980198019802, "accuracy_n": 303, "auc": 0.9801980198019802}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9267454433117084, "accuracy_n": 322, "auc": 0.9267454433117084}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6718984962406015, "accuracy_n": 292, "auc": 0.6718984962406015}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8435487438075018, "accuracy_n": 1902, "auc": 0.8435487438075018}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6508109427503328, "accuracy_n": 2000, "auc": 0.6508109427503328}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.861344537815126, "accuracy_n": 59, "auc": 0.861344537815126}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7444889779559118, "accuracy_n": 998, "auc": 0.7444889779559118}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9233870967741935, "accuracy_n": 992, "auc": 0.9233870967741935}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.29518072289156627, "accuracy_n": 498, "auc": 0.29518072289156627}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5261044176706827, "accuracy_n": 498, "auc": 0.5261044176706827}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5252525252525253, "accuracy_n": 297, "auc": 0.5252525252525253}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5112781954887218, "accuracy_n": 399, "auc": 0.5112781954887218}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7194719471947195, "accuracy_n": 303, "auc": 0.7194719471947195}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8860441767068273, "accuracy_n": 322, "auc": 0.8860441767068273}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7891447368421053, "accuracy_n": 292, "auc": 0.7891447368421053}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7276130130927105, "accuracy_n": 1902, "auc": 0.7276130130927105}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6491513436350522, "accuracy_n": 2000, "auc": 0.6491513436350522}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8781512605042017, "accuracy_n": 59, "auc": 0.8781512605042017}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8307692307692308, "accuracy_n": 23, "auc": 0.8307692307692308}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5290581162324649, "accuracy_n": 998, "auc": 0.5290581162324649}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5362903225806451, "accuracy_n": 992, "auc": 0.5362903225806451}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5217391304347826, "accuracy_n": 276, "auc": 0.5217391304347826}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6116700201207244, "accuracy_n": 994, "auc": 0.6116700201207244}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6506024096385542, "accuracy_n": 498, "auc": 0.6506024096385542}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6646586345381527, "accuracy_n": 498, "auc": 0.6646586345381527}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7239057239057239, "accuracy_n": 297, "auc": 0.7239057239057239}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7443609022556391, "accuracy_n": 399, "auc": 0.7443609022556391}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5808580858085809, "accuracy_n": 303, "auc": 0.5808580858085809}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8858510966944702, "accuracy_n": 322, "auc": 0.8858510966944702}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6844924812030075, "accuracy_n": 292, "auc": 0.6844924812030075}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9806295399515739, "accuracy_n": 413, "auc": 0.9806295399515739}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6923865445859873, "accuracy_n": 1902, "auc": 0.6923865445859873}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5822401887081236, "accuracy_n": 2000, "auc": 0.5822401887081236}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7352941176470588, "accuracy_n": 59, "auc": 0.7352941176470588}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.935871743486974, "accuracy_n": 998, "auc": 0.935871743486974}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9143145161290323, "accuracy_n": 992, "auc": 0.9143145161290323}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5652173913043478, "accuracy_n": 276, "auc": 0.5652173913043478}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7877263581488934, "accuracy_n": 994, "auc": 0.7877263581488934}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.606425702811245, "accuracy_n": 498, "auc": 0.606425702811245}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6807228915662651, "accuracy_n": 498, "auc": 0.6807228915662651}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7676767676767676, "accuracy_n": 297, "auc": 0.7676767676767676}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7920792079207921, "accuracy_n": 303, "auc": 0.7920792079207921}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8646509113376585, "accuracy_n": 322, "auc": 0.8646509113376585}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8822838345864661, "accuracy_n": 292, "auc": 0.8822838345864661}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5765425955414012, "accuracy_n": 1902, "auc": 0.5765425955414012}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.615962362412831, "accuracy_n": 2000, "auc": 0.615962362412831}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8137254901960784, "accuracy_n": 59, "auc": 0.8137254901960784}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7835671342685371, "accuracy_n": 998, "auc": 0.7835671342685371}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7943548387096774, "accuracy_n": 992, "auc": 0.7943548387096774}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6385542168674698, "accuracy_n": 498, "auc": 0.6385542168674698}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6987951807228916, "accuracy_n": 498, "auc": 0.6987951807228916}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7643097643097643, "accuracy_n": 297, "auc": 0.7643097643097643}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7844611528822055, "accuracy_n": 399, "auc": 0.7844611528822055}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8811881188118812, "accuracy_n": 303, "auc": 0.8811881188118812}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8491272783441458, "accuracy_n": 322, "auc": 0.8491272783441458}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8604793233082707, "accuracy_n": 292, "auc": 0.8604793233082707}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5155011500353858, "accuracy_n": 1902, "auc": 0.5155011500353858}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6276065659703153, "accuracy_n": 2000, "auc": 0.6276065659703153}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8543417366946778, "accuracy_n": 59, "auc": 0.8543417366946778}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8336673346693386, "accuracy_n": 998, "auc": 0.8336673346693386}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8608870967741935, "accuracy_n": 992, "auc": 0.8608870967741935}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6546184738955824, "accuracy_n": 498, "auc": 0.6546184738955824}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6927710843373494, "accuracy_n": 498, "auc": 0.6927710843373494}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7710437710437711, "accuracy_n": 297, "auc": 0.7710437710437711}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6336633663366337, "accuracy_n": 303, "auc": 0.6336633663366337}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8195088044485634, "accuracy_n": 322, "auc": 0.8195088044485634}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7288533834586466, "accuracy_n": 292, "auc": 0.7288533834586466}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.937046004842615, "accuracy_n": 413, "auc": 0.937046004842615}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.70421974522293, "accuracy_n": 1902, "auc": 0.70421974522293}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5637315070740537, "accuracy_n": 2000, "auc": 0.5637315070740537}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6974789915966386, "accuracy_n": 59, "auc": 0.6974789915966386}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9438877755511023, "accuracy_n": 998, "auc": 0.9438877755511023}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9324596774193549, "accuracy_n": 992, "auc": 0.9324596774193549}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7753623188405797, "accuracy_n": 276, "auc": 0.7753623188405797}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8138832997987927, "accuracy_n": 994, "auc": 0.8138832997987927}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.41365461847389556, "accuracy_n": 498, "auc": 0.41365461847389556}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4983164983164983, "accuracy_n": 297, "auc": 0.4983164983164983}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.37844611528822053, "accuracy_n": 399, "auc": 0.37844611528822053}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9735973597359736, "accuracy_n": 303, "auc": 0.9735973597359736}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9411105962310781, "accuracy_n": 322, "auc": 0.9411105962310781}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6732142857142858, "accuracy_n": 292, "auc": 0.6732142857142858}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6611918347487615, "accuracy_n": 1902, "auc": 0.6611918347487615}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6920238205992364, "accuracy_n": 2000, "auc": 0.6920238205992364}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9089635854341735, "accuracy_n": 59, "auc": 0.9089635854341735}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9418837675350702, "accuracy_n": 998, "auc": 0.9418837675350702}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9506048387096774, "accuracy_n": 992, "auc": 0.9506048387096774}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6557971014492754, "accuracy_n": 276, "auc": 0.6557971014492754}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7464788732394366, "accuracy_n": 994, "auc": 0.7464788732394366}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.39558232931726905, "accuracy_n": 498, "auc": 0.39558232931726905}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2962962962962963, "accuracy_n": 297, "auc": 0.2962962962962963}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.43107769423558895, "accuracy_n": 399, "auc": 0.43107769423558895}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9265523632993512, "accuracy_n": 322, "auc": 0.9265523632993512}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7821428571428571, "accuracy_n": 292, "auc": 0.7821428571428571}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5847089525831564, "accuracy_n": 1902, "auc": 0.5847089525831564}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7243014728316922, "accuracy_n": 2000, "auc": 0.7243014728316922}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9047619047619048, "accuracy_n": 59, "auc": 0.9047619047619048}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9428857715430862, "accuracy_n": 998, "auc": 0.9428857715430862}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9203629032258065, "accuracy_n": 992, "auc": 0.9203629032258065}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.822463768115942, "accuracy_n": 276, "auc": 0.822463768115942}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8058350100603622, "accuracy_n": 994, "auc": 0.8058350100603622}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.4036144578313253, "accuracy_n": 498, "auc": 0.4036144578313253}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5060240963855421, "accuracy_n": 498, "auc": 0.5060240963855421}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5555555555555556, "accuracy_n": 297, "auc": 0.5555555555555556}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6416040100250626, "accuracy_n": 399, "auc": 0.6416040100250626}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9735973597359736, "accuracy_n": 303, "auc": 0.9735973597359736}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9876814952116156, "accuracy_n": 322, "auc": 0.9876814952116156}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7820958646616541, "accuracy_n": 292, "auc": 0.7820958646616541}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5180069002123142, "accuracy_n": 1902, "auc": 0.5180069002123142}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7280058100974451, "accuracy_n": 2000, "auc": 0.7280058100974451}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.876750700280112, "accuracy_n": 59, "auc": 0.876750700280112}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9230769230769231, "accuracy_n": 23, "auc": 0.9230769230769231}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9138276553106213, "accuracy_n": 998, "auc": 0.9138276553106213}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9012096774193549, "accuracy_n": 992, "auc": 0.9012096774193549}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5760869565217391, "accuracy_n": 276, "auc": 0.5760869565217391}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6458752515090543, "accuracy_n": 994, "auc": 0.6458752515090543}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5060240963855421, "accuracy_n": 498, "auc": 0.5060240963855421}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5883534136546185, "accuracy_n": 498, "auc": 0.5883534136546185}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 297, "auc": 0.6666666666666666}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6365914786967418, "accuracy_n": 399, "auc": 0.6365914786967418}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9438943894389439, "accuracy_n": 303, "auc": 0.9438943894389439}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9291396354649366, "accuracy_n": 322, "auc": 0.9291396354649366}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9160244360902257, "accuracy_n": 292, "auc": 0.9160244360902257}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.547057457537155, "accuracy_n": 1902, "auc": 0.547057457537155}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7182192771590544, "accuracy_n": 2000, "auc": 0.7182192771590544}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7899159663865546, "accuracy_n": 59, "auc": 0.7899159663865546}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9378757515030061, "accuracy_n": 998, "auc": 0.9378757515030061}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9455645161290323, "accuracy_n": 992, "auc": 0.9455645161290323}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7065217391304348, "accuracy_n": 276, "auc": 0.7065217391304348}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8058350100603622, "accuracy_n": 994, "auc": 0.8058350100603622}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3092369477911647, "accuracy_n": 498, "auc": 0.3092369477911647}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.40160642570281124, "accuracy_n": 498, "auc": 0.40160642570281124}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5589225589225589, "accuracy_n": 297, "auc": 0.5589225589225589}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3007518796992481, "accuracy_n": 399, "auc": 0.3007518796992481}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8859669447018845, "accuracy_n": 322, "auc": 0.8859669447018845}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.75625, "accuracy_n": 292, "auc": 0.75625}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7382630042462845, "accuracy_n": 1902, "auc": 0.7382630042462845}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6614267750657987, "accuracy_n": 2000, "auc": 0.6614267750657987}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9481792717086834, "accuracy_n": 59, "auc": 0.9481792717086834}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7374749498997996, "accuracy_n": 998, "auc": 0.7374749498997996}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9284274193548387, "accuracy_n": 992, "auc": 0.9284274193548387}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 276, "auc": 0.8333333333333334}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7364185110663984, "accuracy_n": 994, "auc": 0.7364185110663984}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3032128514056225, "accuracy_n": 498, "auc": 0.3032128514056225}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2791164658634538, "accuracy_n": 498, "auc": 0.2791164658634538}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.28619528619528617, "accuracy_n": 297, "auc": 0.28619528619528617}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2907268170426065, "accuracy_n": 399, "auc": 0.2907268170426065}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5457599629286376, "accuracy_n": 322, "auc": 0.5457599629286376}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5721334586466165, "accuracy_n": 292, "auc": 0.5721334586466165}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9207669851380043, "accuracy_n": 1902, "auc": 0.9207669851380043}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5404250934587386, "accuracy_n": 2000, "auc": 0.5404250934587386}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7184873949579832, "accuracy_n": 59, "auc": 0.7184873949579832}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9398797595190381, "accuracy_n": 998, "auc": 0.9398797595190381}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9455645161290323, "accuracy_n": 992, "auc": 0.9455645161290323}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8514492753623188, "accuracy_n": 276, "auc": 0.8514492753623188}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8098591549295775, "accuracy_n": 994, "auc": 0.8098591549295775}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3152610441767068, "accuracy_n": 498, "auc": 0.3152610441767068}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 498, "auc": 0.5}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5723905723905723, "accuracy_n": 297, "auc": 0.5723905723905723}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.3609022556390977, "accuracy_n": 399, "auc": 0.3609022556390977}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.958487797343219, "accuracy_n": 322, "auc": 0.958487797343219}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8052631578947369, "accuracy_n": 292, "auc": 0.8052631578947369}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7386920559094126, "accuracy_n": 1902, "auc": 0.7386920559094126}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7719786843050342, "accuracy_n": 2000, "auc": 0.7719786843050342}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9131652661064426, "accuracy_n": 59, "auc": 0.9131652661064426}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9428857715430862, "accuracy_n": 998, "auc": 0.9428857715430862}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.938508064516129, "accuracy_n": 992, "auc": 0.938508064516129}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8369565217391305, "accuracy_n": 276, "auc": 0.8369565217391305}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8048289738430584, "accuracy_n": 994, "auc": 0.8048289738430584}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.29116465863453816, "accuracy_n": 498, "auc": 0.29116465863453816}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.43172690763052207, "accuracy_n": 498, "auc": 0.43172690763052207}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.46464646464646464, "accuracy_n": 297, "auc": 0.46464646464646464}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.2731829573934837, "accuracy_n": 399, "auc": 0.2731829573934837}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9570957095709571, "accuracy_n": 303, "auc": 0.9570957095709571}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8513670064874884, "accuracy_n": 322, "auc": 0.8513670064874884}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6994830827067668, "accuracy_n": 292, "auc": 0.6994830827067668}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7852076698513801, "accuracy_n": 1902, "auc": 0.7852076698513801}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6581555941694952, "accuracy_n": 2000, "auc": 0.6581555941694952}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9243697478991596, "accuracy_n": 59, "auc": 0.9243697478991596}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9298597194388778, "accuracy_n": 998, "auc": 0.9298597194388778}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5181451612903226, "accuracy_n": 992, "auc": 0.5181451612903226}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7427536231884058, "accuracy_n": 276, "auc": 0.7427536231884058}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.40763052208835343, "accuracy_n": 498, "auc": 0.40763052208835343}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.41365461847389556, "accuracy_n": 498, "auc": 0.41365461847389556}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.44107744107744107, "accuracy_n": 297, "auc": 0.44107744107744107}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.37092731829573933, "accuracy_n": 399, "auc": 0.37092731829573933}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9537953795379538, "accuracy_n": 303, "auc": 0.9537953795379538}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.7511198640716713, "accuracy_n": 322, "auc": 0.7511198640716713}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.5007988721804512, "accuracy_n": 292, "auc": 0.5007988721804512}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.62779436482661, "accuracy_n": 1902, "auc": 0.62779436482661}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.6283628389848736, "accuracy_n": 2000, "auc": 0.6283628389848736}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.8557422969187675, "accuracy_n": 59, "auc": 0.8557422969187675}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h21", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
