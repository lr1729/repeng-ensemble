{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8719758064516129, "accuracy_n": 992, "auc": 0.8719758064516129}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5507246376811594, "accuracy_n": 276, "auc": 0.5507246376811594}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2891566265060241, "accuracy_n": 498, "auc": 0.2891566265060241}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2895622895622896, "accuracy_n": 297, "auc": 0.2895622895622896}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5980074142724745, "accuracy_n": 322, "auc": 0.5980074142724745}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5803571428571428, "accuracy_n": 292, "auc": 0.5803571428571428}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7070439667374381, "accuracy_n": 1902, "auc": 0.7070439667374381}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5422947684113966, "accuracy_n": 2000, "auc": 0.5422947684113966}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6624649859943977, "accuracy_n": 59, "auc": 0.6624649859943977}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.655310621242485, "accuracy_n": 998, "auc": 0.655310621242485}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6925403225806451, "accuracy_n": 992, "auc": 0.6925403225806451}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5471014492753623, "accuracy_n": 276, "auc": 0.5471014492753623}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6146881287726358, "accuracy_n": 994, "auc": 0.6146881287726358}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3032128514056225, "accuracy_n": 498, "auc": 0.3032128514056225}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3032128514056225, "accuracy_n": 498, "auc": 0.3032128514056225}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2356902356902357, "accuracy_n": 297, "auc": 0.2356902356902357}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5176475131294408, "accuracy_n": 322, "auc": 0.5176475131294408}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5091165413533835, "accuracy_n": 292, "auc": 0.5091165413533835}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8334936748053785, "accuracy_n": 1902, "auc": 0.8334936748053785}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5765621389321545, "accuracy_n": 2000, "auc": 0.5765621389321545}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5084033613445378, "accuracy_n": 59, "auc": 0.5084033613445378}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5201612903225806, "accuracy_n": 992, "auc": 0.5201612903225806}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3172690763052209, "accuracy_n": 498, "auc": 0.3172690763052209}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25502008032128515, "accuracy_n": 498, "auc": 0.25502008032128515}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23905723905723905, "accuracy_n": 297, "auc": 0.23905723905723905}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6283595922150139, "accuracy_n": 322, "auc": 0.6283595922150139}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5956766917293232, "accuracy_n": 292, "auc": 0.5956766917293232}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.618166135881104, "accuracy_n": 1902, "auc": 0.618166135881104}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5119178023266399, "accuracy_n": 2000, "auc": 0.5119178023266399}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5504201680672269, "accuracy_n": 59, "auc": 0.5504201680672269}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6307692307692309, "accuracy_n": 23, "auc": 0.6307692307692309}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5170340681362725, "accuracy_n": 998, "auc": 0.5170340681362725}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5262096774193549, "accuracy_n": 992, "auc": 0.5262096774193549}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.29116465863453816, "accuracy_n": 498, "auc": 0.29116465863453816}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.285140562248996, "accuracy_n": 498, "auc": 0.285140562248996}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26936026936026936, "accuracy_n": 297, "auc": 0.26936026936026936}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5327463700957678, "accuracy_n": 322, "auc": 0.5327463700957678}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5444078947368421, "accuracy_n": 292, "auc": 0.5444078947368421}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6208289101203115, "accuracy_n": 1902, "auc": 0.6208289101203115}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5101601678205833, "accuracy_n": 2000, "auc": 0.5101601678205833}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5406162464985995, "accuracy_n": 59, "auc": 0.5406162464985995}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5020161290322581, "accuracy_n": 992, "auc": 0.5020161290322581}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.27710843373493976, "accuracy_n": 498, "auc": 0.27710843373493976}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26104417670682734, "accuracy_n": 498, "auc": 0.26104417670682734}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26936026936026936, "accuracy_n": 297, "auc": 0.26936026936026936}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.19799498746867167, "accuracy_n": 399, "auc": 0.19799498746867167}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5061013283904849, "accuracy_n": 322, "auc": 0.5061013283904849}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6158364661654135, "accuracy_n": 292, "auc": 0.6158364661654135}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6431329617834395, "accuracy_n": 1902, "auc": 0.6431329617834395}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5051623636132644, "accuracy_n": 2000, "auc": 0.5051623636132644}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5406162464985994, "accuracy_n": 59, "auc": 0.5406162464985994}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6118951612903226, "accuracy_n": 992, "auc": 0.6118951612903226}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.43373493975903615, "accuracy_n": 498, "auc": 0.43373493975903615}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3373493975903614, "accuracy_n": 498, "auc": 0.3373493975903614}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.32323232323232326, "accuracy_n": 297, "auc": 0.32323232323232326}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5819431572443621, "accuracy_n": 322, "auc": 0.5819431572443621}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5607612781954887, "accuracy_n": 292, "auc": 0.5607612781954887}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5641646489104116, "accuracy_n": 413, "auc": 0.5641646489104116}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5094291843595187, "accuracy_n": 1902, "auc": 0.5094291843595187}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5241392142563465, "accuracy_n": 2000, "auc": 0.5241392142563465}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5406162464985994, "accuracy_n": 59, "auc": 0.5406162464985994}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.49899193548387094, "accuracy_n": 992, "auc": 0.49899193548387094}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.41566265060240964, "accuracy_n": 498, "auc": 0.41566265060240964}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.40160642570281124, "accuracy_n": 498, "auc": 0.40160642570281124}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2828282828282828, "accuracy_n": 297, "auc": 0.2828282828282828}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2982456140350877, "accuracy_n": 399, "auc": 0.2982456140350877}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5553367315415508, "accuracy_n": 322, "auc": 0.5553367315415508}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5226503759398495, "accuracy_n": 292, "auc": 0.5226503759398495}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6186338906581741, "accuracy_n": 1902, "auc": 0.6186338906581741}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5324242051380548, "accuracy_n": 2000, "auc": 0.5324242051380548}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.511204481792717, "accuracy_n": 59, "auc": 0.511204481792717}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5493951612903226, "accuracy_n": 992, "auc": 0.5493951612903226}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 994, "auc": 0.5392354124748491}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3514056224899598, "accuracy_n": 498, "auc": 0.3514056224899598}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.31313131313131315, "accuracy_n": 297, "auc": 0.31313131313131315}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24060150375939848, "accuracy_n": 399, "auc": 0.24060150375939848}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5924853259190608, "accuracy_n": 322, "auc": 0.5924853259190608}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5262218045112782, "accuracy_n": 292, "auc": 0.5262218045112782}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6618144019815995, "accuracy_n": 1902, "auc": 0.6618144019815995}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5313198064501286, "accuracy_n": 2000, "auc": 0.5313198064501286}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5504201680672269, "accuracy_n": 59, "auc": 0.5504201680672269}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5504032258064516, "accuracy_n": 992, "auc": 0.5504032258064516}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5462776659959758, "accuracy_n": 994, "auc": 0.5462776659959758}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28313253012048195, "accuracy_n": 498, "auc": 0.28313253012048195}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26907630522088355, "accuracy_n": 498, "auc": 0.26907630522088355}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26262626262626265, "accuracy_n": 297, "auc": 0.26262626262626265}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2631578947368421, "accuracy_n": 399, "auc": 0.2631578947368421}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6105610561056105, "accuracy_n": 303, "auc": 0.6105610561056105}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5470342910101946, "accuracy_n": 322, "auc": 0.5470342910101946}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5022086466165414, "accuracy_n": 292, "auc": 0.5022086466165414}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5508017073602265, "accuracy_n": 1902, "auc": 0.5508017073602265}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5063617966085757, "accuracy_n": 2000, "auc": 0.5063617966085757}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5546218487394958, "accuracy_n": 59, "auc": 0.5546218487394958}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6280241935483871, "accuracy_n": 992, "auc": 0.6280241935483871}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26907630522088355, "accuracy_n": 498, "auc": 0.26907630522088355}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26506024096385544, "accuracy_n": 498, "auc": 0.26506024096385544}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2828282828282828, "accuracy_n": 297, "auc": 0.2828282828282828}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21804511278195488, "accuracy_n": 399, "auc": 0.21804511278195488}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.8382838283828383, "accuracy_n": 303, "auc": 0.8382838283828383}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5452193388940376, "accuracy_n": 322, "auc": 0.5452193388940376}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5303101503759398, "accuracy_n": 292, "auc": 0.5303101503759398}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6020977087756547, "accuracy_n": 1902, "auc": 0.6020977087756547}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5319920491297359, "accuracy_n": 2000, "auc": 0.5319920491297359}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5084033613445378, "accuracy_n": 59, "auc": 0.5084033613445378}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6633064516129032, "accuracy_n": 992, "auc": 0.6633064516129032}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26104417670682734, "accuracy_n": 498, "auc": 0.26104417670682734}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24096385542168675, "accuracy_n": 498, "auc": 0.24096385542168675}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26936026936026936, "accuracy_n": 297, "auc": 0.26936026936026936}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21303258145363407, "accuracy_n": 399, "auc": 0.21303258145363407}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6387859128822985, "accuracy_n": 322, "auc": 0.6387859128822985}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5302161654135339, "accuracy_n": 292, "auc": 0.5302161654135339}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5690072639225182, "accuracy_n": 413, "auc": 0.5690072639225182}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.650800601556971, "accuracy_n": 1902, "auc": 0.650800601556971}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5142846567610907, "accuracy_n": 2000, "auc": 0.5142846567610907}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5392156862745099, "accuracy_n": 59, "auc": 0.5392156862745099}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5591182364729459, "accuracy_n": 998, "auc": 0.5591182364729459}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5050403225806451, "accuracy_n": 992, "auc": 0.5050403225806451}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24497991967871485, "accuracy_n": 498, "auc": 0.24497991967871485}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.22895622895622897, "accuracy_n": 297, "auc": 0.22895622895622897}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23558897243107768, "accuracy_n": 399, "auc": 0.23558897243107768}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6961692925548348, "accuracy_n": 322, "auc": 0.6961692925548348}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6742011278195489, "accuracy_n": 292, "auc": 0.6742011278195489}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.565878228945506, "accuracy_n": 1902, "auc": 0.565878228945506}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5742142913591807, "accuracy_n": 2000, "auc": 0.5742142913591807}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5644257703081232, "accuracy_n": 59, "auc": 0.5644257703081232}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5661322645290581, "accuracy_n": 998, "auc": 0.5661322645290581}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6481854838709677, "accuracy_n": 992, "auc": 0.6481854838709677}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26305220883534136, "accuracy_n": 498, "auc": 0.26305220883534136}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24899598393574296, "accuracy_n": 498, "auc": 0.24899598393574296}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2558922558922559, "accuracy_n": 297, "auc": 0.2558922558922559}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20802005012531327, "accuracy_n": 399, "auc": 0.20802005012531327}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6105610561056105, "accuracy_n": 303, "auc": 0.6105610561056105}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7048192771084337, "accuracy_n": 322, "auc": 0.7048192771084337}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6743890977443608, "accuracy_n": 292, "auc": 0.6743890977443608}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6319612590799032, "accuracy_n": 413, "auc": 0.6319612590799032}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5942442940552016, "accuracy_n": 1902, "auc": 0.5942442940552016}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.601883279864031, "accuracy_n": 2000, "auc": 0.601883279864031}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5084033613445378, "accuracy_n": 59, "auc": 0.5084033613445378}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5977822580645161, "accuracy_n": 992, "auc": 0.5977822580645161}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3232931726907631, "accuracy_n": 498, "auc": 0.3232931726907631}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26104417670682734, "accuracy_n": 498, "auc": 0.26104417670682734}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2727272727272727, "accuracy_n": 297, "auc": 0.2727272727272727}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.19298245614035087, "accuracy_n": 399, "auc": 0.19298245614035087}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.504286376274328, "accuracy_n": 322, "auc": 0.504286376274328}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.531296992481203, "accuracy_n": 292, "auc": 0.531296992481203}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5417639773531493, "accuracy_n": 1902, "auc": 0.5417639773531493}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5357714134802665, "accuracy_n": 2000, "auc": 0.5357714134802665}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6050420168067226, "accuracy_n": 59, "auc": 0.6050420168067226}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5615384615384617, "accuracy_n": 23, "auc": 0.5615384615384617}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5584677419354839, "accuracy_n": 992, "auc": 0.5584677419354839}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2971887550200803, "accuracy_n": 498, "auc": 0.2971887550200803}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.265993265993266, "accuracy_n": 297, "auc": 0.265993265993266}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5141720729070127, "accuracy_n": 322, "auc": 0.5141720729070127}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.575187969924812, "accuracy_n": 292, "auc": 0.575187969924812}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.931784102972399, "accuracy_n": 1902, "auc": 0.931784102972399}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5164414353581643, "accuracy_n": 2000, "auc": 0.5164414353581643}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5406162464985994, "accuracy_n": 59, "auc": 0.5406162464985994}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.561122244488978, "accuracy_n": 998, "auc": 0.561122244488978}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.9002016129032258, "accuracy_n": 992, "auc": 0.9002016129032258}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.26907630522088355, "accuracy_n": 498, "auc": 0.26907630522088355}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24242424242424243, "accuracy_n": 297, "auc": 0.24242424242424243}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7006873648439913, "accuracy_n": 322, "auc": 0.7006873648439913}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6765977443609024, "accuracy_n": 292, "auc": 0.6765977443609024}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6687787508846426, "accuracy_n": 1902, "auc": 0.6687787508846426}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6857905703959128, "accuracy_n": 2000, "auc": 0.6857905703959128}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5420168067226891, "accuracy_n": 59, "auc": 0.5420168067226891}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5961923847695391, "accuracy_n": 998, "auc": 0.5961923847695391}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6209677419354839, "accuracy_n": 992, "auc": 0.6209677419354839}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.2971887550200803, "accuracy_n": 498, "auc": 0.2971887550200803}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.3433734939759036, "accuracy_n": 498, "auc": 0.3433734939759036}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.24915824915824916, "accuracy_n": 297, "auc": 0.24915824915824916}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6501650165016502, "accuracy_n": 303, "auc": 0.6501650165016502}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6641566265060241, "accuracy_n": 322, "auc": 0.6641566265060241}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5052631578947369, "accuracy_n": 292, "auc": 0.5052631578947369}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.767584483368719, "accuracy_n": 1902, "auc": 0.767584483368719}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5339567583897787, "accuracy_n": 2000, "auc": 0.5339567583897787}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6316526610644257, "accuracy_n": 59, "auc": 0.6316526610644257}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5262096774193549, "accuracy_n": 992, "auc": 0.5262096774193549}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5144927536231884, "accuracy_n": 276, "auc": 0.5144927536231884}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.28112449799196787, "accuracy_n": 498, "auc": 0.28112449799196787}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.25100401606425704, "accuracy_n": 498, "auc": 0.25100401606425704}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.265993265993266, "accuracy_n": 297, "auc": 0.265993265993266}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.21804511278195488, "accuracy_n": 399, "auc": 0.21804511278195488}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.535333642261353, "accuracy_n": 322, "auc": 0.535333642261353}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5358082706766917, "accuracy_n": 292, "auc": 0.5358082706766917}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5302663438256658, "accuracy_n": 413, "auc": 0.5302663438256658}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6003472222222223, "accuracy_n": 1902, "auc": 0.6003472222222223}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.5034227356075544, "accuracy_n": 2000, "auc": 0.5034227356075544}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.6162464985994398, "accuracy_n": 59, "auc": 0.6162464985994398}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h13", "token_idx": -1, "accuracy": 0.7230769230769232, "accuracy_n": 23, "auc": 0.7230769230769232}}
