{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9428857715430862, "accuracy_n": 998, "auc": 0.9428857715430862}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.936491935483871, "accuracy_n": 992, "auc": 0.936491935483871}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6116700201207244, "accuracy_n": 994, "auc": 0.6116700201207244}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3253012048192771, "accuracy_n": 498, "auc": 0.3253012048192771}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5863453815261044, "accuracy_n": 498, "auc": 0.5863453815261044}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.43434343434343436, "accuracy_n": 297, "auc": 0.43434343434343436}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6140350877192983, "accuracy_n": 399, "auc": 0.6140350877192983}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8679867986798679, "accuracy_n": 303, "auc": 0.8679867986798679}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8384306456595614, "accuracy_n": 322, "auc": 0.8384306456595614}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5576597744360903, "accuracy_n": 292, "auc": 0.5576597744360903}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5128892427459306, "accuracy_n": 1902, "auc": 0.5128892427459306}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.564914434110714, "accuracy_n": 2000, "auc": 0.564914434110714}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7184873949579832, "accuracy_n": 59, "auc": 0.7184873949579832}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9378757515030061, "accuracy_n": 998, "auc": 0.9378757515030061}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9405241935483871, "accuracy_n": 992, "auc": 0.9405241935483871}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8297101449275363, "accuracy_n": 276, "auc": 0.8297101449275363}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.528169014084507, "accuracy_n": 994, "auc": 0.528169014084507}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2710843373493976, "accuracy_n": 498, "auc": 0.2710843373493976}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.572289156626506, "accuracy_n": 498, "auc": 0.572289156626506}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2760942760942761, "accuracy_n": 297, "auc": 0.2760942760942761}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3433583959899749, "accuracy_n": 399, "auc": 0.3433583959899749}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6831683168316832, "accuracy_n": 303, "auc": 0.6831683168316832}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8010117392647514, "accuracy_n": 322, "auc": 0.8010117392647514}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5801691729323308, "accuracy_n": 292, "auc": 0.5801691729323308}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9031476997578692, "accuracy_n": 413, "auc": 0.9031476997578692}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5213386854210899, "accuracy_n": 1902, "auc": 0.5213386854210899}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5365852072598208, "accuracy_n": 2000, "auc": 0.5365852072598208}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5392156862745098, "accuracy_n": 59, "auc": 0.5392156862745098}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9408817635270541, "accuracy_n": 998, "auc": 0.9408817635270541}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9334677419354839, "accuracy_n": 992, "auc": 0.9334677419354839}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8586956521739131, "accuracy_n": 276, "auc": 0.8586956521739131}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5845070422535211, "accuracy_n": 994, "auc": 0.5845070422535211}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5281124497991968, "accuracy_n": 498, "auc": 0.5281124497991968}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3265993265993266, "accuracy_n": 297, "auc": 0.3265993265993266}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.45112781954887216, "accuracy_n": 399, "auc": 0.45112781954887216}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5973597359735974, "accuracy_n": 303, "auc": 0.5973597359735974}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9031510658016682, "accuracy_n": 322, "auc": 0.9031510658016682}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5537124060150376, "accuracy_n": 292, "auc": 0.5537124060150376}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6876513317191283, "accuracy_n": 413, "auc": 0.6876513317191283}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5498485049539986, "accuracy_n": 1902, "auc": 0.5498485049539986}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5305690354217872, "accuracy_n": 2000, "auc": 0.5305690354217872}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5770308123249299, "accuracy_n": 59, "auc": 0.5770308123249299}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9228456913827655, "accuracy_n": 998, "auc": 0.9228456913827655}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7671370967741935, "accuracy_n": 992, "auc": 0.7671370967741935}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8442028985507246, "accuracy_n": 276, "auc": 0.8442028985507246}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 994, "auc": 0.5392354124748491}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.25100401606425704, "accuracy_n": 498, "auc": 0.25100401606425704}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3353413654618474, "accuracy_n": 498, "auc": 0.3353413654618474}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5151515151515151, "accuracy_n": 297, "auc": 0.5151515151515151}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.42105263157894735, "accuracy_n": 399, "auc": 0.42105263157894735}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7425742574257426, "accuracy_n": 303, "auc": 0.7425742574257426}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7876506024096386, "accuracy_n": 322, "auc": 0.7876506024096386}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5476973684210527, "accuracy_n": 292, "auc": 0.5476973684210527}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6295399515738499, "accuracy_n": 413, "auc": 0.6295399515738499}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.508722576079264, "accuracy_n": 1902, "auc": 0.508722576079264}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5188332988208744, "accuracy_n": 2000, "auc": 0.5188332988208744}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6834733893557423, "accuracy_n": 59, "auc": 0.6834733893557423}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7328629032258065, "accuracy_n": 992, "auc": 0.7328629032258065}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28714859437751006, "accuracy_n": 498, "auc": 0.28714859437751006}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.39959839357429716, "accuracy_n": 498, "auc": 0.39959839357429716}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4175084175084175, "accuracy_n": 297, "auc": 0.4175084175084175}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.49874686716791977, "accuracy_n": 399, "auc": 0.49874686716791977}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7491749174917491, "accuracy_n": 303, "auc": 0.7491749174917491}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6614148903305529, "accuracy_n": 322, "auc": 0.6614148903305529}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6026315789473684, "accuracy_n": 292, "auc": 0.6026315789473684}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9055690072639225, "accuracy_n": 413, "auc": 0.9055690072639225}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5780254777070064, "accuracy_n": 1902, "auc": 0.5780254777070064}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5471645263940282, "accuracy_n": 2000, "auc": 0.5471645263940282}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7240896358543418, "accuracy_n": 59, "auc": 0.7240896358543418}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8957915831663327, "accuracy_n": 998, "auc": 0.8957915831663327}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5826612903225806, "accuracy_n": 992, "auc": 0.5826612903225806}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5543478260869565, "accuracy_n": 276, "auc": 0.5543478260869565}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5694164989939637, "accuracy_n": 994, "auc": 0.5694164989939637}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6626506024096386, "accuracy_n": 498, "auc": 0.6626506024096386}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7188755020080321, "accuracy_n": 498, "auc": 0.7188755020080321}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8114478114478114, "accuracy_n": 297, "auc": 0.8114478114478114}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8045112781954887, "accuracy_n": 399, "auc": 0.8045112781954887}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7326732673267327, "accuracy_n": 303, "auc": 0.7326732673267327}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5124343527957985, "accuracy_n": 322, "auc": 0.5124343527957985}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6106672932330827, "accuracy_n": 292, "auc": 0.6106672932330827}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5339083510261855, "accuracy_n": 1902, "auc": 0.5339083510261855}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5569350535543331, "accuracy_n": 2000, "auc": 0.5569350535543331}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.757703081232493, "accuracy_n": 59, "auc": 0.757703081232493}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5923076923076922, "accuracy_n": 23, "auc": 0.5923076923076922}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9438877755511023, "accuracy_n": 998, "auc": 0.9438877755511023}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9012096774193549, "accuracy_n": 992, "auc": 0.9012096774193549}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6702898550724637, "accuracy_n": 276, "auc": 0.6702898550724637}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5563380281690141, "accuracy_n": 994, "auc": 0.5563380281690141}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6526104417670683, "accuracy_n": 498, "auc": 0.6526104417670683}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7389558232931727, "accuracy_n": 498, "auc": 0.7389558232931727}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8148148148148148, "accuracy_n": 297, "auc": 0.8148148148148148}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8070175438596491, "accuracy_n": 399, "auc": 0.8070175438596491}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8250825082508251, "accuracy_n": 303, "auc": 0.8250825082508251}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7369477911646587, "accuracy_n": 322, "auc": 0.7369477911646587}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6990131578947368, "accuracy_n": 292, "auc": 0.6990131578947368}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5543015746638358, "accuracy_n": 1902, "auc": 0.5543015746638358}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5890366422278444, "accuracy_n": 2000, "auc": 0.5890366422278444}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 59, "auc": 0.6666666666666666}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6783567134268537, "accuracy_n": 998, "auc": 0.6783567134268537}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5090725806451613, "accuracy_n": 992, "auc": 0.5090725806451613}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5398550724637681, "accuracy_n": 276, "auc": 0.5398550724637681}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5382293762575453, "accuracy_n": 994, "auc": 0.5382293762575453}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4738955823293173, "accuracy_n": 498, "auc": 0.4738955823293173}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6987951807228916, "accuracy_n": 498, "auc": 0.6987951807228916}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8047138047138047, "accuracy_n": 297, "auc": 0.8047138047138047}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7969924812030075, "accuracy_n": 399, "auc": 0.7969924812030075}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8085808580858086, "accuracy_n": 303, "auc": 0.8085808580858086}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6768226753166512, "accuracy_n": 322, "auc": 0.6768226753166512}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5708646616541353, "accuracy_n": 292, "auc": 0.5708646616541353}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9733656174334141, "accuracy_n": 413, "auc": 0.9733656174334141}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5498872080679406, "accuracy_n": 1902, "auc": 0.5498872080679406}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5391661389761704, "accuracy_n": 2000, "auc": 0.5391661389761704}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7436974789915967, "accuracy_n": 59, "auc": 0.7436974789915967}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8707414829659319, "accuracy_n": 998, "auc": 0.8707414829659319}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5625, "accuracy_n": 992, "auc": 0.5625}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5398550724637681, "accuracy_n": 276, "auc": 0.5398550724637681}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5845070422535211, "accuracy_n": 994, "auc": 0.5845070422535211}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5240963855421686, "accuracy_n": 498, "auc": 0.5240963855421686}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7048192771084337, "accuracy_n": 498, "auc": 0.7048192771084337}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8080808080808081, "accuracy_n": 297, "auc": 0.8080808080808081}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8045112781954887, "accuracy_n": 399, "auc": 0.8045112781954887}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8052805280528053, "accuracy_n": 303, "auc": 0.8052805280528053}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.621447327772629, "accuracy_n": 322, "auc": 0.621447327772629}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5765977443609023, "accuracy_n": 292, "auc": 0.5765977443609023}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5515923566878981, "accuracy_n": 1902, "auc": 0.5515923566878981}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5428959854507477, "accuracy_n": 2000, "auc": 0.5428959854507477}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7170868347338936, "accuracy_n": 59, "auc": 0.7170868347338936}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8046092184368737, "accuracy_n": 998, "auc": 0.8046092184368737}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6340725806451613, "accuracy_n": 992, "auc": 0.6340725806451613}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2931726907630522, "accuracy_n": 498, "auc": 0.2931726907630522}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4678714859437751, "accuracy_n": 498, "auc": 0.4678714859437751}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6026936026936027, "accuracy_n": 297, "auc": 0.6026936026936027}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5413533834586466, "accuracy_n": 399, "auc": 0.5413533834586466}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5491195551436515, "accuracy_n": 322, "auc": 0.5491195551436515}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5525845864661654, "accuracy_n": 292, "auc": 0.5525845864661654}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.551968329794763, "accuracy_n": 1902, "auc": 0.551968329794763}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5637995316309188, "accuracy_n": 2000, "auc": 0.5637995316309188}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8375350140056023, "accuracy_n": 59, "auc": 0.8375350140056023}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8466933867735471, "accuracy_n": 998, "auc": 0.8466933867735471}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6481854838709677, "accuracy_n": 992, "auc": 0.6481854838709677}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5833333333333334, "accuracy_n": 276, "auc": 0.5833333333333334}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3253012048192771, "accuracy_n": 498, "auc": 0.3253012048192771}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.45180722891566266, "accuracy_n": 498, "auc": 0.45180722891566266}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6464646464646465, "accuracy_n": 297, "auc": 0.6464646464646465}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7117794486215538, "accuracy_n": 399, "auc": 0.7117794486215538}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9471947194719472, "accuracy_n": 303, "auc": 0.9471947194719472}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7083333333333333, "accuracy_n": 322, "auc": 0.7083333333333333}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5703477443609023, "accuracy_n": 292, "auc": 0.5703477443609023}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5476081475583864, "accuracy_n": 1902, "auc": 0.5476081475583864}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6095850602067346, "accuracy_n": 2000, "auc": 0.6095850602067346}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8123249299719888, "accuracy_n": 59, "auc": 0.8123249299719888}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8797595190380761, "accuracy_n": 998, "auc": 0.8797595190380761}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6108870967741935, "accuracy_n": 992, "auc": 0.6108870967741935}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7318840579710145, "accuracy_n": 276, "auc": 0.7318840579710145}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5603621730382293, "accuracy_n": 994, "auc": 0.5603621730382293}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2710843373493976, "accuracy_n": 498, "auc": 0.2710843373493976}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3514056224899598, "accuracy_n": 498, "auc": 0.3514056224899598}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4444444444444444, "accuracy_n": 297, "auc": 0.4444444444444444}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.23809523809523808, "accuracy_n": 399, "auc": 0.23809523809523808}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6567656765676567, "accuracy_n": 303, "auc": 0.6567656765676567}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9251621872103799, "accuracy_n": 322, "auc": 0.9251621872103799}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5161654135338346, "accuracy_n": 292, "auc": 0.5161654135338346}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6634382566585957, "accuracy_n": 413, "auc": 0.6634382566585957}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5512650389242746, "accuracy_n": 1902, "auc": 0.5512650389242746}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5245298552777553, "accuracy_n": 2000, "auc": 0.5245298552777553}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8987975951903807, "accuracy_n": 998, "auc": 0.8987975951903807}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7237903225806451, "accuracy_n": 992, "auc": 0.7237903225806451}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6992753623188406, "accuracy_n": 276, "auc": 0.6992753623188406}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6448692152917505, "accuracy_n": 994, "auc": 0.6448692152917505}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3192771084337349, "accuracy_n": 498, "auc": 0.3192771084337349}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4899598393574297, "accuracy_n": 498, "auc": 0.4899598393574297}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6397306397306397, "accuracy_n": 297, "auc": 0.6397306397306397}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6691729323308271, "accuracy_n": 399, "auc": 0.6691729323308271}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7986798679867987, "accuracy_n": 303, "auc": 0.7986798679867987}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5309314179796107, "accuracy_n": 322, "auc": 0.5309314179796107}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.793468045112782, "accuracy_n": 292, "auc": 0.793468045112782}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8038740920096852, "accuracy_n": 413, "auc": 0.8038740920096852}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5211960368011324, "accuracy_n": 1902, "auc": 0.5211960368011324}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.509365881083071, "accuracy_n": 2000, "auc": 0.509365881083071}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5336134453781513, "accuracy_n": 59, "auc": 0.5336134453781513}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6692307692307692, "accuracy_n": 23, "auc": 0.6692307692307692}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9238476953907816, "accuracy_n": 998, "auc": 0.9238476953907816}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7600806451612904, "accuracy_n": 992, "auc": 0.7600806451612904}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5688405797101449, "accuracy_n": 276, "auc": 0.5688405797101449}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5803212851405622, "accuracy_n": 498, "auc": 0.5803212851405622}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7306397306397306, "accuracy_n": 297, "auc": 0.7306397306397306}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7518796992481203, "accuracy_n": 399, "auc": 0.7518796992481203}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9141914191419142, "accuracy_n": 303, "auc": 0.9141914191419142}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5922536299042322, "accuracy_n": 322, "auc": 0.5922536299042322}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5657894736842104, "accuracy_n": 292, "auc": 0.5657894736842104}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5475417993630574, "accuracy_n": 1902, "auc": 0.5475417993630574}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5545651980364912, "accuracy_n": 2000, "auc": 0.5545651980364912}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8893557422969187, "accuracy_n": 59, "auc": 0.8893557422969187}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5761523046092184, "accuracy_n": 998, "auc": 0.5761523046092184}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6784274193548387, "accuracy_n": 992, "auc": 0.6784274193548387}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6521739130434783, "accuracy_n": 276, "auc": 0.6521739130434783}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6569416498993964, "accuracy_n": 994, "auc": 0.6569416498993964}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2429718875502008, "accuracy_n": 498, "auc": 0.2429718875502008}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.38755020080321284, "accuracy_n": 498, "auc": 0.38755020080321284}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.531986531986532, "accuracy_n": 297, "auc": 0.531986531986532}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.43358395989974935, "accuracy_n": 399, "auc": 0.43358395989974935}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7953795379537953, "accuracy_n": 303, "auc": 0.7953795379537953}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7899289465554526, "accuracy_n": 322, "auc": 0.7899289465554526}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5415883458646616, "accuracy_n": 292, "auc": 0.5415883458646616}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7869249394673123, "accuracy_n": 413, "auc": 0.7869249394673123}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7545592268223638, "accuracy_n": 1902, "auc": 0.7545592268223638}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5343158880355808, "accuracy_n": 2000, "auc": 0.5343158880355808}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7296918767507002, "accuracy_n": 59, "auc": 0.7296918767507002}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7014028056112225, "accuracy_n": 998, "auc": 0.7014028056112225}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5423387096774194, "accuracy_n": 992, "auc": 0.5423387096774194}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.605072463768116, "accuracy_n": 276, "auc": 0.605072463768116}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.613682092555332, "accuracy_n": 994, "auc": 0.613682092555332}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2791164658634538, "accuracy_n": 498, "auc": 0.2791164658634538}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.44779116465863456, "accuracy_n": 498, "auc": 0.44779116465863456}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5791245791245792, "accuracy_n": 297, "auc": 0.5791245791245792}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5413533834586466, "accuracy_n": 399, "auc": 0.5413533834586466}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.900990099009901, "accuracy_n": 303, "auc": 0.900990099009901}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5512820512820513, "accuracy_n": 322, "auc": 0.5512820512820513}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5322838345864661, "accuracy_n": 292, "auc": 0.5322838345864661}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5660385704175512, "accuracy_n": 1902, "auc": 0.5660385704175512}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6157672919924092, "accuracy_n": 2000, "auc": 0.6157672919924092}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8585434173669467, "accuracy_n": 59, "auc": 0.8585434173669467}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7925851703406813, "accuracy_n": 998, "auc": 0.7925851703406813}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6129032258064516, "accuracy_n": 992, "auc": 0.6129032258064516}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6014492753623188, "accuracy_n": 276, "auc": 0.6014492753623188}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28714859437751006, "accuracy_n": 498, "auc": 0.28714859437751006}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4036144578313253, "accuracy_n": 498, "auc": 0.4036144578313253}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5824915824915825, "accuracy_n": 297, "auc": 0.5824915824915825}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.49874686716791977, "accuracy_n": 399, "auc": 0.49874686716791977}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9108910891089109, "accuracy_n": 303, "auc": 0.9108910891089109}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5074915044794563, "accuracy_n": 322, "auc": 0.5074915044794563}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5249999999999999, "accuracy_n": 292, "auc": 0.5249999999999999}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5573093595187544, "accuracy_n": 1902, "auc": 0.5573093595187544}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5723126048503511, "accuracy_n": 2000, "auc": 0.5723126048503511}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8683473389355741, "accuracy_n": 59, "auc": 0.8683473389355741}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8987975951903807, "accuracy_n": 998, "auc": 0.8987975951903807}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.844758064516129, "accuracy_n": 992, "auc": 0.844758064516129}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7862318840579711, "accuracy_n": 276, "auc": 0.7862318840579711}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5985915492957746, "accuracy_n": 994, "auc": 0.5985915492957746}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.27710843373493976, "accuracy_n": 498, "auc": 0.27710843373493976}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.2710843373493976, "accuracy_n": 498, "auc": 0.2710843373493976}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3771043771043771, "accuracy_n": 297, "auc": 0.3771043771043771}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.40852130325814534, "accuracy_n": 399, "auc": 0.40852130325814534}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7623762376237624, "accuracy_n": 303, "auc": 0.7623762376237624}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7049351251158481, "accuracy_n": 322, "auc": 0.7049351251158481}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5220864661654135, "accuracy_n": 292, "auc": 0.5220864661654135}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6682808716707022, "accuracy_n": 413, "auc": 0.6682808716707022}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5887650389242747, "accuracy_n": 1902, "auc": 0.5887650389242747}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5000085030696082, "accuracy_n": 2000, "auc": 0.5000085030696082}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5518207282913165, "accuracy_n": 59, "auc": 0.5518207282913165}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
