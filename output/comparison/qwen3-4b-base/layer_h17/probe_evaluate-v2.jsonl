{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8657314629258517, "accuracy_n": 998, "auc": 0.8657314629258517}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9274193548387096, "accuracy_n": 992, "auc": 0.9274193548387096}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.641851106639839, "accuracy_n": 994, "auc": 0.641851106639839}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3393574297188755, "accuracy_n": 498, "auc": 0.3393574297188755}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.23694779116465864, "accuracy_n": 498, "auc": 0.23694779116465864}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2828282828282828, "accuracy_n": 297, "auc": 0.2828282828282828}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2731829573934837, "accuracy_n": 399, "auc": 0.2731829573934837}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8778877887788779, "accuracy_n": 303, "auc": 0.8778877887788779}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8468875502008031, "accuracy_n": 322, "auc": 0.8468875502008031}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5754699248120301, "accuracy_n": 292, "auc": 0.5754699248120301}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6687732218683652, "accuracy_n": 1902, "auc": 0.6687732218683652}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7229679914449116, "accuracy_n": 2000, "auc": 0.7229679914449116}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9215686274509804, "accuracy_n": 59, "auc": 0.9215686274509804}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9228456913827655, "accuracy_n": 998, "auc": 0.9228456913827655}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9475806451612904, "accuracy_n": 992, "auc": 0.9475806451612904}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6861167002012073, "accuracy_n": 994, "auc": 0.6861167002012073}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3232931726907631, "accuracy_n": 498, "auc": 0.3232931726907631}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.24899598393574296, "accuracy_n": 498, "auc": 0.24899598393574296}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30303030303030304, "accuracy_n": 297, "auc": 0.30303030303030304}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2706766917293233, "accuracy_n": 399, "auc": 0.2706766917293233}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.905583873957368, "accuracy_n": 322, "auc": 0.905583873957368}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5645676691729323, "accuracy_n": 292, "auc": 0.5645676691729323}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8687544232130219, "accuracy_n": 1902, "auc": 0.8687544232130219}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7467945928480182, "accuracy_n": 2000, "auc": 0.7467945928480182}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8571428571428571, "accuracy_n": 59, "auc": 0.8571428571428571}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5170340681362725, "accuracy_n": 998, "auc": 0.5170340681362725}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9536290322580645, "accuracy_n": 992, "auc": 0.9536290322580645}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5833333333333334, "accuracy_n": 276, "auc": 0.5833333333333334}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6589537223340041, "accuracy_n": 994, "auc": 0.6589537223340041}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3273092369477912, "accuracy_n": 498, "auc": 0.3273092369477912}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2971887550200803, "accuracy_n": 498, "auc": 0.2971887550200803}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 297, "auc": 0.3333333333333333}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2781954887218045, "accuracy_n": 399, "auc": 0.2781954887218045}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6788307074451654, "accuracy_n": 322, "auc": 0.6788307074451654}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7258458646616541, "accuracy_n": 292, "auc": 0.7258458646616541}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8389828821656051, "accuracy_n": 1902, "auc": 0.8389828821656051}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.734482148055448, "accuracy_n": 2000, "auc": 0.734482148055448}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8543417366946778, "accuracy_n": 59, "auc": 0.8543417366946778}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7254509018036072, "accuracy_n": 998, "auc": 0.7254509018036072}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9203629032258065, "accuracy_n": 992, "auc": 0.9203629032258065}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5110663983903421, "accuracy_n": 994, "auc": 0.5110663983903421}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27309236947791166, "accuracy_n": 498, "auc": 0.27309236947791166}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.30976430976430974, "accuracy_n": 297, "auc": 0.30976430976430974}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3308270676691729, "accuracy_n": 399, "auc": 0.3308270676691729}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9537953795379538, "accuracy_n": 303, "auc": 0.9537953795379538}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8234476367006488, "accuracy_n": 322, "auc": 0.8234476367006488}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5015977443609023, "accuracy_n": 292, "auc": 0.5015977443609023}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6996107572540694, "accuracy_n": 1902, "auc": 0.6996107572540694}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7113527983602079, "accuracy_n": 2000, "auc": 0.7113527983602079}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8375350140056023, "accuracy_n": 59, "auc": 0.8375350140056023}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.53125, "accuracy_n": 992, "auc": 0.53125}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2791164658634538, "accuracy_n": 498, "auc": 0.2791164658634538}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.25502008032128515, "accuracy_n": 498, "auc": 0.25502008032128515}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2558922558922559, "accuracy_n": 297, "auc": 0.2558922558922559}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.19298245614035087, "accuracy_n": 399, "auc": 0.19298245614035087}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.758186592523942, "accuracy_n": 322, "auc": 0.758186592523942}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6614661654135338, "accuracy_n": 292, "auc": 0.6614661654135338}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8547215496368039, "accuracy_n": 413, "auc": 0.8547215496368039}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6736055820948337, "accuracy_n": 1902, "auc": 0.6736055820948337}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5777585708440747, "accuracy_n": 2000, "auc": 0.5777585708440747}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5840336134453781, "accuracy_n": 59, "auc": 0.5840336134453781}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5282258064516129, "accuracy_n": 992, "auc": 0.5282258064516129}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6338028169014085, "accuracy_n": 994, "auc": 0.6338028169014085}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5522088353413654, "accuracy_n": 498, "auc": 0.5522088353413654}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3815261044176707, "accuracy_n": 498, "auc": 0.3815261044176707}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4208754208754209, "accuracy_n": 297, "auc": 0.4208754208754209}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.48370927318295737, "accuracy_n": 399, "auc": 0.48370927318295737}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8217821782178217, "accuracy_n": 303, "auc": 0.8217821782178217}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7832483781278962, "accuracy_n": 322, "auc": 0.7832483781278962}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6343045112781954, "accuracy_n": 292, "auc": 0.6343045112781954}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5041677724699222, "accuracy_n": 1902, "auc": 0.5041677724699222}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5597350643582333, "accuracy_n": 2000, "auc": 0.5597350643582333}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7408963585434174, "accuracy_n": 59, "auc": 0.7408963585434174}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5230460921843687, "accuracy_n": 998, "auc": 0.5230460921843687}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5866935483870968, "accuracy_n": 992, "auc": 0.5866935483870968}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5181159420289855, "accuracy_n": 276, "auc": 0.5181159420289855}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4718875502008032, "accuracy_n": 498, "auc": 0.4718875502008032}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4899598393574297, "accuracy_n": 498, "auc": 0.4899598393574297}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4444444444444444, "accuracy_n": 297, "auc": 0.4444444444444444}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.531328320802005, "accuracy_n": 399, "auc": 0.531328320802005}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7748300895891258, "accuracy_n": 322, "auc": 0.7748300895891258}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7372180451127819, "accuracy_n": 292, "auc": 0.7372180451127819}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6137473460721868, "accuracy_n": 1902, "auc": 0.6137473460721868}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.594118476770114, "accuracy_n": 2000, "auc": 0.594118476770114}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5090180360721442, "accuracy_n": 998, "auc": 0.5090180360721442}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5020161290322581, "accuracy_n": 992, "auc": 0.5020161290322581}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5441767068273092, "accuracy_n": 498, "auc": 0.5441767068273092}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.45582329317269077, "accuracy_n": 498, "auc": 0.45582329317269077}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.47474747474747475, "accuracy_n": 297, "auc": 0.47474747474747475}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6090225563909775, "accuracy_n": 399, "auc": 0.6090225563909775}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5544554455445545, "accuracy_n": 303, "auc": 0.5544554455445545}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9199104108742663, "accuracy_n": 322, "auc": 0.9199104108742663}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.830075187969925, "accuracy_n": 292, "auc": 0.830075187969925}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5322507519462137, "accuracy_n": 1902, "auc": 0.5322507519462137}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6710852617795023, "accuracy_n": 2000, "auc": 0.6710852617795023}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7857142857142857, "accuracy_n": 59, "auc": 0.7857142857142857}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7264529058116233, "accuracy_n": 998, "auc": 0.7264529058116233}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6270161290322581, "accuracy_n": 992, "auc": 0.6270161290322581}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5724346076458753, "accuracy_n": 994, "auc": 0.5724346076458753}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.41967871485943775, "accuracy_n": 498, "auc": 0.41967871485943775}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.43373493975903615, "accuracy_n": 498, "auc": 0.43373493975903615}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.4006734006734007, "accuracy_n": 297, "auc": 0.4006734006734007}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5764411027568922, "accuracy_n": 399, "auc": 0.5764411027568922}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6672072907012665, "accuracy_n": 322, "auc": 0.6672072907012665}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5118421052631579, "accuracy_n": 292, "auc": 0.5118421052631579}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5331055378627034, "accuracy_n": 1902, "auc": 0.5331055378627034}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5535908462955127, "accuracy_n": 2000, "auc": 0.5535908462955127}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5294117647058824, "accuracy_n": 59, "auc": 0.5294117647058824}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7254509018036072, "accuracy_n": 998, "auc": 0.7254509018036072}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9405241935483871, "accuracy_n": 992, "auc": 0.9405241935483871}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3393574297188755, "accuracy_n": 498, "auc": 0.3393574297188755}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2570281124497992, "accuracy_n": 498, "auc": 0.2570281124497992}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.28619528619528617, "accuracy_n": 297, "auc": 0.28619528619528617}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.20802005012531327, "accuracy_n": 399, "auc": 0.20802005012531327}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9801980198019802, "accuracy_n": 303, "auc": 0.9801980198019802}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8890948409020698, "accuracy_n": 322, "auc": 0.8890948409020698}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6411654135338345, "accuracy_n": 292, "auc": 0.6411654135338345}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7134200283085633, "accuracy_n": 1902, "auc": 0.7134200283085633}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.74101050479223, "accuracy_n": 2000, "auc": 0.74101050479223}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8781512605042017, "accuracy_n": 59, "auc": 0.8781512605042017}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 998, "auc": 0.6192384769539078}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.936491935483871, "accuracy_n": 992, "auc": 0.936491935483871}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6217303822937625, "accuracy_n": 994, "auc": 0.6217303822937625}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.25903614457831325, "accuracy_n": 498, "auc": 0.25903614457831325}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.27309236947791166, "accuracy_n": 498, "auc": 0.27309236947791166}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 297, "auc": 0.3333333333333333}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3609022556390977, "accuracy_n": 399, "auc": 0.3609022556390977}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8631448872412728, "accuracy_n": 322, "auc": 0.8631448872412728}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7584116541353383, "accuracy_n": 292, "auc": 0.7584116541353383}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5986680599787686, "accuracy_n": 1902, "auc": 0.5986680599787686}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7286620469989666, "accuracy_n": 2000, "auc": 0.7286620469989666}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.861344537815126, "accuracy_n": 59, "auc": 0.861344537815126}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5200400801603207, "accuracy_n": 998, "auc": 0.5200400801603207}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9233870967741935, "accuracy_n": 992, "auc": 0.9233870967741935}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.42369477911646586, "accuracy_n": 498, "auc": 0.42369477911646586}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3795180722891566, "accuracy_n": 498, "auc": 0.3795180722891566}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3939393939393939, "accuracy_n": 297, "auc": 0.3939393939393939}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2531328320802005, "accuracy_n": 399, "auc": 0.2531328320802005}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9290624034599938, "accuracy_n": 322, "auc": 0.9290624034599938}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8704417293233082, "accuracy_n": 292, "auc": 0.8704417293233082}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6721370753715499, "accuracy_n": 1902, "auc": 0.6721370753715499}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.770380107218706, "accuracy_n": 2000, "auc": 0.770380107218706}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8025210084033614, "accuracy_n": 59, "auc": 0.8025210084033614}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.654308617234469, "accuracy_n": 998, "auc": 0.654308617234469}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6028225806451613, "accuracy_n": 992, "auc": 0.6028225806451613}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.367003367003367, "accuracy_n": 297, "auc": 0.367003367003367}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2631578947368421, "accuracy_n": 399, "auc": 0.2631578947368421}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8976897689768977, "accuracy_n": 303, "auc": 0.8976897689768977}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8519462465245597, "accuracy_n": 322, "auc": 0.8519462465245597}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8171052631578949, "accuracy_n": 292, "auc": 0.8171052631578949}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5502277954706298, "accuracy_n": 1902, "auc": 0.5502277954706298}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7008510072136043, "accuracy_n": 2000, "auc": 0.7008510072136043}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.627450980392157, "accuracy_n": 59, "auc": 0.627450980392157}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5160320641282565, "accuracy_n": 998, "auc": 0.5160320641282565}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9405241935483871, "accuracy_n": 992, "auc": 0.9405241935483871}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.40562248995983935, "accuracy_n": 498, "auc": 0.40562248995983935}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2570281124497992, "accuracy_n": 498, "auc": 0.2570281124497992}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.36363636363636365, "accuracy_n": 297, "auc": 0.36363636363636365}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.19548872180451127, "accuracy_n": 399, "auc": 0.19548872180451127}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8842678405931418, "accuracy_n": 322, "auc": 0.8842678405931418}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7426221804511279, "accuracy_n": 292, "auc": 0.7426221804511279}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7458454971691435, "accuracy_n": 1902, "auc": 0.7458454971691435}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7675810967759362, "accuracy_n": 2000, "auc": 0.7675810967759362}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8963585434173669, "accuracy_n": 59, "auc": 0.8963585434173669}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5480961923847696, "accuracy_n": 998, "auc": 0.5480961923847696}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6653225806451613, "accuracy_n": 992, "auc": 0.6653225806451613}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6014492753623188, "accuracy_n": 276, "auc": 0.6014492753623188}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2991967871485944, "accuracy_n": 498, "auc": 0.2991967871485944}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3253012048192771, "accuracy_n": 498, "auc": 0.3253012048192771}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2727272727272727, "accuracy_n": 297, "auc": 0.2727272727272727}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6022551745443312, "accuracy_n": 322, "auc": 0.6022551745443312}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5160714285714285, "accuracy_n": 292, "auc": 0.5160714285714285}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9636803874092009, "accuracy_n": 413, "auc": 0.9636803874092009}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.932037331917905, "accuracy_n": 1902, "auc": 0.932037331917905}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5223720763195514, "accuracy_n": 2000, "auc": 0.5223720763195514}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6470588235294117, "accuracy_n": 59, "auc": 0.6470588235294117}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9158316633266533, "accuracy_n": 998, "auc": 0.9158316633266533}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9506048387096774, "accuracy_n": 992, "auc": 0.9506048387096774}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.635814889336016, "accuracy_n": 994, "auc": 0.635814889336016}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3654618473895582, "accuracy_n": 498, "auc": 0.3654618473895582}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.28714859437751006, "accuracy_n": 498, "auc": 0.28714859437751006}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3367003367003367, "accuracy_n": 297, "auc": 0.3367003367003367}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9534291010194624, "accuracy_n": 322, "auc": 0.9534291010194624}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7925281954887218, "accuracy_n": 292, "auc": 0.7925281954887218}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6628792905166313, "accuracy_n": 1902, "auc": 0.6628792905166313}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8297275316389217, "accuracy_n": 2000, "auc": 0.8297275316389217}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8711484593837535, "accuracy_n": 59, "auc": 0.8711484593837535}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 998, "auc": 0.6192384769539078}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9445564516129032, "accuracy_n": 992, "auc": 0.9445564516129032}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5579710144927537, "accuracy_n": 276, "auc": 0.5579710144927537}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6327967806841046, "accuracy_n": 994, "auc": 0.6327967806841046}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3534136546184739, "accuracy_n": 498, "auc": 0.3534136546184739}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 498, "auc": 0.3333333333333333}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2962962962962963, "accuracy_n": 297, "auc": 0.2962962962962963}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.20802005012531327, "accuracy_n": 399, "auc": 0.20802005012531327}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9355885078776645, "accuracy_n": 322, "auc": 0.9355885078776645}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6642387218045112, "accuracy_n": 292, "auc": 0.6642387218045112}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.7897834837225761, "accuracy_n": 1902, "auc": 0.7897834837225761}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6851623436060418, "accuracy_n": 2000, "auc": 0.6851623436060418}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 59, "auc": 0.8333333333333334}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5100200400801603, "accuracy_n": 998, "auc": 0.5100200400801603}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5504032258064516, "accuracy_n": 992, "auc": 0.5504032258064516}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3433734939759036, "accuracy_n": 498, "auc": 0.3433734939759036}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.3192771084337349, "accuracy_n": 498, "auc": 0.3192771084337349}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.2760942760942761, "accuracy_n": 297, "auc": 0.2760942760942761}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.759075907590759, "accuracy_n": 303, "auc": 0.759075907590759}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5054448563484709, "accuracy_n": 322, "auc": 0.5054448563484709}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5640977443609022, "accuracy_n": 292, "auc": 0.5640977443609022}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5811138014527845, "accuracy_n": 413, "auc": 0.5811138014527845}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6209195859872612, "accuracy_n": 1902, "auc": 0.6209195859872612}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.5595344919515945, "accuracy_n": 2000, "auc": 0.5595344919515945}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.6680672268907564, "accuracy_n": 59, "auc": 0.6680672268907564}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h17", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
