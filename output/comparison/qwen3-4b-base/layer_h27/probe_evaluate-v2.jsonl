{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9428857715430862, "accuracy_n": 998, "auc": 0.9428857715430862}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9405241935483871, "accuracy_n": 992, "auc": 0.9405241935483871}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8768115942028986, "accuracy_n": 276, "auc": 0.8768115942028986}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 100, "auc": 0.66}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5663983903420523, "accuracy_n": 994, "auc": 0.5663983903420523}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3453815261044177, "accuracy_n": 498, "auc": 0.3453815261044177}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6285140562248996, "accuracy_n": 498, "auc": 0.6285140562248996}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4175084175084175, "accuracy_n": 297, "auc": 0.4175084175084175}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.556390977443609, "accuracy_n": 399, "auc": 0.556390977443609}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8118811881188119, "accuracy_n": 303, "auc": 0.8118811881188119}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8568504788384306, "accuracy_n": 322, "auc": 0.8568504788384306}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5414473684210526, "accuracy_n": 292, "auc": 0.5414473684210526}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.54302016985138, "accuracy_n": 1902, "auc": 0.54302016985138}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5767342010465778, "accuracy_n": 2000, "auc": 0.5767342010465778}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7801120448179272, "accuracy_n": 59, "auc": 0.7801120448179272}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8230769230769232, "accuracy_n": 23, "auc": 0.8230769230769232}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9418837675350702, "accuracy_n": 998, "auc": 0.9418837675350702}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9455645161290323, "accuracy_n": 992, "auc": 0.9455645161290323}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8768115942028986, "accuracy_n": 276, "auc": 0.8768115942028986}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6448692152917505, "accuracy_n": 994, "auc": 0.6448692152917505}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4819277108433735, "accuracy_n": 498, "auc": 0.4819277108433735}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6827309236947792, "accuracy_n": 498, "auc": 0.6827309236947792}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7542087542087542, "accuracy_n": 297, "auc": 0.7542087542087542}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7443609022556391, "accuracy_n": 399, "auc": 0.7443609022556391}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8514851485148515, "accuracy_n": 303, "auc": 0.8514851485148515}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8885542168674698, "accuracy_n": 322, "auc": 0.8885542168674698}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5088815789473684, "accuracy_n": 292, "auc": 0.5088815789473684}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6981942232837933, "accuracy_n": 1902, "auc": 0.6981942232837933}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6041100837402301, "accuracy_n": 2000, "auc": 0.6041100837402301}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7226890756302521, "accuracy_n": 59, "auc": 0.7226890756302521}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8538461538461538, "accuracy_n": 23, "auc": 0.8538461538461538}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9438877755511023, "accuracy_n": 998, "auc": 0.9438877755511023}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9294354838709677, "accuracy_n": 992, "auc": 0.9294354838709677}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8804347826086957, "accuracy_n": 276, "auc": 0.8804347826086957}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5543259557344065, "accuracy_n": 994, "auc": 0.5543259557344065}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3092369477911647, "accuracy_n": 498, "auc": 0.3092369477911647}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5582329317269076, "accuracy_n": 498, "auc": 0.5582329317269076}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.39730639730639733, "accuracy_n": 297, "auc": 0.39730639730639733}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.46115288220551376, "accuracy_n": 399, "auc": 0.46115288220551376}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6237623762376238, "accuracy_n": 303, "auc": 0.6237623762376238}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8805220883534136, "accuracy_n": 322, "auc": 0.8805220883534136}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5405075187969924, "accuracy_n": 292, "auc": 0.5405075187969924}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6222760290556901, "accuracy_n": 413, "auc": 0.6222760290556901}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5077472576079263, "accuracy_n": 1902, "auc": 0.5077472576079263}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5371829230352158, "accuracy_n": 2000, "auc": 0.5371829230352158}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5728291316526611, "accuracy_n": 59, "auc": 0.5728291316526611}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9230769230769231, "accuracy_n": 23, "auc": 0.9230769230769231}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9338677354709419, "accuracy_n": 998, "auc": 0.9338677354709419}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8074596774193549, "accuracy_n": 992, "auc": 0.8074596774193549}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7355072463768116, "accuracy_n": 276, "auc": 0.7355072463768116}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6670020120724346, "accuracy_n": 994, "auc": 0.6670020120724346}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.26706827309236947, "accuracy_n": 498, "auc": 0.26706827309236947}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.572289156626506, "accuracy_n": 498, "auc": 0.572289156626506}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2996632996632997, "accuracy_n": 297, "auc": 0.2996632996632997}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2731829573934837, "accuracy_n": 399, "auc": 0.2731829573934837}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 303, "auc": 0.6666666666666666}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7767995057151684, "accuracy_n": 322, "auc": 0.7767995057151684}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5398496240601504, "accuracy_n": 292, "auc": 0.5398496240601504}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7869249394673123, "accuracy_n": 413, "auc": 0.7869249394673123}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.552764508138712, "accuracy_n": 1902, "auc": 0.552764508138712}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5281136490272988, "accuracy_n": 2000, "auc": 0.5281136490272988}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6848739495798319, "accuracy_n": 59, "auc": 0.6848739495798319}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 998, "auc": 0.5150300601202404}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8951612903225806, "accuracy_n": 992, "auc": 0.8951612903225806}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3152610441767068, "accuracy_n": 498, "auc": 0.3152610441767068}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.608433734939759, "accuracy_n": 498, "auc": 0.608433734939759}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3434343434343434, "accuracy_n": 297, "auc": 0.3434343434343434}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6290726817042607, "accuracy_n": 399, "auc": 0.6290726817042607}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6402640264026402, "accuracy_n": 303, "auc": 0.6402640264026402}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8827232004942849, "accuracy_n": 322, "auc": 0.8827232004942849}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5293233082706766, "accuracy_n": 292, "auc": 0.5293233082706766}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8668280871670703, "accuracy_n": 413, "auc": 0.8668280871670703}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51050402512385, "accuracy_n": 1902, "auc": 0.51050402512385}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5423417853845238, "accuracy_n": 2000, "auc": 0.5423417853845238}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5504201680672269, "accuracy_n": 59, "auc": 0.5504201680672269}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8006012024048096, "accuracy_n": 998, "auc": 0.8006012024048096}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7167338709677419, "accuracy_n": 992, "auc": 0.7167338709677419}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5652173913043478, "accuracy_n": 276, "auc": 0.5652173913043478}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7072434607645876, "accuracy_n": 994, "auc": 0.7072434607645876}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 498, "auc": 0.6666666666666666}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7329317269076305, "accuracy_n": 498, "auc": 0.7329317269076305}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8047138047138047, "accuracy_n": 297, "auc": 0.8047138047138047}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8045112781954887, "accuracy_n": 399, "auc": 0.8045112781954887}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8646864686468647, "accuracy_n": 303, "auc": 0.8646864686468647}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7721655854185975, "accuracy_n": 322, "auc": 0.7721655854185975}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7269266917293233, "accuracy_n": 292, "auc": 0.7269266917293233}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5684857130219392, "accuracy_n": 1902, "auc": 0.5684857130219392}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5865097300125346, "accuracy_n": 2000, "auc": 0.5865097300125346}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8459383753501402, "accuracy_n": 59, "auc": 0.8459383753501402}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9208416833667334, "accuracy_n": 998, "auc": 0.9208416833667334}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9314516129032258, "accuracy_n": 992, "auc": 0.9314516129032258}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7753623188405797, "accuracy_n": 276, "auc": 0.7753623188405797}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8158953722334004, "accuracy_n": 994, "auc": 0.8158953722334004}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7168674698795181, "accuracy_n": 498, "auc": 0.7168674698795181}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7389558232931727, "accuracy_n": 498, "auc": 0.7389558232931727}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8148148148148148, "accuracy_n": 297, "auc": 0.8148148148148148}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7969924812030075, "accuracy_n": 399, "auc": 0.7969924812030075}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9273927392739274, "accuracy_n": 303, "auc": 0.9273927392739274}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.947829780661106, "accuracy_n": 322, "auc": 0.947829780661106}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6974624060150376, "accuracy_n": 292, "auc": 0.6974624060150376}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5788161270346779, "accuracy_n": 1902, "auc": 0.5788161270346779}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6233390253881651, "accuracy_n": 2000, "auc": 0.6233390253881651}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7338935574229692, "accuracy_n": 59, "auc": 0.7338935574229692}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7344689378757515, "accuracy_n": 998, "auc": 0.7344689378757515}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7308467741935484, "accuracy_n": 992, "auc": 0.7308467741935484}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5869565217391305, "accuracy_n": 276, "auc": 0.5869565217391305}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5432595573440644, "accuracy_n": 994, "auc": 0.5432595573440644}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4718875502008032, "accuracy_n": 498, "auc": 0.4718875502008032}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068273092369478, "accuracy_n": 498, "auc": 0.7068273092369478}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8080808080808081, "accuracy_n": 297, "auc": 0.8080808080808081}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7919799498746867, "accuracy_n": 399, "auc": 0.7919799498746867}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8415841584158416, "accuracy_n": 303, "auc": 0.8415841584158416}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5134383688600557, "accuracy_n": 322, "auc": 0.5134383688600557}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5789943609022556, "accuracy_n": 292, "auc": 0.5789943609022556}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6432070506015569, "accuracy_n": 1902, "auc": 0.6432070506015569}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5617642969111849, "accuracy_n": 2000, "auc": 0.5617642969111849}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7787114845938375, "accuracy_n": 59, "auc": 0.7787114845938375}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.576923076923077, "accuracy_n": 23, "auc": 0.576923076923077}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.874749498997996, "accuracy_n": 998, "auc": 0.874749498997996}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8175403225806451, "accuracy_n": 992, "auc": 0.8175403225806451}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6231884057971014, "accuracy_n": 276, "auc": 0.6231884057971014}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 994, "auc": 0.6599597585513078}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5060240963855421, "accuracy_n": 498, "auc": 0.5060240963855421}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7269076305220884, "accuracy_n": 498, "auc": 0.7269076305220884}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8114478114478114, "accuracy_n": 297, "auc": 0.8114478114478114}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7994987468671679, "accuracy_n": 399, "auc": 0.7994987468671679}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8448844884488449, "accuracy_n": 303, "auc": 0.8448844884488449}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5552981155390794, "accuracy_n": 322, "auc": 0.5552981155390794}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.575046992481203, "accuracy_n": 292, "auc": 0.575046992481203}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6401495046001415, "accuracy_n": 1902, "auc": 0.6401495046001415}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5781432096987013, "accuracy_n": 2000, "auc": 0.5781432096987013}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7338935574229692, "accuracy_n": 59, "auc": 0.7338935574229692}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7995991983967936, "accuracy_n": 998, "auc": 0.7995991983967936}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8235887096774194, "accuracy_n": 992, "auc": 0.8235887096774194}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5615942028985508, "accuracy_n": 276, "auc": 0.5615942028985508}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5915492957746479, "accuracy_n": 994, "auc": 0.5915492957746479}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.30522088353413657, "accuracy_n": 498, "auc": 0.30522088353413657}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5261044176706827, "accuracy_n": 498, "auc": 0.5261044176706827}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.622895622895623, "accuracy_n": 297, "auc": 0.622895622895623}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5789473684210527, "accuracy_n": 399, "auc": 0.5789473684210527}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9438943894389439, "accuracy_n": 303, "auc": 0.9438943894389439}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7282591906085882, "accuracy_n": 322, "auc": 0.7282591906085882}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5809680451127819, "accuracy_n": 292, "auc": 0.5809680451127819}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6484231245576786, "accuracy_n": 1902, "auc": 0.6484231245576786}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5735970685417436, "accuracy_n": 2000, "auc": 0.5735970685417436}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8739495798319328, "accuracy_n": 59, "auc": 0.8739495798319328}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8897795591182365, "accuracy_n": 998, "auc": 0.8897795591182365}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8125, "accuracy_n": 992, "auc": 0.8125}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5108695652173914, "accuracy_n": 276, "auc": 0.5108695652173914}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3413654618473896, "accuracy_n": 498, "auc": 0.3413654618473896}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5401606425702812, "accuracy_n": 498, "auc": 0.5401606425702812}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7070707070707071, "accuracy_n": 297, "auc": 0.7070707070707071}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5864661654135338, "accuracy_n": 399, "auc": 0.5864661654135338}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9273927392739274, "accuracy_n": 303, "auc": 0.9273927392739274}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5991272783441459, "accuracy_n": 322, "auc": 0.5991272783441459}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5240131578947369, "accuracy_n": 292, "auc": 0.5240131578947369}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5919420116772823, "accuracy_n": 1902, "auc": 0.5919420116772823}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6201783843967672, "accuracy_n": 2000, "auc": 0.6201783843967672}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8235294117647058, "accuracy_n": 59, "auc": 0.8235294117647058}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9348697394789579, "accuracy_n": 998, "auc": 0.9348697394789579}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7227822580645161, "accuracy_n": 992, "auc": 0.7227822580645161}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8297101449275363, "accuracy_n": 276, "auc": 0.8297101449275363}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7635814889336016, "accuracy_n": 994, "auc": 0.7635814889336016}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.35943775100401604, "accuracy_n": 498, "auc": 0.35943775100401604}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6044176706827309, "accuracy_n": 498, "auc": 0.6044176706827309}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4377104377104377, "accuracy_n": 297, "auc": 0.4377104377104377}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.49373433583959897, "accuracy_n": 399, "auc": 0.49373433583959897}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9421532282978067, "accuracy_n": 322, "auc": 0.9421532282978067}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5218984962406015, "accuracy_n": 292, "auc": 0.5218984962406015}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.60791533970276, "accuracy_n": 1902, "auc": 0.60791533970276}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6033418063921075, "accuracy_n": 2000, "auc": 0.6033418063921075}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7044817927170869, "accuracy_n": 59, "auc": 0.7044817927170869}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7324649298597194, "accuracy_n": 998, "auc": 0.7324649298597194}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5191532258064516, "accuracy_n": 992, "auc": 0.5191532258064516}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6557971014492754, "accuracy_n": 276, "auc": 0.6557971014492754}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5583501006036218, "accuracy_n": 994, "auc": 0.5583501006036218}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2891566265060241, "accuracy_n": 498, "auc": 0.2891566265060241}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4497991967871486, "accuracy_n": 498, "auc": 0.4497991967871486}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5757575757575758, "accuracy_n": 297, "auc": 0.5757575757575758}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5513784461152882, "accuracy_n": 399, "auc": 0.5513784461152882}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.834983498349835, "accuracy_n": 303, "auc": 0.834983498349835}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6179718875502008, "accuracy_n": 322, "auc": 0.6179718875502008}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7919172932330827, "accuracy_n": 292, "auc": 0.7919172932330827}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9200968523002422, "accuracy_n": 413, "auc": 0.9200968523002422}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6171709129511678, "accuracy_n": 1902, "auc": 0.6171709129511678}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5404491021258674, "accuracy_n": 2000, "auc": 0.5404491021258674}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6372549019607843, "accuracy_n": 59, "auc": 0.6372549019607843}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8917835671342685, "accuracy_n": 998, "auc": 0.8917835671342685}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7731854838709677, "accuracy_n": 992, "auc": 0.7731854838709677}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.572463768115942, "accuracy_n": 276, "auc": 0.572463768115942}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6639839034205232, "accuracy_n": 994, "auc": 0.6639839034205232}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3433734939759036, "accuracy_n": 498, "auc": 0.3433734939759036}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5461847389558233, "accuracy_n": 498, "auc": 0.5461847389558233}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6835016835016835, "accuracy_n": 297, "auc": 0.6835016835016835}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6741854636591479, "accuracy_n": 399, "auc": 0.6741854636591479}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.758920296570899, "accuracy_n": 322, "auc": 0.758920296570899}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.588157894736842, "accuracy_n": 292, "auc": 0.588157894736842}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6216394639065818, "accuracy_n": 1902, "auc": 0.6216394639065818}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5773904379480992, "accuracy_n": 2000, "auc": 0.5773904379480992}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9173669467787114, "accuracy_n": 59, "auc": 0.9173669467787114}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5260521042084169, "accuracy_n": 998, "auc": 0.5260521042084169}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6743951612903226, "accuracy_n": 992, "auc": 0.6743951612903226}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5674044265593562, "accuracy_n": 994, "auc": 0.5674044265593562}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.25301204819277107, "accuracy_n": 498, "auc": 0.25301204819277107}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.3293172690763052, "accuracy_n": 498, "auc": 0.3293172690763052}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5117845117845118, "accuracy_n": 297, "auc": 0.5117845117845118}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.42857142857142855, "accuracy_n": 399, "auc": 0.42857142857142855}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7755775577557755, "accuracy_n": 303, "auc": 0.7755775577557755}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6986407167130059, "accuracy_n": 322, "auc": 0.6986407167130059}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5492011278195489, "accuracy_n": 292, "auc": 0.5492011278195489}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8595641646489104, "accuracy_n": 413, "auc": 0.8595641646489104}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8271585279547062, "accuracy_n": 1902, "auc": 0.8271585279547062}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5473515939254071, "accuracy_n": 2000, "auc": 0.5473515939254071}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7492997198879552, "accuracy_n": 59, "auc": 0.7492997198879552}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7575150300601202, "accuracy_n": 998, "auc": 0.7575150300601202}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7560483870967742, "accuracy_n": 992, "auc": 0.7560483870967742}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6086956521739131, "accuracy_n": 276, "auc": 0.6086956521739131}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6086519114688129, "accuracy_n": 994, "auc": 0.6086519114688129}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2891566265060241, "accuracy_n": 498, "auc": 0.2891566265060241}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4859437751004016, "accuracy_n": 498, "auc": 0.4859437751004016}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5993265993265994, "accuracy_n": 297, "auc": 0.5993265993265994}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5864661654135338, "accuracy_n": 399, "auc": 0.5864661654135338}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9075907590759076, "accuracy_n": 303, "auc": 0.9075907590759076}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6982931726907631, "accuracy_n": 322, "auc": 0.6982931726907631}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5610902255639098, "accuracy_n": 292, "auc": 0.5610902255639098}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6646275654635527, "accuracy_n": 1902, "auc": 0.6646275654635527}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6296573062875698, "accuracy_n": 2000, "auc": 0.6296573062875698}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.876750700280112, "accuracy_n": 59, "auc": 0.876750700280112}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8076152304609219, "accuracy_n": 998, "auc": 0.8076152304609219}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7379032258064516, "accuracy_n": 992, "auc": 0.7379032258064516}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6106639839034205, "accuracy_n": 994, "auc": 0.6106639839034205}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.2971887550200803, "accuracy_n": 498, "auc": 0.2971887550200803}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.4598393574297189, "accuracy_n": 498, "auc": 0.4598393574297189}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5925925925925926, "accuracy_n": 297, "auc": 0.5925925925925926}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5137844611528822, "accuracy_n": 399, "auc": 0.5137844611528822}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9306930693069307, "accuracy_n": 303, "auc": 0.9306930693069307}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6287843682421996, "accuracy_n": 322, "auc": 0.6287843682421996}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5457236842105263, "accuracy_n": 292, "auc": 0.5457236842105263}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6538780520169851, "accuracy_n": 1902, "auc": 0.6538780520169851}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.582950445110685, "accuracy_n": 2000, "auc": 0.582950445110685}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8781512605042017, "accuracy_n": 59, "auc": 0.8781512605042017}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8987975951903807, "accuracy_n": 998, "auc": 0.8987975951903807}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.8941532258064516, "accuracy_n": 992, "auc": 0.8941532258064516}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 276, "auc": 0.782608695652174}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5381526104417671, "accuracy_n": 498, "auc": 0.5381526104417671}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7008032128514057, "accuracy_n": 498, "auc": 0.7008032128514057}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7373737373737373, "accuracy_n": 297, "auc": 0.7373737373737373}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7543859649122807, "accuracy_n": 399, "auc": 0.7543859649122807}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7110750695088044, "accuracy_n": 322, "auc": 0.7110750695088044}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5939379699248121, "accuracy_n": 292, "auc": 0.5939379699248121}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.6011091206652512, "accuracy_n": 1902, "auc": 0.6011091206652512}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.5764911132918984, "accuracy_n": 2000, "auc": 0.5764911132918984}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7212885154061625, "accuracy_n": 59, "auc": 0.7212885154061625}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-4B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h27", "token_idx": -1, "accuracy": 0.7923076923076924, "accuracy_n": 23, "auc": 0.7923076923076924}}
