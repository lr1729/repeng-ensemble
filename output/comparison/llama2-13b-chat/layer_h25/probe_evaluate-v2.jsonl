{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9408224674022067, "accuracy_n": 997, "auc": 0.9408224674022067}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9144869215291751, "accuracy_n": 994, "auc": 0.9144869215291751}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7328519855595668, "accuracy_n": 277, "auc": 0.7328519855595668}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8189134808853119, "accuracy_n": 994, "auc": 0.8189134808853119}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5470941883767535, "accuracy_n": 499, "auc": 0.5470941883767535}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6438631790744467, "accuracy_n": 497, "auc": 0.6438631790744467}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5986622073578596, "accuracy_n": 299, "auc": 0.5986622073578596}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5975, "accuracy_n": 400, "auc": 0.5975}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9930877355576151, "accuracy_n": 322, "auc": 0.9930877355576151}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7084586466165413, "accuracy_n": 292, "auc": 0.7084586466165413}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9346246973365617, "accuracy_n": 413, "auc": 0.9346246973365617}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.710164543524416, "accuracy_n": 1902, "auc": 0.710164543524416}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6317200509383888, "accuracy_n": 2000, "auc": 0.6317200509383888}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8207282913165266, "accuracy_n": 59, "auc": 0.8207282913165266}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9247743229689067, "accuracy_n": 997, "auc": 0.9247743229689067}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9336016096579477, "accuracy_n": 994, "auc": 0.9336016096579477}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6642599277978339, "accuracy_n": 277, "auc": 0.6642599277978339}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 994, "auc": 0.7142857142857143}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.533066132264529, "accuracy_n": 499, "auc": 0.533066132264529}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6338028169014085, "accuracy_n": 497, "auc": 0.6338028169014085}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.568561872909699, "accuracy_n": 299, "auc": 0.568561872909699}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.645, "accuracy_n": 400, "auc": 0.645}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8382838283828383, "accuracy_n": 303, "auc": 0.8382838283828383}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9739728143342601, "accuracy_n": 322, "auc": 0.9739728143342601}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.843468045112782, "accuracy_n": 292, "auc": 0.843468045112782}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9055690072639225, "accuracy_n": 413, "auc": 0.9055690072639225}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6571644992922858, "accuracy_n": 1902, "auc": 0.6571644992922858}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5800954144446144, "accuracy_n": 2000, "auc": 0.5800954144446144}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7436974789915967, "accuracy_n": 59, "auc": 0.7436974789915967}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9144869215291751, "accuracy_n": 994, "auc": 0.9144869215291751}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7545126353790613, "accuracy_n": 277, "auc": 0.7545126353790613}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8128772635814889, "accuracy_n": 994, "auc": 0.8128772635814889}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.44288577154308617, "accuracy_n": 499, "auc": 0.44288577154308617}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6297786720321932, "accuracy_n": 497, "auc": 0.6297786720321932}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.45819397993311034, "accuracy_n": 299, "auc": 0.45819397993311034}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.515, "accuracy_n": 400, "auc": 0.515}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8547854785478548, "accuracy_n": 303, "auc": 0.8547854785478548}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9902301513747297, "accuracy_n": 322, "auc": 0.9902301513747297}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6625939849624061, "accuracy_n": 292, "auc": 0.6625939849624061}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9733656174334141, "accuracy_n": 413, "auc": 0.9733656174334141}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5939523619957536, "accuracy_n": 1902, "auc": 0.5939523619957536}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.556211792457077, "accuracy_n": 2000, "auc": 0.556211792457077}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7282913165266106, "accuracy_n": 59, "auc": 0.7282913165266106}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8259557344064387, "accuracy_n": 994, "auc": 0.8259557344064387}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6137184115523465, "accuracy_n": 277, "auc": 0.6137184115523465}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.829979879275654, "accuracy_n": 994, "auc": 0.829979879275654}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5991983967935872, "accuracy_n": 499, "auc": 0.5991983967935872}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6579476861167002, "accuracy_n": 497, "auc": 0.6579476861167002}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5919732441471572, "accuracy_n": 299, "auc": 0.5919732441471572}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.665, "accuracy_n": 400, "auc": 0.665}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8613861386138614, "accuracy_n": 303, "auc": 0.8613861386138614}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6888322520852641, "accuracy_n": 322, "auc": 0.6888322520852641}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.656156015037594, "accuracy_n": 292, "auc": 0.656156015037594}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.910411622276029, "accuracy_n": 413, "auc": 0.910411622276029}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5301132342533617, "accuracy_n": 1902, "auc": 0.5301132342533617}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6225947567071712, "accuracy_n": 2000, "auc": 0.6225947567071712}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6120448179271708, "accuracy_n": 59, "auc": 0.6120448179271708}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8769230769230769, "accuracy_n": 23, "auc": 0.8769230769230769}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.880641925777332, "accuracy_n": 997, "auc": 0.880641925777332}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9024144869215291, "accuracy_n": 994, "auc": 0.9024144869215291}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6112224448897795, "accuracy_n": 499, "auc": 0.6112224448897795}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6559356136820925, "accuracy_n": 497, "auc": 0.6559356136820925}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6053511705685619, "accuracy_n": 299, "auc": 0.6053511705685619}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 400, "auc": 0.7}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9894192153228298, "accuracy_n": 322, "auc": 0.9894192153228298}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5824718045112782, "accuracy_n": 292, "auc": 0.5824718045112782}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9685230024213075, "accuracy_n": 413, "auc": 0.9685230024213075}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6583598726114649, "accuracy_n": 1902, "auc": 0.6583598726114649}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5492492789897153, "accuracy_n": 2000, "auc": 0.5492492789897153}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7535014005602241, "accuracy_n": 59, "auc": 0.7535014005602241}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9047141424272819, "accuracy_n": 997, "auc": 0.9047141424272819}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7142857142857143, "accuracy_n": 994, "auc": 0.7142857142857143}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7746478873239436, "accuracy_n": 994, "auc": 0.7746478873239436}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.627254509018036, "accuracy_n": 499, "auc": 0.627254509018036}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 497, "auc": 0.6599597585513078}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 400, "auc": 0.71}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9716172381835032, "accuracy_n": 322, "auc": 0.9716172381835032}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8584586466165414, "accuracy_n": 292, "auc": 0.8584586466165414}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5932203389830508, "accuracy_n": 413, "auc": 0.5932203389830508}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5313263004246285, "accuracy_n": 1902, "auc": 0.5313263004246285}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5070620493998333, "accuracy_n": 2000, "auc": 0.5070620493998333}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8487394957983193, "accuracy_n": 59, "auc": 0.8487394957983193}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7404426559356136, "accuracy_n": 994, "auc": 0.7404426559356136}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6245487364620939, "accuracy_n": 277, "auc": 0.6245487364620939}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8219315895372233, "accuracy_n": 994, "auc": 0.8219315895372233}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6212424849699398, "accuracy_n": 499, "auc": 0.6212424849699398}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.676056338028169, "accuracy_n": 497, "auc": 0.676056338028169}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7025, "accuracy_n": 400, "auc": 0.7025}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5742574257425742, "accuracy_n": 303, "auc": 0.5742574257425742}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9940145196169293, "accuracy_n": 322, "auc": 0.9940145196169293}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7539943609022557, "accuracy_n": 292, "auc": 0.7539943609022557}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5811138014527845, "accuracy_n": 413, "auc": 0.5811138014527845}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5067188605803256, "accuracy_n": 1902, "auc": 0.5067188605803256}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6115927849953833, "accuracy_n": 2000, "auc": 0.6115927849953833}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8879551820728291, "accuracy_n": 59, "auc": 0.8879551820728291}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6769230769230768, "accuracy_n": 23, "auc": 0.6769230769230768}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9307923771313942, "accuracy_n": 997, "auc": 0.9307923771313942}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7223340040241448, "accuracy_n": 994, "auc": 0.7223340040241448}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5667870036101083, "accuracy_n": 277, "auc": 0.5667870036101083}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8199195171026157, "accuracy_n": 994, "auc": 0.8199195171026157}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6132264529058116, "accuracy_n": 499, "auc": 0.6132264529058116}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6321070234113713, "accuracy_n": 299, "auc": 0.6321070234113713}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5544554455445545, "accuracy_n": 303, "auc": 0.5544554455445545}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9793790546802594, "accuracy_n": 322, "auc": 0.9793790546802594}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.842857142857143, "accuracy_n": 292, "auc": 0.842857142857143}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6585956416464891, "accuracy_n": 413, "auc": 0.6585956416464891}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5329087048832272, "accuracy_n": 1902, "auc": 0.5329087048832272}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5512840135288839, "accuracy_n": 2000, "auc": 0.5512840135288839}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8977591036414565, "accuracy_n": 59, "auc": 0.8977591036414565}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9317953861584755, "accuracy_n": 997, "auc": 0.9317953861584755}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7082494969818913, "accuracy_n": 994, "auc": 0.7082494969818913}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7867203219315896, "accuracy_n": 994, "auc": 0.7867203219315896}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 499, "auc": 0.6172344689378757}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6720321931589537, "accuracy_n": 497, "auc": 0.6720321931589537}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6254180602006689, "accuracy_n": 299, "auc": 0.6254180602006689}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7025, "accuracy_n": 400, "auc": 0.7025}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9864071671300587, "accuracy_n": 322, "auc": 0.9864071671300587}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8212406015037594, "accuracy_n": 292, "auc": 0.8212406015037594}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5571755573248407, "accuracy_n": 1902, "auc": 0.5571755573248407}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5436962743550422, "accuracy_n": 2000, "auc": 0.5436962743550422}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8151260504201681, "accuracy_n": 59, "auc": 0.8151260504201681}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8756268806419257, "accuracy_n": 997, "auc": 0.8756268806419257}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7263581488933601, "accuracy_n": 994, "auc": 0.7263581488933601}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6770623742454729, "accuracy_n": 994, "auc": 0.6770623742454729}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3527054108216433, "accuracy_n": 499, "auc": 0.3527054108216433}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5050301810865191, "accuracy_n": 497, "auc": 0.5050301810865191}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3612040133779264, "accuracy_n": 299, "auc": 0.3612040133779264}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.28, "accuracy_n": 400, "auc": 0.28}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9958294717330861, "accuracy_n": 322, "auc": 0.9958294717330861}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5634868421052631, "accuracy_n": 292, "auc": 0.5634868421052631}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8861985472154964, "accuracy_n": 413, "auc": 0.8861985472154964}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5692022735314932, "accuracy_n": 1902, "auc": 0.5692022735314932}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6428930844034697, "accuracy_n": 2000, "auc": 0.6428930844034697}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7184873949579832, "accuracy_n": 59, "auc": 0.7184873949579832}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8846153846153847, "accuracy_n": 23, "auc": 0.8846153846153847}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9167502507522568, "accuracy_n": 997, "auc": 0.9167502507522568}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8531187122736419, "accuracy_n": 994, "auc": 0.8531187122736419}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6859205776173285, "accuracy_n": 277, "auc": 0.6859205776173285}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7907444668008048, "accuracy_n": 994, "auc": 0.7907444668008048}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.32595573440643866, "accuracy_n": 497, "auc": 0.32595573440643866}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.2408026755852843, "accuracy_n": 299, "auc": 0.2408026755852843}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 400, "auc": 0.25}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9273927392739274, "accuracy_n": 303, "auc": 0.9273927392739274}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9952116156935433, "accuracy_n": 322, "auc": 0.9952116156935433}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6360902255639098, "accuracy_n": 292, "auc": 0.6360902255639098}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9176755447941889, "accuracy_n": 413, "auc": 0.9176755447941889}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6449741242038216, "accuracy_n": 1902, "auc": 0.6449741242038216}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6667767063910073, "accuracy_n": 2000, "auc": 0.6667767063910073}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6792717086834734, "accuracy_n": 59, "auc": 0.6792717086834734}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7686116700201208, "accuracy_n": 994, "auc": 0.7686116700201208}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5591182364729459, "accuracy_n": 499, "auc": 0.5591182364729459}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6116700201207244, "accuracy_n": 497, "auc": 0.6116700201207244}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5953177257525084, "accuracy_n": 299, "auc": 0.5953177257525084}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 400, "auc": 0.65}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8943894389438944, "accuracy_n": 303, "auc": 0.8943894389438944}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9976830398517146, "accuracy_n": 322, "auc": 0.9976830398517146}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.85578007518797, "accuracy_n": 292, "auc": 0.85578007518797}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9709443099273608, "accuracy_n": 413, "auc": 0.9709443099273608}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5183220541401273, "accuracy_n": 1902, "auc": 0.5183220541401273}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.647598783160721, "accuracy_n": 2000, "auc": 0.647598783160721}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7815126050420168, "accuracy_n": 59, "auc": 0.7815126050420168}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8054162487462387, "accuracy_n": 997, "auc": 0.8054162487462387}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7243460764587525, "accuracy_n": 994, "auc": 0.7243460764587525}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.555956678700361, "accuracy_n": 277, "auc": 0.555956678700361}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5784708249496981, "accuracy_n": 994, "auc": 0.5784708249496981}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5170340681362725, "accuracy_n": 499, "auc": 0.5170340681362725}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.45472837022132795, "accuracy_n": 497, "auc": 0.45472837022132795}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5819397993311036, "accuracy_n": 299, "auc": 0.5819397993311036}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5775, "accuracy_n": 400, "auc": 0.5775}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7062706270627063, "accuracy_n": 303, "auc": 0.7062706270627063}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.993628359592215, "accuracy_n": 322, "auc": 0.993628359592215}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9313439849624061, "accuracy_n": 292, "auc": 0.9313439849624061}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9322033898305084, "accuracy_n": 413, "auc": 0.9322033898305084}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5091040782024062, "accuracy_n": 1902, "auc": 0.5091040782024062}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5134638604536238, "accuracy_n": 2000, "auc": 0.5134638604536238}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7394957983193278, "accuracy_n": 59, "auc": 0.7394957983193278}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7562688064192578, "accuracy_n": 997, "auc": 0.7562688064192578}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8058350100603622, "accuracy_n": 994, "auc": 0.8058350100603622}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6245487364620939, "accuracy_n": 277, "auc": 0.6245487364620939}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.744466800804829, "accuracy_n": 994, "auc": 0.744466800804829}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4168336673346693, "accuracy_n": 499, "auc": 0.4168336673346693}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5935613682092555, "accuracy_n": 497, "auc": 0.5935613682092555}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.46153846153846156, "accuracy_n": 299, "auc": 0.46153846153846156}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.285, "accuracy_n": 400, "auc": 0.285}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9603960396039604, "accuracy_n": 303, "auc": 0.9603960396039604}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9956750077232005, "accuracy_n": 322, "auc": 0.9956750077232005}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.749624060150376, "accuracy_n": 292, "auc": 0.749624060150376}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5365954529370134, "accuracy_n": 1902, "auc": 0.5365954529370134}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6582096136705351, "accuracy_n": 2000, "auc": 0.6582096136705351}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6470588235294118, "accuracy_n": 59, "auc": 0.6470588235294118}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9358074222668004, "accuracy_n": 997, "auc": 0.9358074222668004}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7816901408450704, "accuracy_n": 994, "auc": 0.7816901408450704}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6101083032490975, "accuracy_n": 277, "auc": 0.6101083032490975}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6941649899396378, "accuracy_n": 994, "auc": 0.6941649899396378}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.38877755511022044, "accuracy_n": 499, "auc": 0.38877755511022044}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.317907444668008, "accuracy_n": 497, "auc": 0.317907444668008}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.35785953177257523, "accuracy_n": 299, "auc": 0.35785953177257523}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5275, "accuracy_n": 400, "auc": 0.5275}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9636963696369637, "accuracy_n": 303, "auc": 0.9636963696369637}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6075841828853877, "accuracy_n": 322, "auc": 0.6075841828853877}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5903665413533835, "accuracy_n": 292, "auc": 0.5903665413533835}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9685230024213075, "accuracy_n": 413, "auc": 0.9685230024213075}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9873252830856334, "accuracy_n": 1902, "auc": 0.9873252830856334}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6083586174609034, "accuracy_n": 2000, "auc": 0.6083586174609034}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6666666666666667, "accuracy_n": 59, "auc": 0.6666666666666667}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7723169508525577, "accuracy_n": 997, "auc": 0.7723169508525577}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6871227364185111, "accuracy_n": 994, "auc": 0.6871227364185111}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5563380281690141, "accuracy_n": 994, "auc": 0.5563380281690141}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3066132264529058, "accuracy_n": 499, "auc": 0.3066132264529058}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.49899396378269617, "accuracy_n": 497, "auc": 0.49899396378269617}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3010033444816054, "accuracy_n": 299, "auc": 0.3010033444816054}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3025, "accuracy_n": 400, "auc": 0.3025}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9306930693069307, "accuracy_n": 303, "auc": 0.9306930693069307}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9435434043867779, "accuracy_n": 322, "auc": 0.9435434043867779}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5398496240601504, "accuracy_n": 292, "auc": 0.5398496240601504}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9564164648910412, "accuracy_n": 413, "auc": 0.9564164648910412}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5524073336871903, "accuracy_n": 1902, "auc": 0.5524073336871903}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6646499386278446, "accuracy_n": 2000, "auc": 0.6646499386278446}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65406162464986, "accuracy_n": 59, "auc": 0.65406162464986}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8886659979939819, "accuracy_n": 997, "auc": 0.8886659979939819}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7183098591549296, "accuracy_n": 994, "auc": 0.7183098591549296}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7766599597585513, "accuracy_n": 994, "auc": 0.7766599597585513}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.49498997995991983, "accuracy_n": 499, "auc": 0.49498997995991983}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6096579476861167, "accuracy_n": 497, "auc": 0.6096579476861167}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5183946488294314, "accuracy_n": 299, "auc": 0.5183946488294314}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5825, "accuracy_n": 400, "auc": 0.5825}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8877887788778878, "accuracy_n": 303, "auc": 0.8877887788778878}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9938600556070435, "accuracy_n": 322, "auc": 0.9938600556070435}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6420112781954888, "accuracy_n": 292, "auc": 0.6420112781954888}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6866861288039632, "accuracy_n": 1902, "auc": 0.6866861288039632}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6885375620599037, "accuracy_n": 2000, "auc": 0.6885375620599037}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7535014005602241, "accuracy_n": 59, "auc": 0.7535014005602241}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6098294884653962, "accuracy_n": 997, "auc": 0.6098294884653962}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8008048289738431, "accuracy_n": 994, "auc": 0.8008048289738431}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5985915492957746, "accuracy_n": 994, "auc": 0.5985915492957746}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3346693386773547, "accuracy_n": 499, "auc": 0.3346693386773547}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5513078470824949, "accuracy_n": 497, "auc": 0.5513078470824949}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3775, "accuracy_n": 400, "auc": 0.3775}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9747451343836887, "accuracy_n": 322, "auc": 0.9747451343836887}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6265507518796992, "accuracy_n": 292, "auc": 0.6265507518796992}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.801452784503632, "accuracy_n": 413, "auc": 0.801452784503632}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5051408793347487, "accuracy_n": 1902, "auc": 0.5051408793347487}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5980398924011567, "accuracy_n": 2000, "auc": 0.5980398924011567}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6470588235294118, "accuracy_n": 59, "auc": 0.6470588235294118}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9538461538461538, "accuracy_n": 23, "auc": 0.9538461538461538}}
