{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9247743229689067, "accuracy_n": 997, "auc": 0.9247743229689067}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9426559356136821, "accuracy_n": 994, "auc": 0.9426559356136821}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7003610108303249, "accuracy_n": 277, "auc": 0.7003610108303249}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8138832997987927, "accuracy_n": 994, "auc": 0.8138832997987927}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3627254509018036, "accuracy_n": 499, "auc": 0.3627254509018036}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.42655935613682094, "accuracy_n": 497, "auc": 0.42655935613682094}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2425, "accuracy_n": 400, "auc": 0.2425}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9922767995057151, "accuracy_n": 322, "auc": 0.9922767995057151}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5945488721804512, "accuracy_n": 292, "auc": 0.5945488721804512}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7615744426751593, "accuracy_n": 1902, "auc": 0.7615744426751593}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7083397106355394, "accuracy_n": 2000, "auc": 0.7083397106355394}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7759103641456583, "accuracy_n": 59, "auc": 0.7759103641456583}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5615384615384617, "accuracy_n": 23, "auc": 0.5615384615384617}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9287863590772317, "accuracy_n": 997, "auc": 0.9287863590772317}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9416498993963782, "accuracy_n": 994, "auc": 0.9416498993963782}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7111913357400722, "accuracy_n": 277, "auc": 0.7111913357400722}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.817907444668008, "accuracy_n": 994, "auc": 0.817907444668008}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3807615230460922, "accuracy_n": 499, "auc": 0.3807615230460922}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5694164989939637, "accuracy_n": 497, "auc": 0.5694164989939637}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.46153846153846156, "accuracy_n": 299, "auc": 0.46153846153846156}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.385, "accuracy_n": 400, "auc": 0.385}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9938214396045721, "accuracy_n": 322, "auc": 0.9938214396045721}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7788533834586467, "accuracy_n": 292, "auc": 0.7788533834586467}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7794287420382167, "accuracy_n": 1902, "auc": 0.7794287420382167}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7587128953552231, "accuracy_n": 2000, "auc": 0.7587128953552231}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6862745098039216, "accuracy_n": 59, "auc": 0.6862745098039216}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7148014440433214, "accuracy_n": 277, "auc": 0.7148014440433214}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.795774647887324, "accuracy_n": 994, "auc": 0.795774647887324}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.36472945891783565, "accuracy_n": 499, "auc": 0.36472945891783565}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5271629778672032, "accuracy_n": 497, "auc": 0.5271629778672032}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3010033444816054, "accuracy_n": 299, "auc": 0.3010033444816054}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.29, "accuracy_n": 400, "auc": 0.29}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9926243435279579, "accuracy_n": 322, "auc": 0.9926243435279579}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8100563909774436, "accuracy_n": 292, "auc": 0.8100563909774436}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7806307501769285, "accuracy_n": 1902, "auc": 0.7806307501769285}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.726355214232338, "accuracy_n": 2000, "auc": 0.726355214232338}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7058823529411764, "accuracy_n": 59, "auc": 0.7058823529411764}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9087261785356068, "accuracy_n": 997, "auc": 0.9087261785356068}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9255533199195171, "accuracy_n": 994, "auc": 0.9255533199195171}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6678700361010831, "accuracy_n": 277, "auc": 0.6678700361010831}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 994, "auc": 0.8088531187122736}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5831663326653307, "accuracy_n": 499, "auc": 0.5831663326653307}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6277665995975855, "accuracy_n": 497, "auc": 0.6277665995975855}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.66, "accuracy_n": 400, "auc": 0.66}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9879518072289157, "accuracy_n": 322, "auc": 0.9879518072289157}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8336936090225565, "accuracy_n": 292, "auc": 0.8336936090225565}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6516929847841473, "accuracy_n": 1902, "auc": 0.6516929847841473}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6992519299467108, "accuracy_n": 2000, "auc": 0.6992519299467108}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7254901960784315, "accuracy_n": 59, "auc": 0.7254901960784315}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9087261785356068, "accuracy_n": 997, "auc": 0.9087261785356068}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8531187122736419, "accuracy_n": 994, "auc": 0.8531187122736419}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28256513026052105, "accuracy_n": 499, "auc": 0.28256513026052105}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5130784708249497, "accuracy_n": 497, "auc": 0.5130784708249497}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3411371237458194, "accuracy_n": 299, "auc": 0.3411371237458194}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4425, "accuracy_n": 400, "auc": 0.4425}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6600660066006601, "accuracy_n": 303, "auc": 0.6600660066006601}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9818890948409021, "accuracy_n": 322, "auc": 0.9818890948409021}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6295112781954887, "accuracy_n": 292, "auc": 0.6295112781954887}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9661016949152542, "accuracy_n": 413, "auc": 0.9661016949152542}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7562251194267515, "accuracy_n": 1902, "auc": 0.7562251194267515}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7441071226712843, "accuracy_n": 2000, "auc": 0.7441071226712843}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6330532212885154, "accuracy_n": 59, "auc": 0.6330532212885154}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6880641925777332, "accuracy_n": 997, "auc": 0.6880641925777332}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.693158953722334, "accuracy_n": 994, "auc": 0.693158953722334}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5261569416498993, "accuracy_n": 994, "auc": 0.5261569416498993}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6438631790744467, "accuracy_n": 497, "auc": 0.6438631790744467}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6287625418060201, "accuracy_n": 299, "auc": 0.6287625418060201}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7125, "accuracy_n": 400, "auc": 0.7125}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9465168365770775, "accuracy_n": 322, "auc": 0.9465168365770775}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8679511278195489, "accuracy_n": 292, "auc": 0.8679511278195489}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.648910411622276, "accuracy_n": 413, "auc": 0.648910411622276}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5340145081387119, "accuracy_n": 1902, "auc": 0.5340145081387119}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6489802818817593, "accuracy_n": 2000, "auc": 0.6489802818817593}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7563025210084033, "accuracy_n": 59, "auc": 0.7563025210084033}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6538461538461537, "accuracy_n": 23, "auc": 0.6538461538461537}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7031093279839519, "accuracy_n": 997, "auc": 0.7031093279839519}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8118712273641852, "accuracy_n": 994, "auc": 0.8118712273641852}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6787003610108303, "accuracy_n": 277, "auc": 0.6787003610108303}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7283702213279678, "accuracy_n": 994, "auc": 0.7283702213279678}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6332665330661322, "accuracy_n": 499, "auc": 0.6332665330661322}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6780684104627767, "accuracy_n": 497, "auc": 0.6780684104627767}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6454849498327759, "accuracy_n": 299, "auc": 0.6454849498327759}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5577557755775577, "accuracy_n": 303, "auc": 0.5577557755775577}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9732777262897745, "accuracy_n": 322, "auc": 0.9732777262897745}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8538063909774436, "accuracy_n": 292, "auc": 0.8538063909774436}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5368719037508847, "accuracy_n": 1902, "auc": 0.5368719037508847}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7423970053189202, "accuracy_n": 2000, "auc": 0.7423970053189202}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.711484593837535, "accuracy_n": 59, "auc": 0.711484593837535}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6058174523570712, "accuracy_n": 997, "auc": 0.6058174523570712}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7243460764587525, "accuracy_n": 994, "auc": 0.7243460764587525}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5845070422535211, "accuracy_n": 994, "auc": 0.5845070422535211}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6539235412474849, "accuracy_n": 497, "auc": 0.6539235412474849}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9471346926166203, "accuracy_n": 322, "auc": 0.9471346926166203}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.850328947368421, "accuracy_n": 292, "auc": 0.850328947368421}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8426150121065376, "accuracy_n": 413, "auc": 0.8426150121065376}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6007032908704883, "accuracy_n": 1902, "auc": 0.6007032908704883}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6505628531900016, "accuracy_n": 2000, "auc": 0.6505628531900016}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7997198879551821, "accuracy_n": 59, "auc": 0.7997198879551821}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6461538461538461, "accuracy_n": 23, "auc": 0.6461538461538461}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5416248746238717, "accuracy_n": 997, "auc": 0.5416248746238717}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.710261569416499, "accuracy_n": 994, "auc": 0.710261569416499}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6277665995975855, "accuracy_n": 994, "auc": 0.6277665995975855}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6212424849699398, "accuracy_n": 499, "auc": 0.6212424849699398}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6287625418060201, "accuracy_n": 299, "auc": 0.6287625418060201}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5214521452145214, "accuracy_n": 303, "auc": 0.5214521452145214}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9267454433117084, "accuracy_n": 322, "auc": 0.9267454433117084}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.856109022556391, "accuracy_n": 292, "auc": 0.856109022556391}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.559322033898305, "accuracy_n": 413, "auc": 0.559322033898305}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5462026716206653, "accuracy_n": 1902, "auc": 0.5462026716206653}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6336507479199991, "accuracy_n": 2000, "auc": 0.6336507479199991}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7521008403361344, "accuracy_n": 59, "auc": 0.7521008403361344}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8702213279678068, "accuracy_n": 994, "auc": 0.8702213279678068}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5667870036101083, "accuracy_n": 277, "auc": 0.5667870036101083}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7947686116700201, "accuracy_n": 994, "auc": 0.7947686116700201}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.280561122244489, "accuracy_n": 499, "auc": 0.280561122244489}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.48490945674044267, "accuracy_n": 497, "auc": 0.48490945674044267}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.431438127090301, "accuracy_n": 299, "auc": 0.431438127090301}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.23, "accuracy_n": 400, "auc": 0.23}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9983008958912574, "accuracy_n": 322, "auc": 0.9983008958912574}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8202067669172932, "accuracy_n": 292, "auc": 0.8202067669172932}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9733656174334141, "accuracy_n": 413, "auc": 0.9733656174334141}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.752796576433121, "accuracy_n": 1902, "auc": 0.752796576433121}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7558738704672388, "accuracy_n": 2000, "auc": 0.7558738704672388}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.819327731092437, "accuracy_n": 59, "auc": 0.819327731092437}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.925777331995988, "accuracy_n": 997, "auc": 0.925777331995988}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8611670020120724, "accuracy_n": 994, "auc": 0.8611670020120724}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6787003610108303, "accuracy_n": 277, "auc": 0.6787003610108303}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7565392354124748, "accuracy_n": 994, "auc": 0.7565392354124748}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.342685370741483, "accuracy_n": 499, "auc": 0.342685370741483}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.48490945674044267, "accuracy_n": 497, "auc": 0.48490945674044267}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3210702341137124, "accuracy_n": 299, "auc": 0.3210702341137124}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.36, "accuracy_n": 400, "auc": 0.36}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.976897689768977, "accuracy_n": 303, "auc": 0.976897689768977}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9981078158789002, "accuracy_n": 322, "auc": 0.9981078158789002}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7643327067669172, "accuracy_n": 292, "auc": 0.7643327067669172}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9346246973365617, "accuracy_n": 413, "auc": 0.9346246973365617}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.733430644019816, "accuracy_n": 1902, "auc": 0.733430644019816}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7451750081779521, "accuracy_n": 2000, "auc": 0.7451750081779521}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7717086834733894, "accuracy_n": 59, "auc": 0.7717086834733894}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9157472417251755, "accuracy_n": 997, "auc": 0.9157472417251755}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9004024144869215, "accuracy_n": 994, "auc": 0.9004024144869215}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5018050541516246, "accuracy_n": 277, "auc": 0.5018050541516246}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7635814889336016, "accuracy_n": 994, "auc": 0.7635814889336016}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6372745490981964, "accuracy_n": 499, "auc": 0.6372745490981964}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.682092555331992, "accuracy_n": 497, "auc": 0.682092555331992}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6925, "accuracy_n": 400, "auc": 0.6925}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9991890639481, "accuracy_n": 322, "auc": 0.9991890639481}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8858552631578948, "accuracy_n": 292, "auc": 0.8858552631578948}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9757869249394673, "accuracy_n": 413, "auc": 0.9757869249394673}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6499922593772116, "accuracy_n": 1902, "auc": 0.6499922593772116}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.792439070504452, "accuracy_n": 2000, "auc": 0.792439070504452}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8165266106442577, "accuracy_n": 59, "auc": 0.8165266106442577}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7402206619859579, "accuracy_n": 997, "auc": 0.7402206619859579}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9195171026156942, "accuracy_n": 994, "auc": 0.9195171026156942}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6559356136820925, "accuracy_n": 497, "auc": 0.6559356136820925}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6254180602006689, "accuracy_n": 299, "auc": 0.6254180602006689}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6625, "accuracy_n": 400, "auc": 0.6625}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9603960396039604, "accuracy_n": 303, "auc": 0.9603960396039604}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9926629595304295, "accuracy_n": 322, "auc": 0.9926629595304295}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9367951127819548, "accuracy_n": 292, "auc": 0.9367951127819548}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5222454440905874, "accuracy_n": 1902, "auc": 0.5222454440905874}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7447768644480657, "accuracy_n": 2000, "auc": 0.7447768644480657}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8081232492997199, "accuracy_n": 59, "auc": 0.8081232492997199}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9164989939637826, "accuracy_n": 994, "auc": 0.9164989939637826}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7220216606498195, "accuracy_n": 277, "auc": 0.7220216606498195}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7997987927565392, "accuracy_n": 994, "auc": 0.7997987927565392}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3486973947895792, "accuracy_n": 499, "auc": 0.3486973947895792}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6257545271629779, "accuracy_n": 497, "auc": 0.6257545271629779}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4214046822742475, "accuracy_n": 299, "auc": 0.4214046822742475}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.175, "accuracy_n": 400, "auc": 0.175}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9955977757182576, "accuracy_n": 322, "auc": 0.9955977757182576}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8597744360902254, "accuracy_n": 292, "auc": 0.8597744360902254}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7155310067232838, "accuracy_n": 1902, "auc": 0.7155310067232838}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7964305114146207, "accuracy_n": 2000, "auc": 0.7964305114146207}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8585434173669468, "accuracy_n": 59, "auc": 0.8585434173669468}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8384615384615384, "accuracy_n": 23, "auc": 0.8384615384615384}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9277833500501504, "accuracy_n": 997, "auc": 0.9277833500501504}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8832997987927566, "accuracy_n": 994, "auc": 0.8832997987927566}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6519114688128773, "accuracy_n": 994, "auc": 0.6519114688128773}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2905811623246493, "accuracy_n": 499, "auc": 0.2905811623246493}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2535211267605634, "accuracy_n": 497, "auc": 0.2535211267605634}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3979933110367893, "accuracy_n": 299, "auc": 0.3979933110367893}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.285, "accuracy_n": 400, "auc": 0.285}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.900990099009901, "accuracy_n": 303, "auc": 0.900990099009901}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.827116156935434, "accuracy_n": 322, "auc": 0.827116156935434}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5086466165413533, "accuracy_n": 292, "auc": 0.5086466165413533}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6949152542372882, "accuracy_n": 413, "auc": 0.6949152542372882}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9775566171266808, "accuracy_n": 1902, "auc": 0.9775566171266808}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5815189283331283, "accuracy_n": 2000, "auc": 0.5815189283331283}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6624649859943978, "accuracy_n": 59, "auc": 0.6624649859943978}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.925777331995988, "accuracy_n": 997, "auc": 0.925777331995988}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9175050301810865, "accuracy_n": 994, "auc": 0.9175050301810865}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7220216606498195, "accuracy_n": 277, "auc": 0.7220216606498195}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7374245472837022, "accuracy_n": 994, "auc": 0.7374245472837022}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.531062124248497, "accuracy_n": 499, "auc": 0.531062124248497}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6317907444668008, "accuracy_n": 497, "auc": 0.6317907444668008}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5551839464882943, "accuracy_n": 299, "auc": 0.5551839464882943}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 400, "auc": 0.62}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9438943894389439, "accuracy_n": 303, "auc": 0.9438943894389439}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9964859437751005, "accuracy_n": 322, "auc": 0.9964859437751005}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8517857142857143, "accuracy_n": 292, "auc": 0.8517857142857143}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7036215056617128, "accuracy_n": 1902, "auc": 0.7036215056617128}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8290757963624868, "accuracy_n": 2000, "auc": 0.8290757963624868}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.788515406162465, "accuracy_n": 59, "auc": 0.788515406162465}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8461538461538463, "accuracy_n": 23, "auc": 0.8461538461538463}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9047141424272819, "accuracy_n": 997, "auc": 0.9047141424272819}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8340040241448692, "accuracy_n": 994, "auc": 0.8340040241448692}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8028169014084507, "accuracy_n": 994, "auc": 0.8028169014084507}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270541082164328, "accuracy_n": 499, "auc": 0.5270541082164328}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6297786720321932, "accuracy_n": 497, "auc": 0.6297786720321932}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5785953177257525, "accuracy_n": 299, "auc": 0.5785953177257525}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5675, "accuracy_n": 400, "auc": 0.5675}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9966017917825146, "accuracy_n": 322, "auc": 0.9966017917825146}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8231672932330827, "accuracy_n": 292, "auc": 0.8231672932330827}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7590642692852088, "accuracy_n": 1902, "auc": 0.7590642692852088}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7536340618963445, "accuracy_n": 2000, "auc": 0.7536340618963445}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8109243697478992, "accuracy_n": 59, "auc": 0.8109243697478992}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6615384615384616, "accuracy_n": 23, "auc": 0.6615384615384616}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7873620862587764, "accuracy_n": 997, "auc": 0.7873620862587764}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6619718309859155, "accuracy_n": 994, "auc": 0.6619718309859155}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6086519114688129, "accuracy_n": 994, "auc": 0.6086519114688129}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2845691382765531, "accuracy_n": 499, "auc": 0.2845691382765531}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5110663983903421, "accuracy_n": 497, "auc": 0.5110663983903421}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2842809364548495, "accuracy_n": 299, "auc": 0.2842809364548495}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.195, "accuracy_n": 400, "auc": 0.195}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9932808155699723, "accuracy_n": 322, "auc": 0.9932808155699723}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5176691729323307, "accuracy_n": 292, "auc": 0.5176691729323307}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6850097310686483, "accuracy_n": 1902, "auc": 0.6850097310686483}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5973106291371184, "accuracy_n": 2000, "auc": 0.5973106291371184}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6358543417366946, "accuracy_n": 59, "auc": 0.6358543417366946}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 23, "auc": 1.0}}
