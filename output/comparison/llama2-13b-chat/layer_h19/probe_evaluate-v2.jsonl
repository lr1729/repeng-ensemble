{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9235412474849095, "accuracy_n": 994, "auc": 0.9235412474849095}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7220216606498195, "accuracy_n": 277, "auc": 0.7220216606498195}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8279678068410463, "accuracy_n": 994, "auc": 0.8279678068410463}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.44288577154308617, "accuracy_n": 499, "auc": 0.44288577154308617}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5935613682092555, "accuracy_n": 497, "auc": 0.5935613682092555}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.48494983277591974, "accuracy_n": 299, "auc": 0.48494983277591974}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4925, "accuracy_n": 400, "auc": 0.4925}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8415841584158416, "accuracy_n": 303, "auc": 0.8415841584158416}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9883765832561013, "accuracy_n": 322, "auc": 0.9883765832561013}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8285244360902255, "accuracy_n": 292, "auc": 0.8285244360902255}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7210025212314225, "accuracy_n": 1902, "auc": 0.7210025212314225}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6551790196260849, "accuracy_n": 2000, "auc": 0.6551790196260849}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8865546218487396, "accuracy_n": 59, "auc": 0.8865546218487396}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9327983951855566, "accuracy_n": 997, "auc": 0.9327983951855566}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7148014440433214, "accuracy_n": 277, "auc": 0.7148014440433214}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 994, "auc": 0.8088531187122736}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6861167002012073, "accuracy_n": 497, "auc": 0.6861167002012073}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6488294314381271, "accuracy_n": 299, "auc": 0.6488294314381271}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6475, "accuracy_n": 400, "auc": 0.6475}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9570957095709571, "accuracy_n": 303, "auc": 0.9570957095709571}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9986870559159716, "accuracy_n": 322, "auc": 0.9986870559159716}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8198308270676692, "accuracy_n": 292, "auc": 0.8198308270676692}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9830508474576272, "accuracy_n": 413, "auc": 0.9830508474576272}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7677802105449398, "accuracy_n": 1902, "auc": 0.7677802105449398}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7164186271243919, "accuracy_n": 2000, "auc": 0.7164186271243919}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.742296918767507, "accuracy_n": 59, "auc": 0.742296918767507}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9692307692307693, "accuracy_n": 23, "auc": 0.9692307692307693}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9408224674022067, "accuracy_n": 997, "auc": 0.9408224674022067}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9255533199195171, "accuracy_n": 994, "auc": 0.9255533199195171}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7545126353790613, "accuracy_n": 277, "auc": 0.7545126353790613}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8219315895372233, "accuracy_n": 994, "auc": 0.8219315895372233}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5410821643286573, "accuracy_n": 499, "auc": 0.5410821643286573}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 497, "auc": 0.6599597585513078}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5752508361204013, "accuracy_n": 299, "auc": 0.5752508361204013}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6025, "accuracy_n": 400, "auc": 0.6025}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8811881188118812, "accuracy_n": 303, "auc": 0.8811881188118812}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9985325919060859, "accuracy_n": 322, "auc": 0.9985325919060859}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8456766917293231, "accuracy_n": 292, "auc": 0.8456766917293231}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9757869249394673, "accuracy_n": 413, "auc": 0.9757869249394673}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.739023796886058, "accuracy_n": 1902, "auc": 0.739023796886058}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6559247888487744, "accuracy_n": 2000, "auc": 0.6559247888487744}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6694677871148459, "accuracy_n": 59, "auc": 0.6694677871148459}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9287863590772317, "accuracy_n": 997, "auc": 0.9287863590772317}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8742454728370221, "accuracy_n": 994, "auc": 0.8742454728370221}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6498194945848376, "accuracy_n": 277, "auc": 0.6498194945848376}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8309859154929577, "accuracy_n": 994, "auc": 0.8309859154929577}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5991983967935872, "accuracy_n": 499, "auc": 0.5991983967935872}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 497, "auc": 0.6599597585513078}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6120401337792643, "accuracy_n": 299, "auc": 0.6120401337792643}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6567656765676567, "accuracy_n": 303, "auc": 0.6567656765676567}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.983240654927402, "accuracy_n": 322, "auc": 0.983240654927402}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8690319548872181, "accuracy_n": 292, "auc": 0.8690319548872181}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.801452784503632, "accuracy_n": 413, "auc": 0.801452784503632}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5831177016985137, "accuracy_n": 1902, "auc": 0.5831177016985137}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6353063455907583, "accuracy_n": 2000, "auc": 0.6353063455907583}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6708683473389356, "accuracy_n": 59, "auc": 0.6708683473389356}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9153846153846154, "accuracy_n": 23, "auc": 0.9153846153846154}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9267803410230692, "accuracy_n": 997, "auc": 0.9267803410230692}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9275653923541247, "accuracy_n": 994, "auc": 0.9275653923541247}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5771543086172345, "accuracy_n": 499, "auc": 0.5771543086172345}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.647887323943662, "accuracy_n": 497, "auc": 0.647887323943662}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6825, "accuracy_n": 400, "auc": 0.6825}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5412541254125413, "accuracy_n": 303, "auc": 0.5412541254125413}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9915817114612295, "accuracy_n": 322, "auc": 0.9915817114612295}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7371710526315789, "accuracy_n": 292, "auc": 0.7371710526315789}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7504799186128803, "accuracy_n": 1902, "auc": 0.7504799186128803}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6651151065534657, "accuracy_n": 2000, "auc": 0.6651151065534657}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7268907563025211, "accuracy_n": 59, "auc": 0.7268907563025211}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8946840521564694, "accuracy_n": 997, "auc": 0.8946840521564694}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8712273641851107, "accuracy_n": 994, "auc": 0.8712273641851107}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6859205776173285, "accuracy_n": 277, "auc": 0.6859205776173285}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8018108651911469, "accuracy_n": 994, "auc": 0.8018108651911469}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6521739130434783, "accuracy_n": 299, "auc": 0.6521739130434783}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7075, "accuracy_n": 400, "auc": 0.7075}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9639326536916899, "accuracy_n": 322, "auc": 0.9639326536916899}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8599624060150376, "accuracy_n": 292, "auc": 0.8599624060150376}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6222760290556901, "accuracy_n": 413, "auc": 0.6222760290556901}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6078113941967446, "accuracy_n": 1902, "auc": 0.6078113941967446}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6245564648838231, "accuracy_n": 2000, "auc": 0.6245564648838231}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8459383753501402, "accuracy_n": 59, "auc": 0.8459383753501402}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9398194583751254, "accuracy_n": 997, "auc": 0.9398194583751254}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.864185110663984, "accuracy_n": 994, "auc": 0.864185110663984}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7075812274368231, "accuracy_n": 277, "auc": 0.7075812274368231}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8289738430583501, "accuracy_n": 994, "auc": 0.8289738430583501}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6352705410821643, "accuracy_n": 499, "auc": 0.6352705410821643}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6800804828973843, "accuracy_n": 497, "auc": 0.6800804828973843}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6975, "accuracy_n": 400, "auc": 0.6975}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9904618473895582, "accuracy_n": 322, "auc": 0.9904618473895582}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8535244360902255, "accuracy_n": 292, "auc": 0.8535244360902255}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.927360774818402, "accuracy_n": 413, "auc": 0.927360774818402}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5829075990799717, "accuracy_n": 1902, "auc": 0.5829075990799717}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7020514405700458, "accuracy_n": 2000, "auc": 0.7020514405700458}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8207282913165266, "accuracy_n": 59, "auc": 0.8207282913165266}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8853118712273642, "accuracy_n": 994, "auc": 0.8853118712273642}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7328519855595668, "accuracy_n": 277, "auc": 0.7328519855595668}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7857142857142857, "accuracy_n": 994, "auc": 0.7857142857142857}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6132264529058116, "accuracy_n": 499, "auc": 0.6132264529058116}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 400, "auc": 0.7}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5247524752475248, "accuracy_n": 303, "auc": 0.5247524752475248}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9705359901143034, "accuracy_n": 322, "auc": 0.9705359901143034}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8590695488721805, "accuracy_n": 292, "auc": 0.8590695488721805}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7336561743341404, "accuracy_n": 413, "auc": 0.7336561743341404}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6244670028308563, "accuracy_n": 1902, "auc": 0.6244670028308563}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6423798991435907, "accuracy_n": 2000, "auc": 0.6423798991435907}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8529411764705882, "accuracy_n": 59, "auc": 0.8529411764705882}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.876629889669007, "accuracy_n": 997, "auc": 0.876629889669007}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8893360160965795, "accuracy_n": 994, "auc": 0.8893360160965795}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6137184115523465, "accuracy_n": 277, "auc": 0.6137184115523465}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8118712273641852, "accuracy_n": 994, "auc": 0.8118712273641852}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.676056338028169, "accuracy_n": 497, "auc": 0.676056338028169}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7025, "accuracy_n": 400, "auc": 0.7025}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9822366388631449, "accuracy_n": 322, "auc": 0.9822366388631449}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.862593984962406, "accuracy_n": 292, "auc": 0.862593984962406}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6144146983368719, "accuracy_n": 1902, "auc": 0.6144146983368719}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6398369811501952, "accuracy_n": 2000, "auc": 0.6398369811501952}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8025210084033614, "accuracy_n": 59, "auc": 0.8025210084033614}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 994, "auc": 0.8088531187122736}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.772635814889336, "accuracy_n": 994, "auc": 0.772635814889336}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.48893360160965793, "accuracy_n": 497, "auc": 0.48893360160965793}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.405, "accuracy_n": 400, "auc": 0.405}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9971037998146431, "accuracy_n": 322, "auc": 0.9971037998146431}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7530545112781954, "accuracy_n": 292, "auc": 0.7530545112781954}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9539951573849879, "accuracy_n": 413, "auc": 0.9539951573849879}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7040594037508847, "accuracy_n": 1902, "auc": 0.7040594037508847}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6755628781990299, "accuracy_n": 2000, "auc": 0.6755628781990299}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8109243697478992, "accuracy_n": 59, "auc": 0.8109243697478992}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8307692307692307, "accuracy_n": 23, "auc": 0.8307692307692307}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9004024144869215, "accuracy_n": 994, "auc": 0.9004024144869215}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7436823104693141, "accuracy_n": 277, "auc": 0.7436823104693141}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7967806841046278, "accuracy_n": 994, "auc": 0.7967806841046278}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.29458917835671344, "accuracy_n": 499, "auc": 0.29458917835671344}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 497, "auc": 0.5392354124748491}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3745819397993311, "accuracy_n": 299, "auc": 0.3745819397993311}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 400, "auc": 0.26}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9570957095709571, "accuracy_n": 303, "auc": 0.9570957095709571}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9967948717948718, "accuracy_n": 322, "auc": 0.9967948717948718}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7601973684210526, "accuracy_n": 292, "auc": 0.7601973684210526}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9007263922518159, "accuracy_n": 413, "auc": 0.9007263922518159}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7006656935598019, "accuracy_n": 1902, "auc": 0.7006656935598019}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.707982581711998, "accuracy_n": 2000, "auc": 0.707982581711998}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7296918767507004, "accuracy_n": 59, "auc": 0.7296918767507004}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6846153846153847, "accuracy_n": 23, "auc": 0.6846153846153847}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9267803410230692, "accuracy_n": 997, "auc": 0.9267803410230692}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8983903420523138, "accuracy_n": 994, "auc": 0.8983903420523138}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7887323943661971, "accuracy_n": 994, "auc": 0.7887323943661971}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.625250501002004, "accuracy_n": 499, "auc": 0.625250501002004}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.647887323943662, "accuracy_n": 497, "auc": 0.647887323943662}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6287625418060201, "accuracy_n": 299, "auc": 0.6287625418060201}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6925, "accuracy_n": 400, "auc": 0.6925}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8151815181518152, "accuracy_n": 303, "auc": 0.8151815181518152}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9986870559159715, "accuracy_n": 322, "auc": 0.9986870559159715}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8898966165413534, "accuracy_n": 292, "auc": 0.8898966165413534}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7336561743341404, "accuracy_n": 413, "auc": 0.7336561743341404}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6184669143665958, "accuracy_n": 1902, "auc": 0.6184669143665958}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7399781321056902, "accuracy_n": 2000, "auc": 0.7399781321056902}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 59, "auc": 0.8333333333333334}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9097291875626881, "accuracy_n": 997, "auc": 0.9097291875626881}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8913480885311871, "accuracy_n": 994, "auc": 0.8913480885311871}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6056338028169014, "accuracy_n": 994, "auc": 0.6056338028169014}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5791583166332666, "accuracy_n": 499, "auc": 0.5791583166332666}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6217303822937625, "accuracy_n": 497, "auc": 0.6217303822937625}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.655, "accuracy_n": 400, "auc": 0.655}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8547854785478548, "accuracy_n": 303, "auc": 0.8547854785478548}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9956750077232005, "accuracy_n": 322, "auc": 0.9956750077232005}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9372650375939849, "accuracy_n": 292, "auc": 0.9372650375939849}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6561743341404358, "accuracy_n": 413, "auc": 0.6561743341404358}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5782588021939137, "accuracy_n": 1902, "auc": 0.5782588021939137}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6658433694563738, "accuracy_n": 2000, "auc": 0.6658433694563738}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7675070028011204, "accuracy_n": 59, "auc": 0.7675070028011204}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9117352056168505, "accuracy_n": 997, "auc": 0.9117352056168505}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8380281690140845, "accuracy_n": 994, "auc": 0.8380281690140845}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6570397111913358, "accuracy_n": 277, "auc": 0.6570397111913358}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7816901408450704, "accuracy_n": 994, "auc": 0.7816901408450704}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3907815631262525, "accuracy_n": 499, "auc": 0.3907815631262525}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.635814889336016, "accuracy_n": 497, "auc": 0.635814889336016}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.46488294314381273, "accuracy_n": 299, "auc": 0.46488294314381273}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3675, "accuracy_n": 400, "auc": 0.3675}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9987256719184431, "accuracy_n": 322, "auc": 0.9987256719184431}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8359962406015038, "accuracy_n": 292, "auc": 0.8359962406015038}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5106101822363764, "accuracy_n": 1902, "auc": 0.5106101822363764}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7151161569326527, "accuracy_n": 2000, "auc": 0.7151161569326527}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7773109243697478, "accuracy_n": 59, "auc": 0.7773109243697478}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8561368209255533, "accuracy_n": 994, "auc": 0.8561368209255533}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6859205776173285, "accuracy_n": 277, "auc": 0.6859205776173285}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6901408450704225, "accuracy_n": 994, "auc": 0.6901408450704225}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.374749498997996, "accuracy_n": 499, "auc": 0.374749498997996}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3702213279678068, "accuracy_n": 497, "auc": 0.3702213279678068}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4280936454849498, "accuracy_n": 299, "auc": 0.4280936454849498}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4975, "accuracy_n": 400, "auc": 0.4975}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7491749174917491, "accuracy_n": 303, "auc": 0.7491749174917491}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7779579857893111, "accuracy_n": 322, "auc": 0.7779579857893111}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7213345864661653, "accuracy_n": 292, "auc": 0.7213345864661653}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9820804582448691, "accuracy_n": 1902, "auc": 0.9820804582448691}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5768197319232243, "accuracy_n": 2000, "auc": 0.5768197319232243}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.788515406162465, "accuracy_n": 59, "auc": 0.788515406162465}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9317953861584755, "accuracy_n": 997, "auc": 0.9317953861584755}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.852112676056338, "accuracy_n": 994, "auc": 0.852112676056338}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7766599597585513, "accuracy_n": 994, "auc": 0.7766599597585513}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.593186372745491, "accuracy_n": 499, "auc": 0.593186372745491}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6680080482897385, "accuracy_n": 497, "auc": 0.6680080482897385}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5785953177257525, "accuracy_n": 299, "auc": 0.5785953177257525}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 400, "auc": 0.61}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8712871287128713, "accuracy_n": 303, "auc": 0.8712871287128713}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.998725671918443, "accuracy_n": 322, "auc": 0.998725671918443}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7711466165413534, "accuracy_n": 292, "auc": 0.7711466165413534}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6190618365180467, "accuracy_n": 1902, "auc": 0.6190618365180467}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7366909454313008, "accuracy_n": 2000, "auc": 0.7366909454313008}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7661064425770308, "accuracy_n": 59, "auc": 0.7661064425770308}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.823943661971831, "accuracy_n": 994, "auc": 0.823943661971831}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.738430583501006, "accuracy_n": 994, "auc": 0.738430583501006}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.49498997995991983, "accuracy_n": 499, "auc": 0.49498997995991983}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5975855130784709, "accuracy_n": 497, "auc": 0.5975855130784709}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5825, "accuracy_n": 400, "auc": 0.5825}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8217821782178217, "accuracy_n": 303, "auc": 0.8217821782178217}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9909638554216869, "accuracy_n": 322, "auc": 0.9909638554216869}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7752819548872181, "accuracy_n": 292, "auc": 0.7752819548872181}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.937046004842615, "accuracy_n": 413, "auc": 0.937046004842615}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.711156449044586, "accuracy_n": 1902, "auc": 0.711156449044586}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6953130079958865, "accuracy_n": 2000, "auc": 0.6953130079958865}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.792717086834734, "accuracy_n": 59, "auc": 0.792717086834734}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5692307692307693, "accuracy_n": 23, "auc": 0.5692307692307693}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5436308926780341, "accuracy_n": 997, "auc": 0.5436308926780341}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7515090543259557, "accuracy_n": 994, "auc": 0.7515090543259557}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5875251509054326, "accuracy_n": 994, "auc": 0.5875251509054326}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.39478957915831664, "accuracy_n": 499, "auc": 0.39478957915831664}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6559356136820925, "accuracy_n": 497, "auc": 0.6559356136820925}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.42474916387959866, "accuracy_n": 299, "auc": 0.42474916387959866}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3625, "accuracy_n": 400, "auc": 0.3625}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8778877887788779, "accuracy_n": 303, "auc": 0.8778877887788779}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9925471115230151, "accuracy_n": 322, "auc": 0.9925471115230151}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6554511278195488, "accuracy_n": 292, "auc": 0.6554511278195488}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9903147699757869, "accuracy_n": 413, "auc": 0.9903147699757869}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5781117303609342, "accuracy_n": 1902, "auc": 0.5781117303609342}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.588359397742585, "accuracy_n": 2000, "auc": 0.588359397742585}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6638655462184875, "accuracy_n": 59, "auc": 0.6638655462184875}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9923076923076923, "accuracy_n": 23, "auc": 0.9923076923076923}}
