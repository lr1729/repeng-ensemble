{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9436619718309859, "accuracy_n": 994, "auc": 0.9436619718309859}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9658291457286432, "accuracy_n": 995, "auc": 0.9658291457286432}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9186746987951807, "accuracy_n": 996, "auc": 0.9186746987951807}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9467336683417086, "accuracy_n": 995, "auc": 0.9467336683417086}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7184115523465704, "accuracy_n": 277, "auc": 0.7184115523465704}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8232931726907631, "accuracy_n": 996, "auc": 0.8232931726907631}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5370741482965932, "accuracy_n": 499, "auc": 0.5370741482965932}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6633266533066132, "accuracy_n": 499, "auc": 0.6633266533066132}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6187290969899666, "accuracy_n": 299, "auc": 0.6187290969899666}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8016032064128257, "accuracy_n": 499, "auc": 0.8016032064128257}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6375, "accuracy_n": 400, "auc": 0.6375}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8973509933774835, "accuracy_n": 302, "auc": 0.8973509933774835}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9916589434661723, "accuracy_n": 322, "auc": 0.9916589434661723}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7351973684210527, "accuracy_n": 292, "auc": 0.7351973684210527}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9466019417475728, "accuracy_n": 412, "auc": 0.9466019417475728}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.937625754527163, "accuracy_n": 994, "auc": 0.937625754527163}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9658291457286432, "accuracy_n": 995, "auc": 0.9658291457286432}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9146586345381527, "accuracy_n": 996, "auc": 0.9146586345381527}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.964824120603015, "accuracy_n": 995, "auc": 0.964824120603015}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7111913357400722, "accuracy_n": 277, "auc": 0.7111913357400722}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7269076305220884, "accuracy_n": 996, "auc": 0.7269076305220884}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5711422845691383, "accuracy_n": 499, "auc": 0.5711422845691383}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6613226452905812, "accuracy_n": 499, "auc": 0.6613226452905812}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 299, "auc": 0.6153846153846154}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8256513026052105, "accuracy_n": 499, "auc": 0.8256513026052105}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6275, "accuracy_n": 400, "auc": 0.6275}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8609271523178808, "accuracy_n": 302, "auc": 0.8609271523178808}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9798424467099165, "accuracy_n": 322, "auc": 0.9798424467099165}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8006109022556391, "accuracy_n": 292, "auc": 0.8006109022556391}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "amazon_polarity", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8592233009708737, "accuracy_n": 412, "auc": 0.8592233009708737}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9285714285714286, "accuracy_n": 994, "auc": 0.9285714285714286}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.963819095477387, "accuracy_n": 995, "auc": 0.963819095477387}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.928714859437751, "accuracy_n": 996, "auc": 0.928714859437751}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9758793969849247, "accuracy_n": 995, "auc": 0.9758793969849247}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6714801444043321, "accuracy_n": 277, "auc": 0.6714801444043321}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6475903614457831, "accuracy_n": 996, "auc": 0.6475903614457831}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4909819639278557, "accuracy_n": 499, "auc": 0.4909819639278557}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5418060200668896, "accuracy_n": 299, "auc": 0.5418060200668896}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7635270541082164, "accuracy_n": 499, "auc": 0.7635270541082164}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.625, "accuracy_n": 400, "auc": 0.625}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8708609271523179, "accuracy_n": 302, "auc": 0.8708609271523179}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9819277108433735, "accuracy_n": 322, "auc": 0.9819277108433735}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8479323308270676, "accuracy_n": 292, "auc": 0.8479323308270676}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7087378640776699, "accuracy_n": 412, "auc": 0.7087378640776699}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9285714285714286, "accuracy_n": 994, "auc": 0.9285714285714286}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9618090452261306, "accuracy_n": 995, "auc": 0.9618090452261306}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9337349397590361, "accuracy_n": 996, "auc": 0.9337349397590361}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9688442211055276, "accuracy_n": 995, "auc": 0.9688442211055276}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.628158844765343, "accuracy_n": 277, "auc": 0.628158844765343}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5773092369477911, "accuracy_n": 996, "auc": 0.5773092369477911}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5230460921843687, "accuracy_n": 499, "auc": 0.5230460921843687}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.627254509018036, "accuracy_n": 499, "auc": 0.627254509018036}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5451505016722408, "accuracy_n": 299, "auc": 0.5451505016722408}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7875751503006012, "accuracy_n": 499, "auc": 0.7875751503006012}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6375, "accuracy_n": 400, "auc": 0.6375}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7582781456953642, "accuracy_n": 302, "auc": 0.7582781456953642}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9665585418597467, "accuracy_n": 322, "auc": 0.9665585418597467}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8550281954887218, "accuracy_n": 292, "auc": 0.8550281954887218}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "dbpedia_14", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8325242718446602, "accuracy_n": 412, "auc": 0.8325242718446602}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8943661971830986, "accuracy_n": 994, "auc": 0.8943661971830986}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9577889447236181, "accuracy_n": 995, "auc": 0.9577889447236181}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9196787148594378, "accuracy_n": 996, "auc": 0.9196787148594378}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9467336683417086, "accuracy_n": 995, "auc": 0.9467336683417086}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57, "accuracy_n": 100, "auc": 0.57}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5281124497991968, "accuracy_n": 996, "auc": 0.5281124497991968}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.342685370741483, "accuracy_n": 499, "auc": 0.342685370741483}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.593186372745491, "accuracy_n": 499, "auc": 0.593186372745491}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.28762541806020064, "accuracy_n": 299, "auc": 0.28762541806020064}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.35470941883767537, "accuracy_n": 499, "auc": 0.35470941883767537}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.285, "accuracy_n": 400, "auc": 0.285}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8576158940397351, "accuracy_n": 302, "auc": 0.8576158940397351}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.900409329626197, "accuracy_n": 322, "auc": 0.900409329626197}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6039473684210527, "accuracy_n": 292, "auc": 0.6039473684210527}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7330097087378641, "accuracy_n": 412, "auc": 0.7330097087378641}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9235412474849095, "accuracy_n": 994, "auc": 0.9235412474849095}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9587939698492463, "accuracy_n": 995, "auc": 0.9587939698492463}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8383534136546185, "accuracy_n": 996, "auc": 0.8383534136546185}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8723618090452261, "accuracy_n": 995, "auc": 0.8723618090452261}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6173285198555957, "accuracy_n": 277, "auc": 0.6173285198555957}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8263052208835341, "accuracy_n": 996, "auc": 0.8263052208835341}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5691382765531062, "accuracy_n": 499, "auc": 0.5691382765531062}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.657314629258517, "accuracy_n": 499, "auc": 0.657314629258517}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8016032064128257, "accuracy_n": 499, "auc": 0.8016032064128257}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6425, "accuracy_n": 400, "auc": 0.6425}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8443708609271523, "accuracy_n": 302, "auc": 0.8443708609271523}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.57510812480692, "accuracy_n": 322, "auc": 0.57510812480692}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5981203007518797, "accuracy_n": 292, "auc": 0.5981203007518797}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8762135922330098, "accuracy_n": 412, "auc": 0.8762135922330098}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9024144869215291, "accuracy_n": 994, "auc": 0.9024144869215291}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9256281407035176, "accuracy_n": 995, "auc": 0.9256281407035176}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8845381526104418, "accuracy_n": 996, "auc": 0.8845381526104418}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9206030150753769, "accuracy_n": 995, "auc": 0.9206030150753769}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6204819277108434, "accuracy_n": 996, "auc": 0.6204819277108434}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5490981963927856, "accuracy_n": 499, "auc": 0.5490981963927856}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6633266533066132, "accuracy_n": 499, "auc": 0.6633266533066132}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5484949832775919, "accuracy_n": 299, "auc": 0.5484949832775919}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.717434869739479, "accuracy_n": 499, "auc": 0.717434869739479}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.675, "accuracy_n": 400, "auc": 0.675}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9608047574915045, "accuracy_n": 322, "auc": 0.9608047574915045}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6242951127819548, "accuracy_n": 292, "auc": 0.6242951127819548}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.779126213592233, "accuracy_n": 412, "auc": 0.779126213592233}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9164989939637826, "accuracy_n": 994, "auc": 0.9164989939637826}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793969849246231, "accuracy_n": 995, "auc": 0.8793969849246231}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6626506024096386, "accuracy_n": 996, "auc": 0.6626506024096386}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7316582914572864, "accuracy_n": 995, "auc": 0.7316582914572864}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8182730923694779, "accuracy_n": 996, "auc": 0.8182730923694779}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6192384769539078, "accuracy_n": 499, "auc": 0.6192384769539078}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6492985971943888, "accuracy_n": 499, "auc": 0.6492985971943888}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6521739130434783, "accuracy_n": 299, "auc": 0.6521739130434783}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8517034068136272, "accuracy_n": 499, "auc": 0.8517034068136272}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 400, "auc": 0.71}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5662251655629139, "accuracy_n": 302, "auc": 0.5662251655629139}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9804603027494594, "accuracy_n": 322, "auc": 0.9804603027494594}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8491541353383458, "accuracy_n": 292, "auc": 0.8491541353383458}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6577669902912622, "accuracy_n": 412, "auc": 0.6577669902912622}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9295774647887324, "accuracy_n": 994, "auc": 0.9295774647887324}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9206030150753769, "accuracy_n": 995, "auc": 0.9206030150753769}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7640562248995983, "accuracy_n": 996, "auc": 0.7640562248995983}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7648241206030151, "accuracy_n": 995, "auc": 0.7648241206030151}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8242971887550201, "accuracy_n": 996, "auc": 0.8242971887550201}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 499, "auc": 0.6172344689378757}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6673346693386774, "accuracy_n": 499, "auc": 0.6673346693386774}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6354515050167224, "accuracy_n": 299, "auc": 0.6354515050167224}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8677354709418837, "accuracy_n": 499, "auc": 0.8677354709418837}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7025, "accuracy_n": 400, "auc": 0.7025}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6026490066225165, "accuracy_n": 302, "auc": 0.6026490066225165}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9198717948717948, "accuracy_n": 322, "auc": 0.9198717948717948}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5092575187969924, "accuracy_n": 292, "auc": 0.5092575187969924}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7475728155339806, "accuracy_n": 412, "auc": 0.7475728155339806}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8963782696177063, "accuracy_n": 994, "auc": 0.8963782696177063}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8944723618090452, "accuracy_n": 995, "auc": 0.8944723618090452}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7098393574297188, "accuracy_n": 996, "auc": 0.7098393574297188}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.770854271356784, "accuracy_n": 995, "auc": 0.770854271356784}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7951807228915663, "accuracy_n": 996, "auc": 0.7951807228915663}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6012024048096193, "accuracy_n": 499, "auc": 0.6012024048096193}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6633266533066132, "accuracy_n": 499, "auc": 0.6633266533066132}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6354515050167224, "accuracy_n": 299, "auc": 0.6354515050167224}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8537074148296593, "accuracy_n": 499, "auc": 0.8537074148296593}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7025, "accuracy_n": 400, "auc": 0.7025}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6192052980132451, "accuracy_n": 302, "auc": 0.6192052980132451}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9673308619091752, "accuracy_n": 322, "auc": 0.9673308619091752}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8711466165413534, "accuracy_n": 292, "auc": 0.8711466165413534}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5533980582524272, "accuracy_n": 412, "auc": 0.5533980582524272}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9366197183098591, "accuracy_n": 994, "auc": 0.9366197183098591}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9226130653266331, "accuracy_n": 995, "auc": 0.9226130653266331}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.713855421686747, "accuracy_n": 996, "auc": 0.713855421686747}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7778894472361809, "accuracy_n": 995, "auc": 0.7778894472361809}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8002008032128514, "accuracy_n": 996, "auc": 0.8002008032128514}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6072144288577155, "accuracy_n": 499, "auc": 0.6072144288577155}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6513026052104208, "accuracy_n": 499, "auc": 0.6513026052104208}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6354515050167224, "accuracy_n": 299, "auc": 0.6354515050167224}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8677354709418837, "accuracy_n": 499, "auc": 0.8677354709418837}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7075, "accuracy_n": 400, "auc": 0.7075}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6125827814569537, "accuracy_n": 302, "auc": 0.6125827814569537}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9752085264133457, "accuracy_n": 322, "auc": 0.9752085264133457}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8766917293233083, "accuracy_n": 292, "auc": 0.8766917293233083}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_easy", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5533980582524272, "accuracy_n": 412, "auc": 0.5533980582524272}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9346076458752515, "accuracy_n": 994, "auc": 0.9346076458752515}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9065326633165829, "accuracy_n": 995, "auc": 0.9065326633165829}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7299196787148594, "accuracy_n": 996, "auc": 0.7299196787148594}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7728643216080402, "accuracy_n": 995, "auc": 0.7728643216080402}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7821285140562249, "accuracy_n": 996, "auc": 0.7821285140562249}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6613226452905812, "accuracy_n": 499, "auc": 0.6613226452905812}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6421404682274248, "accuracy_n": 299, "auc": 0.6421404682274248}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8697394789579158, "accuracy_n": 499, "auc": 0.8697394789579158}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 400, "auc": 0.7}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9072443620636391, "accuracy_n": 322, "auc": 0.9072443620636391}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6301221804511278, "accuracy_n": 292, "auc": 0.6301221804511278}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5703883495145631, "accuracy_n": 412, "auc": 0.5703883495145631}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6951710261569416, "accuracy_n": 994, "auc": 0.6951710261569416}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8984924623115578, "accuracy_n": 995, "auc": 0.8984924623115578}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.679718875502008, "accuracy_n": 996, "auc": 0.679718875502008}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7316582914572864, "accuracy_n": 995, "auc": 0.7316582914572864}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5773092369477911, "accuracy_n": 996, "auc": 0.5773092369477911}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3687374749498998, "accuracy_n": 499, "auc": 0.3687374749498998}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.533066132264529, "accuracy_n": 499, "auc": 0.533066132264529}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.36789297658862874, "accuracy_n": 299, "auc": 0.36789297658862874}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3687374749498998, "accuracy_n": 499, "auc": 0.3687374749498998}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.345, "accuracy_n": 400, "auc": 0.345}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9768211920529801, "accuracy_n": 302, "auc": 0.9768211920529801}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.995443311708372, "accuracy_n": 322, "auc": 0.995443311708372}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.606813909774436, "accuracy_n": 292, "auc": 0.606813909774436}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9029126213592233, "accuracy_n": 412, "auc": 0.9029126213592233}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7334004024144869, "accuracy_n": 994, "auc": 0.7334004024144869}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9316582914572864, "accuracy_n": 995, "auc": 0.9316582914572864}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7269076305220884, "accuracy_n": 996, "auc": 0.7269076305220884}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8482412060301507, "accuracy_n": 995, "auc": 0.8482412060301507}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6245487364620939, "accuracy_n": 277, "auc": 0.6245487364620939}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.643574297188755, "accuracy_n": 996, "auc": 0.643574297188755}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3927855711422846, "accuracy_n": 499, "auc": 0.3927855711422846}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3406813627254509, "accuracy_n": 499, "auc": 0.3406813627254509}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4214046822742475, "accuracy_n": 299, "auc": 0.4214046822742475}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6292585170340681, "accuracy_n": 499, "auc": 0.6292585170340681}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3925, "accuracy_n": 400, "auc": 0.3925}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9105960264900662, "accuracy_n": 302, "auc": 0.9105960264900662}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9933966635773865, "accuracy_n": 322, "auc": 0.9933966635773865}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5511278195488721, "accuracy_n": 292, "auc": 0.5511278195488721}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8276699029126213, "accuracy_n": 412, "auc": 0.8276699029126213}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8913480885311871, "accuracy_n": 994, "auc": 0.8913480885311871}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9587939698492463, "accuracy_n": 995, "auc": 0.9587939698492463}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8493975903614458, "accuracy_n": 996, "auc": 0.8493975903614458}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9125628140703518, "accuracy_n": 995, "auc": 0.9125628140703518}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6498194945848376, "accuracy_n": 277, "auc": 0.6498194945848376}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5763052208835341, "accuracy_n": 996, "auc": 0.5763052208835341}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5190380761523046, "accuracy_n": 499, "auc": 0.5190380761523046}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5450901803607214, "accuracy_n": 499, "auc": 0.5450901803607214}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5250836120401338, "accuracy_n": 299, "auc": 0.5250836120401338}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7414829659318637, "accuracy_n": 499, "auc": 0.7414829659318637}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.585, "accuracy_n": 400, "auc": 0.585}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8576158940397351, "accuracy_n": 302, "auc": 0.8576158940397351}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9966404077849861, "accuracy_n": 322, "auc": 0.9966404077849861}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8584586466165415, "accuracy_n": 292, "auc": 0.8584586466165415}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9635922330097088, "accuracy_n": 412, "auc": 0.9635922330097088}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6830985915492958, "accuracy_n": 994, "auc": 0.6830985915492958}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7819095477386935, "accuracy_n": 995, "auc": 0.7819095477386935}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 996, "auc": 0.6666666666666666}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.770854271356784, "accuracy_n": 995, "auc": 0.770854271356784}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5441767068273092, "accuracy_n": 996, "auc": 0.5441767068273092}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.48296593186372744, "accuracy_n": 499, "auc": 0.48296593186372744}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.29458917835671344, "accuracy_n": 499, "auc": 0.29458917835671344}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4983277591973244, "accuracy_n": 299, "auc": 0.4983277591973244}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7354709418837675, "accuracy_n": 499, "auc": 0.7354709418837675}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4025, "accuracy_n": 400, "auc": 0.4025}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5033112582781457, "accuracy_n": 302, "auc": 0.5033112582781457}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9830089589125734, "accuracy_n": 322, "auc": 0.9830089589125734}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9368421052631579, "accuracy_n": 292, "auc": 0.9368421052631579}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6893203883495146, "accuracy_n": 412, "auc": 0.6893203883495146}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5814889336016097, "accuracy_n": 994, "auc": 0.5814889336016097}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "amazon_polarity", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.7296482412060301, "accuracy_n": 995, "auc": 0.7296482412060301}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5863453815261044, "accuracy_n": 996, "auc": 0.5863453815261044}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "dbpedia_14", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.6673366834170854, "accuracy_n": 995, "auc": 0.6673366834170854}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.74, "accuracy_n": 100, "auc": 0.74}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5512048192771084, "accuracy_n": 996, "auc": 0.5512048192771084}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.3967935871743487, "accuracy_n": 499, "auc": 0.3967935871743487}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 499, "auc": 0.5150300601202404}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4180602006688963, "accuracy_n": 299, "auc": 0.4180602006688963}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_easy", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.4308617234468938, "accuracy_n": 499, "auc": 0.4308617234468938}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.275, "accuracy_n": 400, "auc": 0.275}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.8774834437086093, "accuracy_n": 302, "auc": 0.8774834437086093}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9733935742971888, "accuracy_n": 322, "auc": 0.9733935742971888}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.5146616541353384, "accuracy_n": 292, "auc": 0.5146616541353384}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h31", "token_idx": -1, "accuracy": 0.9902912621359223, "accuracy_n": 412, "auc": 0.9902912621359223}}
