{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9275653923541247, "accuracy_n": 994, "auc": 0.9275653923541247}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7111913357400722, "accuracy_n": 277, "auc": 0.7111913357400722}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7987927565392354, "accuracy_n": 994, "auc": 0.7987927565392354}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5390781563126252, "accuracy_n": 499, "auc": 0.5390781563126252}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 497, "auc": 0.6599597585513078}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5819397993311036, "accuracy_n": 299, "auc": 0.5819397993311036}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.645, "accuracy_n": 400, "auc": 0.645}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8679867986798679, "accuracy_n": 303, "auc": 0.8679867986798679}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9935897435897436, "accuracy_n": 322, "auc": 0.9935897435897436}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7140037593984964, "accuracy_n": 292, "auc": 0.7140037593984964}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9394673123486683, "accuracy_n": 413, "auc": 0.9394673123486683}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.627476999292286, "accuracy_n": 1902, "auc": 0.627476999292286}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.588109307459993, "accuracy_n": 2000, "auc": 0.588109307459993}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6792717086834734, "accuracy_n": 59, "auc": 0.6792717086834734}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8615384615384616, "accuracy_n": 23, "auc": 0.8615384615384616}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9277833500501504, "accuracy_n": 997, "auc": 0.9277833500501504}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9386317907444668, "accuracy_n": 994, "auc": 0.9386317907444668}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6137184115523465, "accuracy_n": 277, "auc": 0.6137184115523465}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4589178356713427, "accuracy_n": 499, "auc": 0.4589178356713427}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 497, "auc": 0.5855130784708249}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5183946488294314, "accuracy_n": 299, "auc": 0.5183946488294314}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 400, "auc": 0.58}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8415841584158416, "accuracy_n": 303, "auc": 0.8415841584158416}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9609206054989188, "accuracy_n": 322, "auc": 0.9609206054989188}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8459116541353384, "accuracy_n": 292, "auc": 0.8459116541353384}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5738498789346247, "accuracy_n": 413, "auc": 0.5738498789346247}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6463431086341118, "accuracy_n": 1902, "auc": 0.6463431086341118}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5347200339322495, "accuracy_n": 2000, "auc": 0.5347200339322495}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6750700280112045, "accuracy_n": 59, "auc": 0.6750700280112045}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9458375125376128, "accuracy_n": 997, "auc": 0.9458375125376128}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8591549295774648, "accuracy_n": 994, "auc": 0.8591549295774648}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7436823104693141, "accuracy_n": 277, "auc": 0.7436823104693141}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7716297786720322, "accuracy_n": 994, "auc": 0.7716297786720322}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 497, "auc": 0.5553319919517102}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.2909698996655518, "accuracy_n": 299, "auc": 0.2909698996655518}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3175, "accuracy_n": 400, "auc": 0.3175}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9207920792079208, "accuracy_n": 303, "auc": 0.9207920792079208}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.941187828236021, "accuracy_n": 322, "auc": 0.941187828236021}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.669313909774436, "accuracy_n": 292, "auc": 0.669313909774436}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6101694915254238, "accuracy_n": 413, "auc": 0.6101694915254238}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6236608722576079, "accuracy_n": 1902, "auc": 0.6236608722576079}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5041760075387215, "accuracy_n": 2000, "auc": 0.5041760075387215}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6400560224089635, "accuracy_n": 59, "auc": 0.6400560224089635}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9277833500501504, "accuracy_n": 997, "auc": 0.9277833500501504}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8128772635814889, "accuracy_n": 994, "auc": 0.8128772635814889}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8309859154929577, "accuracy_n": 994, "auc": 0.8309859154929577}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.48697394789579157, "accuracy_n": 499, "auc": 0.48697394789579157}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.641851106639839, "accuracy_n": 497, "auc": 0.641851106639839}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5418060200668896, "accuracy_n": 299, "auc": 0.5418060200668896}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6175, "accuracy_n": 400, "auc": 0.6175}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7062706270627063, "accuracy_n": 303, "auc": 0.7062706270627063}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.614071671300587, "accuracy_n": 322, "auc": 0.614071671300587}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5891917293233082, "accuracy_n": 292, "auc": 0.5891917293233082}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8523002421307506, "accuracy_n": 413, "auc": 0.8523002421307506}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5113643400566171, "accuracy_n": 1902, "auc": 0.5113643400566171}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5914034966622951, "accuracy_n": 2000, "auc": 0.5914034966622951}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5574229691876751, "accuracy_n": 59, "auc": 0.5574229691876751}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.892678034102307, "accuracy_n": 997, "auc": 0.892678034102307}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.869215291750503, "accuracy_n": 994, "auc": 0.869215291750503}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4308617234468938, "accuracy_n": 499, "auc": 0.4308617234468938}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6297786720321932, "accuracy_n": 497, "auc": 0.6297786720321932}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.47157190635451507, "accuracy_n": 299, "auc": 0.47157190635451507}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5725, "accuracy_n": 400, "auc": 0.5725}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7018072289156628, "accuracy_n": 322, "auc": 0.7018072289156628}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5587875939849625, "accuracy_n": 292, "auc": 0.5587875939849625}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6513317191283293, "accuracy_n": 413, "auc": 0.6513317191283293}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.57536491507431, "accuracy_n": 1902, "auc": 0.57536491507431}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5280536273594768, "accuracy_n": 2000, "auc": 0.5280536273594768}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8966900702106319, "accuracy_n": 997, "auc": 0.8966900702106319}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5201207243460765, "accuracy_n": 994, "auc": 0.5201207243460765}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 100, "auc": 0.68}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7394366197183099, "accuracy_n": 994, "auc": 0.7394366197183099}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6232464929859719, "accuracy_n": 499, "auc": 0.6232464929859719}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.647887323943662, "accuracy_n": 497, "auc": 0.647887323943662}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6488294314381271, "accuracy_n": 299, "auc": 0.6488294314381271}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 400, "auc": 0.71}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6006600660066007, "accuracy_n": 303, "auc": 0.6006600660066007}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9619632375656473, "accuracy_n": 322, "auc": 0.9619632375656473}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7258458646616541, "accuracy_n": 292, "auc": 0.7258458646616541}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6755447941888619, "accuracy_n": 413, "auc": 0.6755447941888619}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5054969479830148, "accuracy_n": 1902, "auc": 0.5054969479830148}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5563988599884558, "accuracy_n": 2000, "auc": 0.5563988599884558}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7661064425770308, "accuracy_n": 59, "auc": 0.7661064425770308}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8856569709127382, "accuracy_n": 997, "auc": 0.8856569709127382}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8209255533199196, "accuracy_n": 994, "auc": 0.8209255533199196}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6389891696750902, "accuracy_n": 277, "auc": 0.6389891696750902}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7847082494969819, "accuracy_n": 994, "auc": 0.7847082494969819}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6172344689378757, "accuracy_n": 499, "auc": 0.6172344689378757}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6639839034205232, "accuracy_n": 497, "auc": 0.6639839034205232}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6354515050167224, "accuracy_n": 299, "auc": 0.6354515050167224}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7075, "accuracy_n": 400, "auc": 0.7075}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8690145196169292, "accuracy_n": 322, "auc": 0.8690145196169292}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5112312030075188, "accuracy_n": 292, "auc": 0.5112312030075188}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6440677966101694, "accuracy_n": 413, "auc": 0.6440677966101694}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5281084129511677, "accuracy_n": 1902, "auc": 0.5281084129511677}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5952238758191707, "accuracy_n": 2000, "auc": 0.5952238758191707}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6512605042016807, "accuracy_n": 59, "auc": 0.6512605042016807}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.653846153846154, "accuracy_n": 23, "auc": 0.653846153846154}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8976930792377131, "accuracy_n": 997, "auc": 0.8976930792377131}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5985915492957746, "accuracy_n": 994, "auc": 0.5985915492957746}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5631768953068592, "accuracy_n": 277, "auc": 0.5631768953068592}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7012072434607646, "accuracy_n": 994, "auc": 0.7012072434607646}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6032064128256514, "accuracy_n": 499, "auc": 0.6032064128256514}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6599597585513078, "accuracy_n": 497, "auc": 0.6599597585513078}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6454849498327759, "accuracy_n": 299, "auc": 0.6454849498327759}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6105610561056105, "accuracy_n": 303, "auc": 0.6105610561056105}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.93790546802595, "accuracy_n": 322, "auc": 0.93790546802595}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7106672932330826, "accuracy_n": 292, "auc": 0.7106672932330826}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6319612590799032, "accuracy_n": 413, "auc": 0.6319612590799032}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5022226645435244, "accuracy_n": 1902, "auc": 0.5022226645435244}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5798673321068906, "accuracy_n": 2000, "auc": 0.5798673321068906}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7703081232492998, "accuracy_n": 59, "auc": 0.7703081232492998}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9167502507522568, "accuracy_n": 997, "auc": 0.9167502507522568}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6951710261569416, "accuracy_n": 994, "auc": 0.6951710261569416}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.83, "accuracy_n": 100, "auc": 0.83}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7474849094567404, "accuracy_n": 994, "auc": 0.7474849094567404}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6539235412474849, "accuracy_n": 497, "auc": 0.6539235412474849}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6321070234113713, "accuracy_n": 299, "auc": 0.6321070234113713}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.705, "accuracy_n": 400, "auc": 0.705}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8269616929255483, "accuracy_n": 322, "auc": 0.8269616929255483}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5006578947368421, "accuracy_n": 292, "auc": 0.5006578947368421}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5326876513317191, "accuracy_n": 413, "auc": 0.5326876513317191}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5198403220099079, "accuracy_n": 1902, "auc": 0.5198403220099079}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5838787802396666, "accuracy_n": 2000, "auc": 0.5838787802396666}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6666666666666667, "accuracy_n": 59, "auc": 0.6666666666666667}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8375125376128385, "accuracy_n": 997, "auc": 0.8375125376128385}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.727364185110664, "accuracy_n": 994, "auc": 0.727364185110664}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 994, "auc": 0.5553319919517102}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.39879759519038077, "accuracy_n": 499, "auc": 0.39879759519038077}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.545271629778672, "accuracy_n": 497, "auc": 0.545271629778672}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.40468227424749165, "accuracy_n": 299, "auc": 0.40468227424749165}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.44, "accuracy_n": 400, "auc": 0.44}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.933993399339934, "accuracy_n": 303, "auc": 0.933993399339934}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9962928637627433, "accuracy_n": 322, "auc": 0.9962928637627433}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5052161654135339, "accuracy_n": 292, "auc": 0.5052161654135339}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9539951573849879, "accuracy_n": 413, "auc": 0.9539951573849879}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6054184359518755, "accuracy_n": 1902, "auc": 0.6054184359518755}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6445966994084864, "accuracy_n": 2000, "auc": 0.6445966994084864}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7016806722689075, "accuracy_n": 59, "auc": 0.7016806722689075}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8615384615384615, "accuracy_n": 23, "auc": 0.8615384615384615}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.79839518555667, "accuracy_n": 997, "auc": 0.79839518555667}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7354124748490946, "accuracy_n": 994, "auc": 0.7354124748490946}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6534296028880866, "accuracy_n": 277, "auc": 0.6534296028880866}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.53420523138833, "accuracy_n": 994, "auc": 0.53420523138833}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5150300601202404, "accuracy_n": 499, "auc": 0.5150300601202404}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5251509054325956, "accuracy_n": 497, "auc": 0.5251509054325956}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5351170568561873, "accuracy_n": 299, "auc": 0.5351170568561873}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4875, "accuracy_n": 400, "auc": 0.4875}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9306930693069307, "accuracy_n": 303, "auc": 0.9306930693069307}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9897281433426012, "accuracy_n": 322, "auc": 0.9897281433426012}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.587546992481203, "accuracy_n": 292, "auc": 0.587546992481203}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8305084745762712, "accuracy_n": 413, "auc": 0.8305084745762712}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.65794961960368, "accuracy_n": 1902, "auc": 0.65794961960368}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6590649224369997, "accuracy_n": 2000, "auc": 0.6590649224369997}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.680672268907563, "accuracy_n": 59, "auc": 0.680672268907563}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9027081243731193, "accuracy_n": 997, "auc": 0.9027081243731193}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8712273641851107, "accuracy_n": 994, "auc": 0.8712273641851107}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6137184115523465, "accuracy_n": 277, "auc": 0.6137184115523465}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5261569416498993, "accuracy_n": 994, "auc": 0.5261569416498993}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5250501002004008, "accuracy_n": 499, "auc": 0.5250501002004008}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5311871227364185, "accuracy_n": 497, "auc": 0.5311871227364185}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4983277591973244, "accuracy_n": 299, "auc": 0.4983277591973244}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5875, "accuracy_n": 400, "auc": 0.5875}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9174917491749175, "accuracy_n": 303, "auc": 0.9174917491749175}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9960611677479148, "accuracy_n": 322, "auc": 0.9960611677479148}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8584586466165414, "accuracy_n": 292, "auc": 0.8584586466165414}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5343683651804672, "accuracy_n": 1902, "auc": 0.5343683651804672}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5598220957765754, "accuracy_n": 2000, "auc": 0.5598220957765754}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.819327731092437, "accuracy_n": 59, "auc": 0.819327731092437}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5076923076923077, "accuracy_n": 23, "auc": 0.5076923076923077}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.753259779338014, "accuracy_n": 997, "auc": 0.753259779338014}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6881287726358148, "accuracy_n": 994, "auc": 0.6881287726358148}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5945674044265593, "accuracy_n": 994, "auc": 0.5945674044265593}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4849699398797595, "accuracy_n": 499, "auc": 0.4849699398797595}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.43812709030100333, "accuracy_n": 299, "auc": 0.43812709030100333}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 400, "auc": 0.31}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6237623762376238, "accuracy_n": 303, "auc": 0.6237623762376238}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9848239110287302, "accuracy_n": 322, "auc": 0.9848239110287302}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9377819548872179, "accuracy_n": 292, "auc": 0.9377819548872179}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7360774818401937, "accuracy_n": 413, "auc": 0.7360774818401937}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.506359474522293, "accuracy_n": 1902, "auc": 0.506359474522293}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.544038397861628, "accuracy_n": 2000, "auc": 0.544038397861628}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7633053221288516, "accuracy_n": 59, "auc": 0.7633053221288516}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6599799398194583, "accuracy_n": 997, "auc": 0.6599799398194583}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.607645875251509, "accuracy_n": 994, "auc": 0.607645875251509}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5090252707581228, "accuracy_n": 277, "auc": 0.5090252707581228}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5563380281690141, "accuracy_n": 994, "auc": 0.5563380281690141}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3727454909819639, "accuracy_n": 499, "auc": 0.3727454909819639}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5492957746478874, "accuracy_n": 497, "auc": 0.5492957746478874}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.45484949832775917, "accuracy_n": 299, "auc": 0.45484949832775917}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.325, "accuracy_n": 400, "auc": 0.325}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8547854785478548, "accuracy_n": 303, "auc": 0.8547854785478548}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9847466790237874, "accuracy_n": 322, "auc": 0.9847466790237874}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5564849624060151, "accuracy_n": 292, "auc": 0.5564849624060151}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5190983280254777, "accuracy_n": 1902, "auc": 0.5190983280254777}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6506298773857362, "accuracy_n": 2000, "auc": 0.6506298773857362}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.669467787114846, "accuracy_n": 59, "auc": 0.669467787114846}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9147442326980942, "accuracy_n": 997, "auc": 0.9147442326980942}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.693158953722334, "accuracy_n": 994, "auc": 0.693158953722334}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6389891696750902, "accuracy_n": 277, "auc": 0.6389891696750902}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3046092184368738, "accuracy_n": 499, "auc": 0.3046092184368738}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.45472837022132795, "accuracy_n": 497, "auc": 0.45472837022132795}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.445, "accuracy_n": 400, "auc": 0.445}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6204620462046204, "accuracy_n": 303, "auc": 0.6204620462046204}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7215400061785604, "accuracy_n": 322, "auc": 0.7215400061785604}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6457706766917293, "accuracy_n": 292, "auc": 0.6457706766917293}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7021791767554479, "accuracy_n": 413, "auc": 0.7021791767554479}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9314357749469215, "accuracy_n": 1902, "auc": 0.9314357749469215}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5031536384634854, "accuracy_n": 2000, "auc": 0.5031536384634854}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6624649859943978, "accuracy_n": 59, "auc": 0.6624649859943978}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6038114343029087, "accuracy_n": 997, "auc": 0.6038114343029087}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5050301810865191, "accuracy_n": 994, "auc": 0.5050301810865191}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5503018108651911, "accuracy_n": 994, "auc": 0.5503018108651911}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3627254509018036, "accuracy_n": 499, "auc": 0.3627254509018036}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 497, "auc": 0.5392354124748491}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3712374581939799, "accuracy_n": 299, "auc": 0.3712374581939799}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.43, "accuracy_n": 400, "auc": 0.43}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8250825082508251, "accuracy_n": 303, "auc": 0.8250825082508251}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6602564102564101, "accuracy_n": 322, "auc": 0.6602564102564101}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7194078947368422, "accuracy_n": 292, "auc": 0.7194078947368422}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9176755447941889, "accuracy_n": 413, "auc": 0.9176755447941889}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5065286624203822, "accuracy_n": 1902, "auc": 0.5065286624203822}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6414675697926951, "accuracy_n": 2000, "auc": 0.6414675697926951}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7769230769230769, "accuracy_n": 23, "auc": 0.7769230769230769}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.880641925777332, "accuracy_n": 997, "auc": 0.880641925777332}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6720321931589537, "accuracy_n": 994, "auc": 0.6720321931589537}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5050301810865191, "accuracy_n": 994, "auc": 0.5050301810865191}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5210420841683366, "accuracy_n": 499, "auc": 0.5210420841683366}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6317907444668008, "accuracy_n": 497, "auc": 0.6317907444668008}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5317725752508361, "accuracy_n": 299, "auc": 0.5317725752508361}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 400, "auc": 0.56}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8877887788778878, "accuracy_n": 303, "auc": 0.8877887788778878}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.992431263515601, "accuracy_n": 322, "auc": 0.992431263515601}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5825187969924812, "accuracy_n": 292, "auc": 0.5825187969924812}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9806295399515739, "accuracy_n": 413, "auc": 0.9806295399515739}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.692228414720453, "accuracy_n": 1902, "auc": 0.692228414720453}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6705820801309272, "accuracy_n": 2000, "auc": 0.6705820801309272}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7436974789915967, "accuracy_n": 59, "auc": 0.7436974789915967}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6429287863590772, "accuracy_n": 997, "auc": 0.6429287863590772}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7173038229376257, "accuracy_n": 994, "auc": 0.7173038229376257}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.29458917835671344, "accuracy_n": 499, "auc": 0.29458917835671344}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4788732394366197, "accuracy_n": 497, "auc": 0.4788732394366197}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3745819397993311, "accuracy_n": 299, "auc": 0.3745819397993311}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4475, "accuracy_n": 400, "auc": 0.4475}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9207920792079208, "accuracy_n": 303, "auc": 0.9207920792079208}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9474050046339203, "accuracy_n": 322, "auc": 0.9474050046339203}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6555451127819548, "accuracy_n": 292, "auc": 0.6555451127819548}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.784503631961259, "accuracy_n": 413, "auc": 0.784503631961259}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5172062986553432, "accuracy_n": 1902, "auc": 0.5172062986553432}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5864987260401006, "accuracy_n": 2000, "auc": 0.5864987260401006}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6722689075630253, "accuracy_n": 59, "auc": 0.6722689075630253}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
