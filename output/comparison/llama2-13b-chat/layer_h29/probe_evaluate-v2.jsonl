{"key": "result_0", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_1", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.920523138832998, "accuracy_n": 994, "auc": 0.920523138832998}}
{"key": "result_2", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7075812274368231, "accuracy_n": 277, "auc": 0.7075812274368231}}
{"key": "result_3", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_4", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.823943661971831, "accuracy_n": 994, "auc": 0.823943661971831}}
{"key": "result_5", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.48697394789579157, "accuracy_n": 499, "auc": 0.48697394789579157}}
{"key": "result_6", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6498993963782697, "accuracy_n": 497, "auc": 0.6498993963782697}}
{"key": "result_7", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5886287625418061, "accuracy_n": 299, "auc": 0.5886287625418061}}
{"key": "result_8", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6125, "accuracy_n": 400, "auc": 0.6125}}
{"key": "result_9", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9273927392739274, "accuracy_n": 303, "auc": 0.9273927392739274}}
{"key": "result_10", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_11", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9923926475131295, "accuracy_n": 322, "auc": 0.9923926475131295}}
{"key": "result_12", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6495770676691729, "accuracy_n": 292, "auc": 0.6495770676691729}}
{"key": "result_13", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9757869249394673, "accuracy_n": 413, "auc": 0.9757869249394673}}
{"key": "result_14", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6924119780608635, "accuracy_n": 1902, "auc": 0.6924119780608635}}
{"key": "result_15", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6422988698920311, "accuracy_n": 2000, "auc": 0.6422988698920311}}
{"key": "result_16", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7801120448179272, "accuracy_n": 59, "auc": 0.7801120448179272}}
{"key": "result_17", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_18", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9287863590772317, "accuracy_n": 997, "auc": 0.9287863590772317}}
{"key": "result_19", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9396378269617707, "accuracy_n": 994, "auc": 0.9396378269617707}}
{"key": "result_20", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6570397111913358, "accuracy_n": 277, "auc": 0.6570397111913358}}
{"key": "result_21", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_22", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6881287726358148, "accuracy_n": 994, "auc": 0.6881287726358148}}
{"key": "result_23", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.49498997995991983, "accuracy_n": 499, "auc": 0.49498997995991983}}
{"key": "result_24", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6237424547283702, "accuracy_n": 497, "auc": 0.6237424547283702}}
{"key": "result_25", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5551839464882943, "accuracy_n": 299, "auc": 0.5551839464882943}}
{"key": "result_26", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 400, "auc": 0.61}}
{"key": "result_27", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8811881188118812, "accuracy_n": 303, "auc": 0.8811881188118812}}
{"key": "result_28", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_29", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9816573988260735, "accuracy_n": 322, "auc": 0.9816573988260735}}
{"key": "result_30", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8534304511278197, "accuracy_n": 292, "auc": 0.8534304511278197}}
{"key": "result_31", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.837772397094431, "accuracy_n": 413, "auc": 0.837772397094431}}
{"key": "result_32", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6508315640481246, "accuracy_n": 1902, "auc": 0.6508315640481246}}
{"key": "result_33", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5778806149019796, "accuracy_n": 2000, "auc": 0.5778806149019796}}
{"key": "result_34", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7591036414565826, "accuracy_n": 59, "auc": 0.7591036414565826}}
{"key": "result_35", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7923076923076924, "accuracy_n": 23, "auc": 0.7923076923076924}}
{"key": "result_36", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9438314944834504, "accuracy_n": 997, "auc": 0.9438314944834504}}
{"key": "result_37", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9175050301810865, "accuracy_n": 994, "auc": 0.9175050301810865}}
{"key": "result_38", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.740072202166065, "accuracy_n": 277, "auc": 0.740072202166065}}
{"key": "result_39", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_40", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.806841046277666, "accuracy_n": 994, "auc": 0.806841046277666}}
{"key": "result_41", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4749498997995992, "accuracy_n": 499, "auc": 0.4749498997995992}}
{"key": "result_42", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6338028169014085, "accuracy_n": 497, "auc": 0.6338028169014085}}
{"key": "result_43", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.47157190635451507, "accuracy_n": 299, "auc": 0.47157190635451507}}
{"key": "result_44", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5425, "accuracy_n": 400, "auc": 0.5425}}
{"key": "result_45", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6897689768976898, "accuracy_n": 303, "auc": 0.6897689768976898}}
{"key": "result_46", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_47", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9720033982082175, "accuracy_n": 322, "auc": 0.9720033982082175}}
{"key": "result_48", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6142387218045113, "accuracy_n": 292, "auc": 0.6142387218045113}}
{"key": "result_49", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5932203389830508, "accuracy_n": 413, "auc": 0.5932203389830508}}
{"key": "result_50", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5892372169143666, "accuracy_n": 1902, "auc": 0.5892372169143666}}
{"key": "result_51", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.538583428617731, "accuracy_n": 2000, "auc": 0.538583428617731}}
{"key": "result_52", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7030812324929971, "accuracy_n": 59, "auc": 0.7030812324929971}}
{"key": "result_53", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_54", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9307923771313942, "accuracy_n": 997, "auc": 0.9307923771313942}}
{"key": "result_55", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8350100603621731, "accuracy_n": 994, "auc": 0.8350100603621731}}
{"key": "result_56", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6534296028880866, "accuracy_n": 277, "auc": 0.6534296028880866}}
{"key": "result_57", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_58", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8209255533199196, "accuracy_n": 994, "auc": 0.8209255533199196}}
{"key": "result_59", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5771543086172345, "accuracy_n": 499, "auc": 0.5771543086172345}}
{"key": "result_60", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_61", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5719063545150501, "accuracy_n": 299, "auc": 0.5719063545150501}}
{"key": "result_62", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6425, "accuracy_n": 400, "auc": 0.6425}}
{"key": "result_63", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9075907590759076, "accuracy_n": 303, "auc": 0.9075907590759076}}
{"key": "result_64", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_65", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5580012357120792, "accuracy_n": 322, "auc": 0.5580012357120792}}
{"key": "result_66", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5452067669172932, "accuracy_n": 292, "auc": 0.5452067669172932}}
{"key": "result_67", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8401937046004843, "accuracy_n": 413, "auc": 0.8401937046004843}}
{"key": "result_68", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.529058850849257, "accuracy_n": 1902, "auc": 0.529058850849257}}
{"key": "result_69", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6129312681878158, "accuracy_n": 2000, "auc": 0.6129312681878158}}
{"key": "result_70", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6050420168067228, "accuracy_n": 59, "auc": 0.6050420168067228}}
{"key": "result_71", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8615384615384616, "accuracy_n": 23, "auc": 0.8615384615384616}}
{"key": "result_72", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8786359077231695, "accuracy_n": 997, "auc": 0.8786359077231695}}
{"key": "result_73", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8913480885311871, "accuracy_n": 994, "auc": 0.8913480885311871}}
{"key": "result_74", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_75", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_76", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5711422845691383, "accuracy_n": 499, "auc": 0.5711422845691383}}
{"key": "result_78", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.635814889336016, "accuracy_n": 497, "auc": 0.635814889336016}}
{"key": "result_79", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5585284280936454, "accuracy_n": 299, "auc": 0.5585284280936454}}
{"key": "result_80", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.68, "accuracy_n": 400, "auc": 0.68}}
{"key": "result_81", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9578313253012049, "accuracy_n": 322, "auc": 0.9578313253012049}}
{"key": "result_84", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.530451127819549, "accuracy_n": 292, "auc": 0.530451127819549}}
{"key": "result_85", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8547215496368039, "accuracy_n": 413, "auc": 0.8547215496368039}}
{"key": "result_86", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6200504246284502, "accuracy_n": 1902, "auc": 0.6200504246284502}}
{"key": "result_87", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.519351485886405, "accuracy_n": 2000, "auc": 0.519351485886405}}
{"key": "result_88", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6946778711484594, "accuracy_n": 59, "auc": 0.6946778711484594}}
{"key": "result_89", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_90", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8465396188565697, "accuracy_n": 997, "auc": 0.8465396188565697}}
{"key": "result_91", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6408450704225352, "accuracy_n": 994, "auc": 0.6408450704225352}}
{"key": "result_92", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_93", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_94", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7977867203219315, "accuracy_n": 994, "auc": 0.7977867203219315}}
{"key": "result_95", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6232464929859719, "accuracy_n": 499, "auc": 0.6232464929859719}}
{"key": "result_96", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6579476861167002, "accuracy_n": 497, "auc": 0.6579476861167002}}
{"key": "result_97", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6354515050167224, "accuracy_n": 299, "auc": 0.6354515050167224}}
{"key": "result_98", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 400, "auc": 0.71}}
{"key": "result_99", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6072607260726073, "accuracy_n": 303, "auc": 0.6072607260726073}}
{"key": "result_100", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9811940067964164, "accuracy_n": 322, "auc": 0.9811940067964164}}
{"key": "result_102", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8468045112781956, "accuracy_n": 292, "auc": 0.8468045112781956}}
{"key": "result_103", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6755447941888619, "accuracy_n": 413, "auc": 0.6755447941888619}}
{"key": "result_104", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5294563871196036, "accuracy_n": 1902, "auc": 0.5294563871196036}}
{"key": "result_105", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5509498929113409, "accuracy_n": 2000, "auc": 0.5509498929113409}}
{"key": "result_106", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.823529411764706, "accuracy_n": 59, "auc": 0.823529411764706}}
{"key": "result_107", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_108", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9227683049147443, "accuracy_n": 997, "auc": 0.9227683049147443}}
{"key": "result_109", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7293762575452716, "accuracy_n": 994, "auc": 0.7293762575452716}}
{"key": "result_110", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6425992779783394, "accuracy_n": 277, "auc": 0.6425992779783394}}
{"key": "result_111", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_112", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7947686116700201, "accuracy_n": 994, "auc": 0.7947686116700201}}
{"key": "result_113", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_114", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6680080482897385, "accuracy_n": 497, "auc": 0.6680080482897385}}
{"key": "result_115", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6387959866220736, "accuracy_n": 299, "auc": 0.6387959866220736}}
{"key": "result_116", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6975, "accuracy_n": 400, "auc": 0.6975}}
{"key": "result_117", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6171617161716172, "accuracy_n": 303, "auc": 0.6171617161716172}}
{"key": "result_118", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9548192771084337, "accuracy_n": 322, "auc": 0.9548192771084337}}
{"key": "result_120", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5038533834586466, "accuracy_n": 292, "auc": 0.5038533834586466}}
{"key": "result_121", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8256658595641646, "accuracy_n": 413, "auc": 0.8256658595641646}}
{"key": "result_122", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5091372523000708, "accuracy_n": 1902, "auc": 0.5091372523000708}}
{"key": "result_123", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6146758979991777, "accuracy_n": 2000, "auc": 0.6146758979991777}}
{"key": "result_124", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_125", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_126", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9247743229689067, "accuracy_n": 997, "auc": 0.9247743229689067}}
{"key": "result_127", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.682092555331992, "accuracy_n": 994, "auc": 0.682092555331992}}
{"key": "result_128", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_130", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.789738430583501, "accuracy_n": 994, "auc": 0.789738430583501}}
{"key": "result_131", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6112224448897795, "accuracy_n": 499, "auc": 0.6112224448897795}}
{"key": "result_132", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6639839034205232, "accuracy_n": 497, "auc": 0.6639839034205232}}
{"key": "result_133", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6421404682274248, "accuracy_n": 299, "auc": 0.6421404682274248}}
{"key": "result_134", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7075, "accuracy_n": 400, "auc": 0.7075}}
{"key": "result_135", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6138613861386139, "accuracy_n": 303, "auc": 0.6138613861386139}}
{"key": "result_136", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_137", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9834723509422305, "accuracy_n": 322, "auc": 0.9834723509422305}}
{"key": "result_138", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8218515037593984, "accuracy_n": 292, "auc": 0.8218515037593984}}
{"key": "result_139", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7070217917675545, "accuracy_n": 413, "auc": 0.7070217917675545}}
{"key": "result_140", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5091936482661005, "accuracy_n": 1902, "auc": 0.5091936482661005}}
{"key": "result_141", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6263291048068352, "accuracy_n": 2000, "auc": 0.6263291048068352}}
{"key": "result_142", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8571428571428572, "accuracy_n": 59, "auc": 0.8571428571428572}}
{"key": "result_143", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_144", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9317953861584755, "accuracy_n": 997, "auc": 0.9317953861584755}}
{"key": "result_145", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.727364185110664, "accuracy_n": 994, "auc": 0.727364185110664}}
{"key": "result_146", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_148", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7776659959758552, "accuracy_n": 994, "auc": 0.7776659959758552}}
{"key": "result_149", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_150", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_151", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6321070234113713, "accuracy_n": 299, "auc": 0.6321070234113713}}
{"key": "result_152", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6975, "accuracy_n": 400, "auc": 0.6975}}
{"key": "result_153", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_154", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9430027803521779, "accuracy_n": 322, "auc": 0.9430027803521779}}
{"key": "result_156", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6289943609022557, "accuracy_n": 292, "auc": 0.6289943609022557}}
{"key": "result_157", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5544794188861986, "accuracy_n": 413, "auc": 0.5544794188861986}}
{"key": "result_158", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5304693029016279, "accuracy_n": 1902, "auc": 0.5304693029016279}}
{"key": "result_159", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6171627957692727, "accuracy_n": 2000, "auc": 0.6171627957692727}}
{"key": "result_160", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6484593837535014, "accuracy_n": 59, "auc": 0.6484593837535014}}
{"key": "result_161", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_162", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7382146439317954, "accuracy_n": 997, "auc": 0.7382146439317954}}
{"key": "result_163", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6901408450704225, "accuracy_n": 994, "auc": 0.6901408450704225}}
{"key": "result_164", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_165", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.63, "accuracy_n": 100, "auc": 0.63}}
{"key": "result_166", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6257545271629779, "accuracy_n": 994, "auc": 0.6257545271629779}}
{"key": "result_167", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_168", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 497, "auc": 0.5553319919517102}}
{"key": "result_169", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4214046822742475, "accuracy_n": 299, "auc": 0.4214046822742475}}
{"key": "result_170", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.39, "accuracy_n": 400, "auc": 0.39}}
{"key": "result_171", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9306930693069307, "accuracy_n": 303, "auc": 0.9306930693069307}}
{"key": "result_172", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_173", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9949799196787148, "accuracy_n": 322, "auc": 0.9949799196787148}}
{"key": "result_174", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5923872180451129, "accuracy_n": 292, "auc": 0.5923872180451129}}
{"key": "result_175", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8861985472154964, "accuracy_n": 413, "auc": 0.8861985472154964}}
{"key": "result_176", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5452561040339703, "accuracy_n": 1902, "auc": 0.5452561040339703}}
{"key": "result_177", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6425339547576675, "accuracy_n": 2000, "auc": 0.6425339547576675}}
{"key": "result_178", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6946778711484594, "accuracy_n": 59, "auc": 0.6946778711484594}}
{"key": "result_179", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8538461538461538, "accuracy_n": 23, "auc": 0.8538461538461538}}
{"key": "result_180", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8465396188565697, "accuracy_n": 997, "auc": 0.8465396188565697}}
{"key": "result_181", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7847082494969819, "accuracy_n": 994, "auc": 0.7847082494969819}}
{"key": "result_182", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6498194945848376, "accuracy_n": 277, "auc": 0.6498194945848376}}
{"key": "result_183", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 100, "auc": 0.62}}
{"key": "result_184", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5875251509054326, "accuracy_n": 994, "auc": 0.5875251509054326}}
{"key": "result_185", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3286573146292585, "accuracy_n": 499, "auc": 0.3286573146292585}}
{"key": "result_186", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.26156941649899396, "accuracy_n": 497, "auc": 0.26156941649899396}}
{"key": "result_187", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3411371237458194, "accuracy_n": 299, "auc": 0.3411371237458194}}
{"key": "result_188", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.27, "accuracy_n": 400, "auc": 0.27}}
{"key": "result_189", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9372937293729373, "accuracy_n": 303, "auc": 0.9372937293729373}}
{"key": "result_190", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9931263515600866, "accuracy_n": 322, "auc": 0.9931263515600866}}
{"key": "result_192", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5872885338345866, "accuracy_n": 292, "auc": 0.5872885338345866}}
{"key": "result_193", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8426150121065376, "accuracy_n": 413, "auc": 0.8426150121065376}}
{"key": "result_194", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5997744161358811, "accuracy_n": 1902, "auc": 0.5997744161358811}}
{"key": "result_195", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6668997508100424, "accuracy_n": 2000, "auc": 0.6668997508100424}}
{"key": "result_196", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6792717086834734, "accuracy_n": 59, "auc": 0.6792717086834734}}
{"key": "result_197", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_198", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9097291875626881, "accuracy_n": 997, "auc": 0.9097291875626881}}
{"key": "result_199", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8742454728370221, "accuracy_n": 994, "auc": 0.8742454728370221}}
{"key": "result_200", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_201", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_203", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5130260521042084, "accuracy_n": 499, "auc": 0.5130260521042084}}
{"key": "result_204", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5513078470824949, "accuracy_n": 497, "auc": 0.5513078470824949}}
{"key": "result_205", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 299, "auc": 0.5384615384615384}}
{"key": "result_206", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 400, "auc": 0.6}}
{"key": "result_207", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8613861386138614, "accuracy_n": 303, "auc": 0.8613861386138614}}
{"key": "result_208", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_209", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9971037998146433, "accuracy_n": 322, "auc": 0.9971037998146433}}
{"key": "result_210", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8656484962406015, "accuracy_n": 292, "auc": 0.8656484962406015}}
{"key": "result_211", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9782082324455206, "accuracy_n": 413, "auc": 0.9782082324455206}}
{"key": "result_212", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5006878096249114, "accuracy_n": 1902, "auc": 0.5006878096249114}}
{"key": "result_213", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5866987982661741, "accuracy_n": 2000, "auc": 0.5866987982661741}}
{"key": "result_214", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7492997198879552, "accuracy_n": 59, "auc": 0.7492997198879552}}
{"key": "result_215", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_216", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6118355065195586, "accuracy_n": 997, "auc": 0.6118355065195586}}
{"key": "result_217", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6388329979879276, "accuracy_n": 994, "auc": 0.6388329979879276}}
{"key": "result_218", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_219", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_220", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_221", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.48296593186372744, "accuracy_n": 499, "auc": 0.48296593186372744}}
{"key": "result_222", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2837022132796781, "accuracy_n": 497, "auc": 0.2837022132796781}}
{"key": "result_223", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4782608695652174, "accuracy_n": 299, "auc": 0.4782608695652174}}
{"key": "result_224", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3975, "accuracy_n": 400, "auc": 0.3975}}
{"key": "result_225", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.504950495049505, "accuracy_n": 303, "auc": 0.504950495049505}}
{"key": "result_226", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_227", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9839743589743589, "accuracy_n": 322, "auc": 0.9839743589743589}}
{"key": "result_228", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9385808270676691, "accuracy_n": 292, "auc": 0.9385808270676691}}
{"key": "result_229", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6658595641646489, "accuracy_n": 413, "auc": 0.6658595641646489}}
{"key": "result_230", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5122677813163482, "accuracy_n": 1902, "auc": 0.5122677813163482}}
{"key": "result_231", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5510609329968118, "accuracy_n": 2000, "auc": 0.5510609329968118}}
{"key": "result_232", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7394957983193278, "accuracy_n": 59, "auc": 0.7394957983193278}}
{"key": "result_233", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_234", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6148445336008024, "accuracy_n": 997, "auc": 0.6148445336008024}}
{"key": "result_235", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6438631790744467, "accuracy_n": 994, "auc": 0.6438631790744467}}
{"key": "result_236", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_237", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_238", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6086519114688129, "accuracy_n": 994, "auc": 0.6086519114688129}}
{"key": "result_239", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.47695390781563124, "accuracy_n": 499, "auc": 0.47695390781563124}}
{"key": "result_240", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6056338028169014, "accuracy_n": 497, "auc": 0.6056338028169014}}
{"key": "result_241", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5150501672240803, "accuracy_n": 299, "auc": 0.5150501672240803}}
{"key": "result_242", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4225, "accuracy_n": 400, "auc": 0.4225}}
{"key": "result_243", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9141914191419142, "accuracy_n": 303, "auc": 0.9141914191419142}}
{"key": "result_244", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_245", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9892261353104727, "accuracy_n": 322, "auc": 0.9892261353104727}}
{"key": "result_246", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5165883458646617, "accuracy_n": 292, "auc": 0.5165883458646617}}
{"key": "result_247", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_248", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5255728060863412, "accuracy_n": 1902, "auc": 0.5255728060863412}}
{"key": "result_249", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6532828351034724, "accuracy_n": 2000, "auc": 0.6532828351034724}}
{"key": "result_250", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6246498599439776, "accuracy_n": 59, "auc": 0.6246498599439776}}
{"key": "result_251", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8076923076923076, "accuracy_n": 23, "auc": 0.8076923076923076}}
{"key": "result_252", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9358074222668004, "accuracy_n": 997, "auc": 0.9358074222668004}}
{"key": "result_253", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.704225352112676, "accuracy_n": 994, "auc": 0.704225352112676}}
{"key": "result_254", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_255", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7565392354124748, "accuracy_n": 994, "auc": 0.7565392354124748}}
{"key": "result_257", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_258", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3702213279678068, "accuracy_n": 497, "auc": 0.3702213279678068}}
{"key": "result_259", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.31438127090301005, "accuracy_n": 299, "auc": 0.31438127090301005}}
{"key": "result_260", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.485, "accuracy_n": 400, "auc": 0.485}}
{"key": "result_261", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9108910891089109, "accuracy_n": 303, "auc": 0.9108910891089109}}
{"key": "result_262", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_263", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5054448563484708, "accuracy_n": 322, "auc": 0.5054448563484708}}
{"key": "result_264", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7166823308270677, "accuracy_n": 292, "auc": 0.7166823308270677}}
{"key": "result_265", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9661016949152542, "accuracy_n": 413, "auc": 0.9661016949152542}}
{"key": "result_266", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.983430644019816, "accuracy_n": 1902, "auc": 0.983430644019816}}
{"key": "result_267", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6110745979298527, "accuracy_n": 2000, "auc": 0.6110745979298527}}
{"key": "result_268", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_269", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_270", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6328986960882648, "accuracy_n": 997, "auc": 0.6328986960882648}}
{"key": "result_271", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5734406438631791, "accuracy_n": 994, "auc": 0.5734406438631791}}
{"key": "result_272", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_273", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 100, "auc": 0.7}}
{"key": "result_274", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5241448692152918, "accuracy_n": 994, "auc": 0.5241448692152918}}
{"key": "result_275", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3286573146292585, "accuracy_n": 499, "auc": 0.3286573146292585}}
{"key": "result_276", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5472837022132797, "accuracy_n": 497, "auc": 0.5472837022132797}}
{"key": "result_277", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.34782608695652173, "accuracy_n": 299, "auc": 0.34782608695652173}}
{"key": "result_278", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4475, "accuracy_n": 400, "auc": 0.4475}}
{"key": "result_279", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8844884488448845, "accuracy_n": 303, "auc": 0.8844884488448845}}
{"key": "result_280", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_281", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.73671609514983, "accuracy_n": 322, "auc": 0.73671609514983}}
{"key": "result_282", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.737640977443609, "accuracy_n": 292, "auc": 0.737640977443609}}
{"key": "result_283", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9079903147699758, "accuracy_n": 413, "auc": 0.9079903147699758}}
{"key": "result_284", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342024946921444, "accuracy_n": 1902, "auc": 0.5342024946921444}}
{"key": "result_285", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6507859337220737, "accuracy_n": 2000, "auc": 0.6507859337220737}}
{"key": "result_286", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5812324929971988, "accuracy_n": 59, "auc": 0.5812324929971988}}
{"key": "result_287", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
{"key": "result_288", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7893681043129388, "accuracy_n": 997, "auc": 0.7893681043129388}}
{"key": "result_289", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6800804828973843, "accuracy_n": 994, "auc": 0.6800804828973843}}
{"key": "result_290", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_291", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_292", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6629778672032193, "accuracy_n": 994, "auc": 0.6629778672032193}}
{"key": "result_293", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.49298597194388777, "accuracy_n": 499, "auc": 0.49298597194388777}}
{"key": "result_294", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6277665995975855, "accuracy_n": 497, "auc": 0.6277665995975855}}
{"key": "result_295", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5217391304347826, "accuracy_n": 299, "auc": 0.5217391304347826}}
{"key": "result_296", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5375, "accuracy_n": 400, "auc": 0.5375}}
{"key": "result_297", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8712871287128713, "accuracy_n": 303, "auc": 0.8712871287128713}}
{"key": "result_298", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9942462156317577, "accuracy_n": 322, "auc": 0.9942462156317577}}
{"key": "result_300", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5200657894736842, "accuracy_n": 292, "auc": 0.5200657894736842}}
{"key": "result_301", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_302", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.615264508138712, "accuracy_n": 1902, "auc": 0.615264508138712}}
{"key": "result_303", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6818141349026998, "accuracy_n": 2000, "auc": 0.6818141349026998}}
{"key": "result_304", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7450980392156863, "accuracy_n": 59, "auc": 0.7450980392156863}}
{"key": "result_305", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_306", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5586760280842528, "accuracy_n": 997, "auc": 0.5586760280842528}}
{"key": "result_307", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7655935613682092, "accuracy_n": 994, "auc": 0.7655935613682092}}
{"key": "result_308", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.64, "accuracy_n": 100, "auc": 0.64}}
{"key": "result_310", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5955734406438632, "accuracy_n": 994, "auc": 0.5955734406438632}}
{"key": "result_311", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2865731462925852, "accuracy_n": 499, "auc": 0.2865731462925852}}
{"key": "result_312", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4969818913480885, "accuracy_n": 497, "auc": 0.4969818913480885}}
{"key": "result_313", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3010033444816054, "accuracy_n": 299, "auc": 0.3010033444816054}}
{"key": "result_314", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4125, "accuracy_n": 400, "auc": 0.4125}}
{"key": "result_315", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_316", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_317", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9587194933580476, "accuracy_n": 322, "auc": 0.9587194933580476}}
{"key": "result_318", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7178571428571429, "accuracy_n": 292, "auc": 0.7178571428571429}}
{"key": "result_319", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7699757869249395, "accuracy_n": 413, "auc": 0.7699757869249395}}
{"key": "result_320", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5186261500353857, "accuracy_n": 1902, "auc": 0.5186261500353857}}
{"key": "result_321", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6045412393874189, "accuracy_n": 2000, "auc": 0.6045412393874189}}
{"key": "result_322", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6694677871148459, "accuracy_n": 59, "auc": 0.6694677871148459}}
{"key": "result_323", "value": {"llm_id": "Llama-2-13b-chat-hf", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9461538461538461, "accuracy_n": 23, "auc": 0.9461538461538461}}
