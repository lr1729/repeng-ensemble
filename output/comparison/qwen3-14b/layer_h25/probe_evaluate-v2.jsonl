{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9427710843373494, "accuracy_n": 996, "auc": 0.9427710843373494}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7535211267605634, "accuracy_n": 994, "auc": 0.7535211267605634}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6659959758551308, "accuracy_n": 497, "auc": 0.6659959758551308}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7254509018036072, "accuracy_n": 499, "auc": 0.7254509018036072}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8657718120805369, "accuracy_n": 298, "auc": 0.8657718120805369}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7819548872180451, "accuracy_n": 399, "auc": 0.7819548872180451}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9957136237256718, "accuracy_n": 322, "auc": 0.9957136237256718}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8403665413533834, "accuracy_n": 292, "auc": 0.8403665413533834}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7890713464260438, "accuracy_n": 1902, "auc": 0.7890713464260438}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7628203781565145, "accuracy_n": 2000, "auc": 0.7628203781565145}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9971988795518207, "accuracy_n": 59, "auc": 0.9971988795518207}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9615384615384615, "accuracy_n": 23, "auc": 0.9615384615384615}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9417670682730924, "accuracy_n": 996, "auc": 0.9417670682730924}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9454545454545454, "accuracy_n": 990, "auc": 0.9454545454545454}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7303822937625755, "accuracy_n": 994, "auc": 0.7303822937625755}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6981891348088531, "accuracy_n": 497, "auc": 0.6981891348088531}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7334669338677354, "accuracy_n": 499, "auc": 0.7334669338677354}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.87248322147651, "accuracy_n": 298, "auc": 0.87248322147651}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9947096076614148, "accuracy_n": 322, "auc": 0.9947096076614148}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8795582706766918, "accuracy_n": 292, "auc": 0.8795582706766918}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8259255573248407, "accuracy_n": 1902, "auc": 0.8259255573248407}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7935154590807282, "accuracy_n": 2000, "auc": 0.7935154590807282}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9985994397759104, "accuracy_n": 59, "auc": 0.9985994397759104}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9417670682730924, "accuracy_n": 996, "auc": 0.9417670682730924}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9505050505050505, "accuracy_n": 990, "auc": 0.9505050505050505}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7585513078470825, "accuracy_n": 994, "auc": 0.7585513078470825}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5472837022132797, "accuracy_n": 497, "auc": 0.5472837022132797}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5651302605210421, "accuracy_n": 499, "auc": 0.5651302605210421}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7281879194630873, "accuracy_n": 298, "auc": 0.7281879194630873}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7368421052631579, "accuracy_n": 399, "auc": 0.7368421052631579}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9998841519925857, "accuracy_n": 322, "auc": 0.9998841519925857}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9083176691729323, "accuracy_n": 292, "auc": 0.9083176691729323}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8185454263977354, "accuracy_n": 1902, "auc": 0.8185454263977354}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.817362067706442, "accuracy_n": 2000, "auc": 0.817362067706442}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9775910364145658, "accuracy_n": 59, "auc": 0.9775910364145658}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9692307692307692, "accuracy_n": 23, "auc": 0.9692307692307692}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7877263581488934, "accuracy_n": 994, "auc": 0.7877263581488934}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7545271629778671, "accuracy_n": 497, "auc": 0.7545271629778671}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7955911823647295, "accuracy_n": 499, "auc": 0.7955911823647295}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9161073825503355, "accuracy_n": 298, "auc": 0.9161073825503355}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8295739348370927, "accuracy_n": 399, "auc": 0.8295739348370927}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9993049119555144, "accuracy_n": 322, "auc": 0.9993049119555144}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9175751879699248, "accuracy_n": 292, "auc": 0.9175751879699248}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8262274416135881, "accuracy_n": 1902, "auc": 0.8262274416135881}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7635556435873351, "accuracy_n": 2000, "auc": 0.7635556435873351}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7717086834733894, "accuracy_n": 59, "auc": 0.7717086834733894}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9153846153846154, "accuracy_n": 23, "auc": 0.9153846153846154}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9437751004016064, "accuracy_n": 996, "auc": 0.9437751004016064}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9393939393939394, "accuracy_n": 990, "auc": 0.9393939393939394}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7536231884057971, "accuracy_n": 276, "auc": 0.7536231884057971}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7253521126760564, "accuracy_n": 994, "auc": 0.7253521126760564}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6096579476861167, "accuracy_n": 497, "auc": 0.6096579476861167}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.751503006012024, "accuracy_n": 499, "auc": 0.751503006012024}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7994987468671679, "accuracy_n": 399, "auc": 0.7994987468671679}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9996138399752857, "accuracy_n": 322, "auc": 0.9996138399752857}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8678101503759398, "accuracy_n": 292, "auc": 0.8678101503759398}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7266786093418258, "accuracy_n": 1902, "auc": 0.7266786093418258}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8133546210181876, "accuracy_n": 2000, "auc": 0.8133546210181876}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9733893557422969, "accuracy_n": 59, "auc": 0.9733893557422969}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9692307692307693, "accuracy_n": 23, "auc": 0.9692307692307693}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9347389558232931, "accuracy_n": 996, "auc": 0.9347389558232931}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9161616161616162, "accuracy_n": 990, "auc": 0.9161616161616162}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6123188405797102, "accuracy_n": 276, "auc": 0.6123188405797102}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8410462776659959, "accuracy_n": 994, "auc": 0.8410462776659959}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8108651911468813, "accuracy_n": 497, "auc": 0.8108651911468813}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8176352705410822, "accuracy_n": 499, "auc": 0.8176352705410822}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8859060402684564, "accuracy_n": 298, "auc": 0.8859060402684564}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8345864661654135, "accuracy_n": 399, "auc": 0.8345864661654135}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9801980198019802, "accuracy_n": 303, "auc": 0.9801980198019802}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9993435279579858, "accuracy_n": 322, "auc": 0.9993435279579858}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9237312030075189, "accuracy_n": 292, "auc": 0.9237312030075189}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5782488499646143, "accuracy_n": 1902, "auc": 0.5782488499646143}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6960472730655767, "accuracy_n": 2000, "auc": 0.6960472730655767}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9411764705882353, "accuracy_n": 59, "auc": 0.9411764705882353}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7692307692307692, "accuracy_n": 23, "auc": 0.7692307692307692}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9457831325301205, "accuracy_n": 996, "auc": 0.9457831325301205}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9363636363636364, "accuracy_n": 990, "auc": 0.9363636363636364}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6376811594202898, "accuracy_n": 276, "auc": 0.6376811594202898}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8259557344064387, "accuracy_n": 994, "auc": 0.8259557344064387}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8028169014084507, "accuracy_n": 497, "auc": 0.8028169014084507}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8216432865731463, "accuracy_n": 499, "auc": 0.8216432865731463}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9026845637583892, "accuracy_n": 298, "auc": 0.9026845637583892}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8521303258145363, "accuracy_n": 399, "auc": 0.8521303258145363}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9372937293729373, "accuracy_n": 303, "auc": 0.9372937293729373}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9983008958912574, "accuracy_n": 322, "auc": 0.9983008958912574}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9482142857142857, "accuracy_n": 292, "auc": 0.9482142857142857}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6472819355980184, "accuracy_n": 1902, "auc": 0.6472819355980184}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6938754890515475, "accuracy_n": 2000, "auc": 0.6938754890515475}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9635854341736694, "accuracy_n": 59, "auc": 0.9635854341736694}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7846153846153847, "accuracy_n": 23, "auc": 0.7846153846153847}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9477911646586346, "accuracy_n": 996, "auc": 0.9477911646586346}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9383838383838384, "accuracy_n": 990, "auc": 0.9383838383838384}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7608695652173914, "accuracy_n": 276, "auc": 0.7608695652173914}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8611670020120724, "accuracy_n": 994, "auc": 0.8611670020120724}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7907444668008048, "accuracy_n": 497, "auc": 0.7907444668008048}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.811623246492986, "accuracy_n": 499, "auc": 0.811623246492986}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.889261744966443, "accuracy_n": 298, "auc": 0.889261744966443}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8395989974937343, "accuracy_n": 399, "auc": 0.8395989974937343}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9961383997528576, "accuracy_n": 322, "auc": 0.9961383997528576}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9335526315789473, "accuracy_n": 292, "auc": 0.9335526315789473}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.645424186128804, "accuracy_n": 1902, "auc": 0.645424186128804}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6591649585500365, "accuracy_n": 2000, "auc": 0.6591649585500365}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9677871148459383, "accuracy_n": 59, "auc": 0.9677871148459383}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7846153846153846, "accuracy_n": 23, "auc": 0.7846153846153846}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9477911646586346, "accuracy_n": 996, "auc": 0.9477911646586346}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 276, "auc": 0.75}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8672032193158954, "accuracy_n": 994, "auc": 0.8672032193158954}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7987927565392354, "accuracy_n": 497, "auc": 0.7987927565392354}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8136272545090181, "accuracy_n": 499, "auc": 0.8136272545090181}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8859060402684564, "accuracy_n": 298, "auc": 0.8859060402684564}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8270676691729323, "accuracy_n": 399, "auc": 0.8270676691729323}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9994207599629287, "accuracy_n": 322, "auc": 0.9994207599629287}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9484492481203007, "accuracy_n": 292, "auc": 0.9484492481203007}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6013313871196038, "accuracy_n": 1902, "auc": 0.6013313871196038}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7363588255360185, "accuracy_n": 2000, "auc": 0.7363588255360185}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9677871148459383, "accuracy_n": 59, "auc": 0.9677871148459383}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9427710843373494, "accuracy_n": 996, "auc": 0.9427710843373494}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9313131313131313, "accuracy_n": 990, "auc": 0.9313131313131313}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8128772635814889, "accuracy_n": 994, "auc": 0.8128772635814889}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6619718309859155, "accuracy_n": 497, "auc": 0.6619718309859155}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6913827655310621, "accuracy_n": 499, "auc": 0.6913827655310621}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7694235588972431, "accuracy_n": 399, "auc": 0.7694235588972431}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9998841519925857, "accuracy_n": 322, "auc": 0.9998841519925857}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8199248120300751, "accuracy_n": 292, "auc": 0.8199248120300751}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7097144815994338, "accuracy_n": 1902, "auc": 0.7097144815994338}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8210554009997609, "accuracy_n": 2000, "auc": 0.8210554009997609}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9901960784313726, "accuracy_n": 59, "auc": 0.9901960784313726}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9343434343434344, "accuracy_n": 990, "auc": 0.9343434343434344}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.855072463768116, "accuracy_n": 276, "auc": 0.855072463768116}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.823943661971831, "accuracy_n": 994, "auc": 0.823943661971831}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6016096579476862, "accuracy_n": 497, "auc": 0.6016096579476862}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6673346693386774, "accuracy_n": 499, "auc": 0.6673346693386774}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9991890639481001, "accuracy_n": 322, "auc": 0.9991890639481001}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8467105263157894, "accuracy_n": 292, "auc": 0.8467105263157894}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7513711960368011, "accuracy_n": 1902, "auc": 0.7513711960368011}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8085348810920742, "accuracy_n": 2000, "auc": 0.8085348810920742}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9957983193277311, "accuracy_n": 59, "auc": 0.9957983193277311}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9307692307692307, "accuracy_n": 23, "auc": 0.9307692307692307}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9487951807228916, "accuracy_n": 996, "auc": 0.9487951807228916}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9323232323232323, "accuracy_n": 990, "auc": 0.9323232323232323}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8514492753623188, "accuracy_n": 276, "auc": 0.8514492753623188}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.96, "accuracy_n": 100, "auc": 0.96}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.858148893360161, "accuracy_n": 994, "auc": 0.858148893360161}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.806841046277666, "accuracy_n": 497, "auc": 0.806841046277666}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8096192384769539, "accuracy_n": 499, "auc": 0.8096192384769539}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8959731543624161, "accuracy_n": 298, "auc": 0.8959731543624161}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8170426065162907, "accuracy_n": 399, "auc": 0.8170426065162907}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9998841519925857, "accuracy_n": 322, "auc": 0.9998841519925857}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8783364661654136, "accuracy_n": 292, "auc": 0.8783364661654136}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7458034766454353, "accuracy_n": 1902, "auc": 0.7458034766454353}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8371392072538187, "accuracy_n": 2000, "auc": 0.8371392072538187}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 59, "auc": 1.0}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9692307692307692, "accuracy_n": 23, "auc": 0.9692307692307692}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9457831325301205, "accuracy_n": 996, "auc": 0.9457831325301205}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8369565217391305, "accuracy_n": 276, "auc": 0.8369565217391305}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8752515090543259, "accuracy_n": 994, "auc": 0.8752515090543259}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7706237424547284, "accuracy_n": 497, "auc": 0.7706237424547284}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7855711422845691, "accuracy_n": 499, "auc": 0.7855711422845691}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9060402684563759, "accuracy_n": 298, "auc": 0.9060402684563759}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7969924812030075, "accuracy_n": 399, "auc": 0.7969924812030075}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9993049119555144, "accuracy_n": 322, "auc": 0.9993049119555144}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9746710526315788, "accuracy_n": 292, "auc": 0.9746710526315788}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.65970563517339, "accuracy_n": 1902, "auc": 0.65970563517339}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8031669432665193, "accuracy_n": 2000, "auc": 0.8031669432665193}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9341736694677871, "accuracy_n": 59, "auc": 0.9341736694677871}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7923076923076923, "accuracy_n": 23, "auc": 0.7923076923076923}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.927710843373494, "accuracy_n": 996, "auc": 0.927710843373494}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9383838383838384, "accuracy_n": 990, "auc": 0.9383838383838384}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8514492753623188, "accuracy_n": 276, "auc": 0.8514492753623188}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8601609657947686, "accuracy_n": 994, "auc": 0.8601609657947686}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.704225352112676, "accuracy_n": 497, "auc": 0.704225352112676}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7615230460921844, "accuracy_n": 499, "auc": 0.7615230460921844}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8523489932885906, "accuracy_n": 298, "auc": 0.8523489932885906}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8045112781954887, "accuracy_n": 399, "auc": 0.8045112781954887}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9993821439604572, "accuracy_n": 322, "auc": 0.9993821439604572}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8624060150375942, "accuracy_n": 292, "auc": 0.8624060150375942}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6765923566878981, "accuracy_n": 1902, "auc": 0.6765923566878981}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8088049785972735, "accuracy_n": 2000, "auc": 0.8088049785972735}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9943977591036415, "accuracy_n": 59, "auc": 0.9943977591036415}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.923076923076923, "accuracy_n": 23, "auc": 0.923076923076923}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9457831325301205, "accuracy_n": 996, "auc": 0.9457831325301205}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8262626262626263, "accuracy_n": 990, "auc": 0.8262626262626263}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8369565217391305, "accuracy_n": 276, "auc": 0.8369565217391305}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5150905432595574, "accuracy_n": 994, "auc": 0.5150905432595574}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3843058350100604, "accuracy_n": 497, "auc": 0.3843058350100604}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5190380761523046, "accuracy_n": 499, "auc": 0.5190380761523046}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6912751677852349, "accuracy_n": 298, "auc": 0.6912751677852349}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.43358395989974935, "accuracy_n": 399, "auc": 0.43358395989974935}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9215708989805376, "accuracy_n": 322, "auc": 0.9215708989805376}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7616541353383459, "accuracy_n": 292, "auc": 0.7616541353383459}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9022060774946923, "accuracy_n": 1902, "auc": 0.9022060774946923}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6043401668002149, "accuracy_n": 2000, "auc": 0.6043401668002149}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9131652661064426, "accuracy_n": 59, "auc": 0.9131652661064426}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9447791164658634, "accuracy_n": 996, "auc": 0.9447791164658634}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9353535353535354, "accuracy_n": 990, "auc": 0.9353535353535354}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8442028985507246, "accuracy_n": 276, "auc": 0.8442028985507246}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8450704225352113, "accuracy_n": 994, "auc": 0.8450704225352113}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7605633802816901, "accuracy_n": 497, "auc": 0.7605633802816901}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7675350701402806, "accuracy_n": 499, "auc": 0.7675350701402806}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.87248322147651, "accuracy_n": 298, "auc": 0.87248322147651}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8145363408521303, "accuracy_n": 399, "auc": 0.8145363408521303}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9988029039233859, "accuracy_n": 322, "auc": 0.9988029039233859}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8638157894736842, "accuracy_n": 292, "auc": 0.8638157894736842}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7437323071479123, "accuracy_n": 1902, "auc": 0.7437323071479123}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8598704132191721, "accuracy_n": 2000, "auc": 0.8598704132191721}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9901960784313726, "accuracy_n": 59, "auc": 0.9901960784313726}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9461538461538462, "accuracy_n": 23, "auc": 0.9461538461538462}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9427710843373494, "accuracy_n": 996, "auc": 0.9427710843373494}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9444444444444444, "accuracy_n": 990, "auc": 0.9444444444444444}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 276, "auc": 0.8333333333333334}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.783702213279678, "accuracy_n": 994, "auc": 0.783702213279678}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.579476861167002, "accuracy_n": 497, "auc": 0.579476861167002}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6933867735470942, "accuracy_n": 499, "auc": 0.6933867735470942}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8053691275167785, "accuracy_n": 298, "auc": 0.8053691275167785}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7218045112781954, "accuracy_n": 399, "auc": 0.7218045112781954}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9971810318195861, "accuracy_n": 322, "auc": 0.9971810318195861}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8398026315789474, "accuracy_n": 292, "auc": 0.8398026315789474}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7226888711960369, "accuracy_n": 1902, "auc": 0.7226888711960369}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7603004684691174, "accuracy_n": 2000, "auc": 0.7603004684691174}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9789915966386554, "accuracy_n": 59, "auc": 0.9789915966386554}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.923076923076923, "accuracy_n": 23, "auc": 0.923076923076923}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9407630522088354, "accuracy_n": 996, "auc": 0.9407630522088354}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9333333333333333, "accuracy_n": 990, "auc": 0.9333333333333333}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7681159420289855, "accuracy_n": 276, "auc": 0.7681159420289855}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7635814889336016, "accuracy_n": 994, "auc": 0.7635814889336016}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 497, "auc": 0.5855130784708249}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8322147651006712, "accuracy_n": 298, "auc": 0.8322147651006712}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.706766917293233, "accuracy_n": 399, "auc": 0.706766917293233}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9993435279579859, "accuracy_n": 322, "auc": 0.9993435279579859}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8111842105263157, "accuracy_n": 292, "auc": 0.8111842105263157}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5165970010615711, "accuracy_n": 1902, "auc": 0.5165970010615711}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7862338304127791, "accuracy_n": 2000, "auc": 0.7862338304127791}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 59, "auc": 1.0}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9846153846153847, "accuracy_n": 23, "auc": 0.9846153846153847}}
