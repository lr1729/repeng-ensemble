{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9347389558232931, "accuracy_n": 996, "auc": 0.9347389558232931}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9363636363636364, "accuracy_n": 990, "auc": 0.9363636363636364}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8514492753623188, "accuracy_n": 276, "auc": 0.8514492753623188}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 994, "auc": 0.5855130784708249}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6036217303822937, "accuracy_n": 497, "auc": 0.6036217303822937}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.889261744966443, "accuracy_n": 298, "auc": 0.889261744966443}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7368421052631579, "accuracy_n": 399, "auc": 0.7368421052631579}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9945165276490577, "accuracy_n": 322, "auc": 0.9945165276490577}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8080827067669172, "accuracy_n": 292, "auc": 0.8080827067669172}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5473427547770701, "accuracy_n": 1902, "auc": 0.5473427547770701}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7251787895430251, "accuracy_n": 2000, "auc": 0.7251787895430251}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7773109243697479, "accuracy_n": 59, "auc": 0.7773109243697479}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9317269076305221, "accuracy_n": 996, "auc": 0.9317269076305221}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9444444444444444, "accuracy_n": 990, "auc": 0.9444444444444444}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8695652173913043, "accuracy_n": 276, "auc": 0.8695652173913043}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5865191146881288, "accuracy_n": 994, "auc": 0.5865191146881288}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6096579476861167, "accuracy_n": 497, "auc": 0.6096579476861167}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7855711422845691, "accuracy_n": 499, "auc": 0.7855711422845691}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9060402684563759, "accuracy_n": 298, "auc": 0.9060402684563759}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.49624060150375937, "accuracy_n": 399, "auc": 0.49624060150375937}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9876814952116157, "accuracy_n": 322, "auc": 0.9876814952116157}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8422932330827068, "accuracy_n": 292, "auc": 0.8422932330827068}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5376514950460014, "accuracy_n": 1902, "auc": 0.5376514950460014}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6847241854309406, "accuracy_n": 2000, "auc": 0.6847241854309406}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9019607843137254, "accuracy_n": 59, "auc": 0.9019607843137254}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9484848484848485, "accuracy_n": 990, "auc": 0.9484848484848485}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8442028985507246, "accuracy_n": 276, "auc": 0.8442028985507246}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7283702213279678, "accuracy_n": 994, "auc": 0.7283702213279678}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3118712273641851, "accuracy_n": 497, "auc": 0.3118712273641851}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5536912751677853, "accuracy_n": 298, "auc": 0.5536912751677853}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.21804511278195488, "accuracy_n": 399, "auc": 0.21804511278195488}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5874587458745875, "accuracy_n": 303, "auc": 0.5874587458745875}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9429255483472352, "accuracy_n": 322, "auc": 0.9429255483472352}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5449718045112782, "accuracy_n": 292, "auc": 0.5449718045112782}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5326876513317191, "accuracy_n": 413, "auc": 0.5326876513317191}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5419773973814579, "accuracy_n": 1902, "auc": 0.5419773973814579}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.541279401864073, "accuracy_n": 2000, "auc": 0.541279401864073}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.623249299719888, "accuracy_n": 59, "auc": 0.623249299719888}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9505050505050505, "accuracy_n": 990, "auc": 0.9505050505050505}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8731884057971014, "accuracy_n": 276, "auc": 0.8731884057971014}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8682092555331992, "accuracy_n": 994, "auc": 0.8682092555331992}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.358148893360161, "accuracy_n": 497, "auc": 0.358148893360161}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5490981963927856, "accuracy_n": 499, "auc": 0.5490981963927856}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5503355704697986, "accuracy_n": 298, "auc": 0.5503355704697986}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5463659147869674, "accuracy_n": 399, "auc": 0.5463659147869674}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9240924092409241, "accuracy_n": 303, "auc": 0.9240924092409241}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5973123262279889, "accuracy_n": 322, "auc": 0.5973123262279889}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5579417293233082, "accuracy_n": 292, "auc": 0.5579417293233082}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8595641646489104, "accuracy_n": 413, "auc": 0.8595641646489104}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5338530608634112, "accuracy_n": 1902, "auc": 0.5338530608634112}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5516736541891623, "accuracy_n": 2000, "auc": 0.5516736541891623}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8823529411764706, "accuracy_n": 59, "auc": 0.8823529411764706}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9216867469879518, "accuracy_n": 996, "auc": 0.9216867469879518}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9131313131313131, "accuracy_n": 990, "auc": 0.9131313131313131}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8768115942028986, "accuracy_n": 276, "auc": 0.8768115942028986}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8531187122736419, "accuracy_n": 994, "auc": 0.8531187122736419}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.28772635814889336, "accuracy_n": 497, "auc": 0.28772635814889336}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3166332665330661, "accuracy_n": 499, "auc": 0.3166332665330661}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.34563758389261745, "accuracy_n": 298, "auc": 0.34563758389261745}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.44862155388471175, "accuracy_n": 399, "auc": 0.44862155388471175}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5478547854785478, "accuracy_n": 303, "auc": 0.5478547854785478}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7187982700030893, "accuracy_n": 322, "auc": 0.7187982700030893}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6715695488721806, "accuracy_n": 292, "auc": 0.6715695488721806}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.612590799031477, "accuracy_n": 413, "auc": 0.612590799031477}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5064623142250531, "accuracy_n": 1902, "auc": 0.5064623142250531}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5322381379678064, "accuracy_n": 2000, "auc": 0.5322381379678064}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6162464985994398, "accuracy_n": 59, "auc": 0.6162464985994398}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8865461847389559, "accuracy_n": 996, "auc": 0.8865461847389559}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9090909090909091, "accuracy_n": 990, "auc": 0.9090909090909091}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6126760563380281, "accuracy_n": 994, "auc": 0.6126760563380281}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7947686116700201, "accuracy_n": 497, "auc": 0.7947686116700201}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.843687374749499, "accuracy_n": 499, "auc": 0.843687374749499}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8993288590604027, "accuracy_n": 298, "auc": 0.8993288590604027}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8245614035087719, "accuracy_n": 399, "auc": 0.8245614035087719}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8646864686468647, "accuracy_n": 303, "auc": 0.8646864686468647}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9453197405004633, "accuracy_n": 322, "auc": 0.9453197405004633}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8805216165413534, "accuracy_n": 292, "auc": 0.8805216165413534}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5418148443029016, "accuracy_n": 1902, "auc": 0.5418148443029016}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6107834928409156, "accuracy_n": 2000, "auc": 0.6107834928409156}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8445378151260504, "accuracy_n": 59, "auc": 0.8445378151260504}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 23, "auc": 0.6}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8775100401606426, "accuracy_n": 996, "auc": 0.8775100401606426}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9383838383838384, "accuracy_n": 990, "auc": 0.9383838383838384}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5833333333333334, "accuracy_n": 276, "auc": 0.5833333333333334}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6046277665995976, "accuracy_n": 994, "auc": 0.6046277665995976}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7987927565392354, "accuracy_n": 497, "auc": 0.7987927565392354}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.843687374749499, "accuracy_n": 499, "auc": 0.843687374749499}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9060402684563759, "accuracy_n": 298, "auc": 0.9060402684563759}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8370927318295739, "accuracy_n": 399, "auc": 0.8370927318295739}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5775577557755776, "accuracy_n": 303, "auc": 0.5775577557755776}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7563716404077849, "accuracy_n": 322, "auc": 0.7563716404077849}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8231203007518797, "accuracy_n": 292, "auc": 0.8231203007518797}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9927360774818402, "accuracy_n": 413, "auc": 0.9927360774818402}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5426331387119603, "accuracy_n": 1902, "auc": 0.5426331387119603}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5767892209087481, "accuracy_n": 2000, "auc": 0.5767892209087481}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7198879551820728, "accuracy_n": 59, "auc": 0.7198879551820728}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5307692307692309, "accuracy_n": 23, "auc": 0.5307692307692309}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8945783132530121, "accuracy_n": 996, "auc": 0.8945783132530121}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9343434343434344, "accuracy_n": 990, "auc": 0.9343434343434344}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6159420289855072, "accuracy_n": 276, "auc": 0.6159420289855072}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 100, "auc": 0.78}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7927565392354124, "accuracy_n": 497, "auc": 0.7927565392354124}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8396793587174348, "accuracy_n": 499, "auc": 0.8396793587174348}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9026845637583892, "accuracy_n": 298, "auc": 0.9026845637583892}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8370927318295739, "accuracy_n": 399, "auc": 0.8370927318295739}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6303630363036303, "accuracy_n": 303, "auc": 0.6303630363036303}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.91187828236021, "accuracy_n": 322, "auc": 0.91187828236021}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.862453007518797, "accuracy_n": 292, "auc": 0.862453007518797}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5477585368011324, "accuracy_n": 1902, "auc": 0.5477585368011324}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.614279754991552, "accuracy_n": 2000, "auc": 0.614279754991552}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8123249299719888, "accuracy_n": 59, "auc": 0.8123249299719888}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9307228915662651, "accuracy_n": 996, "auc": 0.9307228915662651}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.804040404040404, "accuracy_n": 990, "auc": 0.804040404040404}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5434782608695652, "accuracy_n": 276, "auc": 0.5434782608695652}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.704225352112676, "accuracy_n": 994, "auc": 0.704225352112676}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8088531187122736, "accuracy_n": 497, "auc": 0.8088531187122736}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.843687374749499, "accuracy_n": 499, "auc": 0.843687374749499}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8993288590604027, "accuracy_n": 298, "auc": 0.8993288590604027}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8320802005012531, "accuracy_n": 399, "auc": 0.8320802005012531}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9499922767995057, "accuracy_n": 322, "auc": 0.9499922767995057}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7220864661654135, "accuracy_n": 292, "auc": 0.7220864661654135}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5513689844302901, "accuracy_n": 1902, "auc": 0.5513689844302901}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6044572090524679, "accuracy_n": 2000, "auc": 0.6044572090524679}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6106442577030813, "accuracy_n": 59, "auc": 0.6106442577030813}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5461538461538461, "accuracy_n": 23, "auc": 0.5461538461538461}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8413654618473896, "accuracy_n": 996, "auc": 0.8413654618473896}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9101010101010101, "accuracy_n": 990, "auc": 0.9101010101010101}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 276, "auc": 0.5}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5331991951710262, "accuracy_n": 994, "auc": 0.5331991951710262}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3158953722334004, "accuracy_n": 497, "auc": 0.3158953722334004}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3486973947895792, "accuracy_n": 499, "auc": 0.3486973947895792}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.46308724832214765, "accuracy_n": 298, "auc": 0.46308724832214765}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9711152301513748, "accuracy_n": 322, "auc": 0.9711152301513748}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6847274436090226, "accuracy_n": 292, "auc": 0.6847274436090226}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5209085279547063, "accuracy_n": 1902, "auc": 0.5209085279547063}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6715564318719058, "accuracy_n": 2000, "auc": 0.6715564318719058}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9677871148459383, "accuracy_n": 59, "auc": 0.9677871148459383}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9307228915662651, "accuracy_n": 996, "auc": 0.9307228915662651}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6434343434343435, "accuracy_n": 990, "auc": 0.6434343434343435}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3561368209255533, "accuracy_n": 497, "auc": 0.3561368209255533}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5470941883767535, "accuracy_n": 499, "auc": 0.5470941883767535}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5570469798657718, "accuracy_n": 298, "auc": 0.5570469798657718}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.24561403508771928, "accuracy_n": 399, "auc": 0.24561403508771928}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9911183194315724, "accuracy_n": 322, "auc": 0.9911183194315724}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.599859022556391, "accuracy_n": 292, "auc": 0.599859022556391}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5246173920736023, "accuracy_n": 1902, "auc": 0.5246173920736023}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7290211766447687, "accuracy_n": 2000, "auc": 0.7290211766447687}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9943977591036415, "accuracy_n": 59, "auc": 0.9943977591036415}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9538461538461539, "accuracy_n": 23, "auc": 0.9538461538461539}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9246987951807228, "accuracy_n": 996, "auc": 0.9246987951807228}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9353535353535354, "accuracy_n": 990, "auc": 0.9353535353535354}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5978260869565217, "accuracy_n": 276, "auc": 0.5978260869565217}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5432595573440644, "accuracy_n": 994, "auc": 0.5432595573440644}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 497, "auc": 0.5714285714285714}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5991983967935872, "accuracy_n": 499, "auc": 0.5991983967935872}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8154362416107382, "accuracy_n": 298, "auc": 0.8154362416107382}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5914786967418546, "accuracy_n": 399, "auc": 0.5914786967418546}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9998069199876429, "accuracy_n": 322, "auc": 0.9998069199876429}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8482612781954887, "accuracy_n": 292, "auc": 0.8482612781954887}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5178277600849257, "accuracy_n": 1902, "auc": 0.5178277600849257}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.737678301866974, "accuracy_n": 2000, "auc": 0.737678301866974}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9971988795518207, "accuracy_n": 59, "auc": 0.9971988795518207}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9307692307692308, "accuracy_n": 23, "auc": 0.9307692307692308}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8413654618473896, "accuracy_n": 996, "auc": 0.8413654618473896}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8868686868686869, "accuracy_n": 990, "auc": 0.8868686868686869}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5543478260869565, "accuracy_n": 276, "auc": 0.5543478260869565}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.49899396378269617, "accuracy_n": 497, "auc": 0.49899396378269617}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6152304609218436, "accuracy_n": 499, "auc": 0.6152304609218436}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7651006711409396, "accuracy_n": 298, "auc": 0.7651006711409396}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3508771929824561, "accuracy_n": 399, "auc": 0.3508771929824561}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9898826073524869, "accuracy_n": 322, "auc": 0.9898826073524869}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9660244360902256, "accuracy_n": 292, "auc": 0.9660244360902256}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5282311571125266, "accuracy_n": 1902, "auc": 0.5282311571125266}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6354153849539684, "accuracy_n": 2000, "auc": 0.6354153849539684}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7675070028011204, "accuracy_n": 59, "auc": 0.7675070028011204}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.822289156626506, "accuracy_n": 996, "auc": 0.822289156626506}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8565656565656565, "accuracy_n": 990, "auc": 0.8565656565656565}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5434782608695652, "accuracy_n": 276, "auc": 0.5434782608695652}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.65, "accuracy_n": 100, "auc": 0.65}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5714285714285714, "accuracy_n": 994, "auc": 0.5714285714285714}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4949698189134809, "accuracy_n": 497, "auc": 0.4949698189134809}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6432865731462926, "accuracy_n": 499, "auc": 0.6432865731462926}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7919463087248322, "accuracy_n": 298, "auc": 0.7919463087248322}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5288220551378446, "accuracy_n": 399, "auc": 0.5288220551378446}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9870250231696015, "accuracy_n": 322, "auc": 0.9870250231696015}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7928101503759398, "accuracy_n": 292, "auc": 0.7928101503759398}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5198989295824488, "accuracy_n": 1902, "auc": 0.5198989295824488}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6968025457190046, "accuracy_n": 2000, "auc": 0.6968025457190046}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9915966386554622, "accuracy_n": 59, "auc": 0.9915966386554622}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8999999999999999, "accuracy_n": 23, "auc": 0.8999999999999999}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9347389558232931, "accuracy_n": 996, "auc": 0.9347389558232931}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6606060606060606, "accuracy_n": 990, "auc": 0.6606060606060606}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7971014492753623, "accuracy_n": 276, "auc": 0.7971014492753623}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.82, "accuracy_n": 100, "auc": 0.82}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5482897384305835, "accuracy_n": 994, "auc": 0.5482897384305835}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.42655935613682094, "accuracy_n": 497, "auc": 0.42655935613682094}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6052104208416834, "accuracy_n": 499, "auc": 0.6052104208416834}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7013422818791947, "accuracy_n": 298, "auc": 0.7013422818791947}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.37343358395989973, "accuracy_n": 399, "auc": 0.37343358395989973}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9702970297029703, "accuracy_n": 303, "auc": 0.9702970297029703}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.746910719802286, "accuracy_n": 322, "auc": 0.746910719802286}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6966635338345866, "accuracy_n": 292, "auc": 0.6966635338345866}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9806295399515739, "accuracy_n": 413, "auc": 0.9806295399515739}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8273044939844303, "accuracy_n": 1902, "auc": 0.8273044939844303}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6185002786005748, "accuracy_n": 2000, "auc": 0.6185002786005748}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8333333333333334, "accuracy_n": 59, "auc": 0.8333333333333334}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9166666666666666, "accuracy_n": 996, "auc": 0.9166666666666666}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 990, "auc": 0.9}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6123188405797102, "accuracy_n": 276, "auc": 0.6123188405797102}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5553319919517102, "accuracy_n": 994, "auc": 0.5553319919517102}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.39436619718309857, "accuracy_n": 497, "auc": 0.39436619718309857}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4468937875751503, "accuracy_n": 499, "auc": 0.4468937875751503}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6241610738255033, "accuracy_n": 298, "auc": 0.6241610738255033}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3558897243107769, "accuracy_n": 399, "auc": 0.3558897243107769}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9886468952734012, "accuracy_n": 322, "auc": 0.9886468952734012}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7032424812030076, "accuracy_n": 292, "auc": 0.7032424812030076}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5369349345364474, "accuracy_n": 1902, "auc": 0.5369349345364474}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7380874495692946, "accuracy_n": 2000, "auc": 0.7380874495692946}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9803921568627451, "accuracy_n": 59, "auc": 0.9803921568627451}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8999999999999999, "accuracy_n": 23, "auc": 0.8999999999999999}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8674698795180723, "accuracy_n": 996, "auc": 0.8674698795180723}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8878787878787879, "accuracy_n": 990, "auc": 0.8878787878787879}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5760869565217391, "accuracy_n": 276, "auc": 0.5760869565217391}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.81, "accuracy_n": 100, "auc": 0.81}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5432595573440644, "accuracy_n": 994, "auc": 0.5432595573440644}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.317907444668008, "accuracy_n": 497, "auc": 0.317907444668008}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3346693386773547, "accuracy_n": 499, "auc": 0.3346693386773547}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4597315436241611, "accuracy_n": 298, "auc": 0.4597315436241611}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.2531328320802005, "accuracy_n": 399, "auc": 0.2531328320802005}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9740114303367315, "accuracy_n": 322, "auc": 0.9740114303367315}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6637218045112782, "accuracy_n": 292, "auc": 0.6637218045112782}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.524639508138712, "accuracy_n": 1902, "auc": 0.524639508138712}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6697397760591574, "accuracy_n": 2000, "auc": 0.6697397760591574}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9733893557422968, "accuracy_n": 59, "auc": 0.9733893557422968}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.8153846153846154, "accuracy_n": 23, "auc": 0.8153846153846154}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.7881526104417671, "accuracy_n": 996, "auc": 0.7881526104417671}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9505050505050505, "accuracy_n": 990, "auc": 0.9505050505050505}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.644927536231884, "accuracy_n": 276, "auc": 0.644927536231884}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5663983903420523, "accuracy_n": 994, "auc": 0.5663983903420523}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.3158953722334004, "accuracy_n": 497, "auc": 0.3158953722334004}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.38877755511022044, "accuracy_n": 499, "auc": 0.38877755511022044}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.4798657718120805, "accuracy_n": 298, "auc": 0.4798657718120805}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9793404386777882, "accuracy_n": 322, "auc": 0.9793404386777882}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.6786654135338347, "accuracy_n": 292, "auc": 0.6786654135338347}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.5128251061571125, "accuracy_n": 1902, "auc": 0.5128251061571125}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.654426247875483, "accuracy_n": 2000, "auc": 0.654426247875483}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9775910364145658, "accuracy_n": 59, "auc": 0.9775910364145658}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h35", "token_idx": -1, "accuracy": 0.9076923076923077, "accuracy_n": 23, "auc": 0.9076923076923077}}
