{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9464646464646465, "accuracy_n": 990, "auc": 0.9464646464646465}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7862318840579711, "accuracy_n": 276, "auc": 0.7862318840579711}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.72, "accuracy_n": 100, "auc": 0.72}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5432595573440644, "accuracy_n": 994, "auc": 0.5432595573440644}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5915492957746479, "accuracy_n": 497, "auc": 0.5915492957746479}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7454909819639278, "accuracy_n": 499, "auc": 0.7454909819639278}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8590604026845637, "accuracy_n": 298, "auc": 0.8590604026845637}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6917293233082706, "accuracy_n": 399, "auc": 0.6917293233082706}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9952502316960148, "accuracy_n": 322, "auc": 0.9952502316960148}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8717575187969924, "accuracy_n": 292, "auc": 0.8717575187969924}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7008835368011324, "accuracy_n": 1902, "auc": 0.7008835368011324}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.704310256002417, "accuracy_n": 2000, "auc": 0.704310256002417}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.938375350140056, "accuracy_n": 59, "auc": 0.938375350140056}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8923076923076924, "accuracy_n": 23, "auc": 0.8923076923076924}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9347389558232931, "accuracy_n": 996, "auc": 0.9347389558232931}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8260869565217391, "accuracy_n": 276, "auc": 0.8260869565217391}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5774647887323944, "accuracy_n": 994, "auc": 0.5774647887323944}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5653923541247485, "accuracy_n": 497, "auc": 0.5653923541247485}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7234468937875751, "accuracy_n": 499, "auc": 0.7234468937875751}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8557046979865772, "accuracy_n": 298, "auc": 0.8557046979865772}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5964912280701754, "accuracy_n": 399, "auc": 0.5964912280701754}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9963700957676861, "accuracy_n": 322, "auc": 0.9963700957676861}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8908834586466166, "accuracy_n": 292, "auc": 0.8908834586466166}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7183165251238499, "accuracy_n": 1902, "auc": 0.7183165251238499}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.739831079019526, "accuracy_n": 2000, "auc": 0.739831079019526}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9607843137254901, "accuracy_n": 59, "auc": 0.9607843137254901}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8538461538461538, "accuracy_n": 23, "auc": 0.8538461538461538}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9484848484848485, "accuracy_n": 990, "auc": 0.9484848484848485}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8586956521739131, "accuracy_n": 276, "auc": 0.8586956521739131}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.744466800804829, "accuracy_n": 994, "auc": 0.744466800804829}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.36217303822937624, "accuracy_n": 497, "auc": 0.36217303822937624}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5410821643286573, "accuracy_n": 499, "auc": 0.5410821643286573}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5234899328859061, "accuracy_n": 298, "auc": 0.5234899328859061}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.17543859649122806, "accuracy_n": 399, "auc": 0.17543859649122806}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9834983498349835, "accuracy_n": 303, "auc": 0.9834983498349835}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9990345999382144, "accuracy_n": 322, "auc": 0.9990345999382144}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7947368421052633, "accuracy_n": 292, "auc": 0.7947368421052633}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6694123761500355, "accuracy_n": 1902, "auc": 0.6694123761500355}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.656834117116279, "accuracy_n": 2000, "auc": 0.656834117116279}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.707282913165266, "accuracy_n": 59, "auc": 0.707282913165266}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8846153846153847, "accuracy_n": 23, "auc": 0.8846153846153847}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9505050505050505, "accuracy_n": 990, "auc": 0.9505050505050505}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8659420289855072, "accuracy_n": 276, "auc": 0.8659420289855072}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8199195171026157, "accuracy_n": 994, "auc": 0.8199195171026157}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6277665995975855, "accuracy_n": 497, "auc": 0.6277665995975855}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7775551102204409, "accuracy_n": 499, "auc": 0.7775551102204409}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8791946308724832, "accuracy_n": 298, "auc": 0.8791946308724832}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7919799498746867, "accuracy_n": 399, "auc": 0.7919799498746867}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6798679867986799, "accuracy_n": 303, "auc": 0.6798679867986799}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.829124189063948, "accuracy_n": 322, "auc": 0.829124189063948}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6638627819548872, "accuracy_n": 292, "auc": 0.6638627819548872}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7627118644067796, "accuracy_n": 413, "auc": 0.7627118644067796}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7288625707714083, "accuracy_n": 1902, "auc": 0.7288625707714083}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5065398608897812, "accuracy_n": 2000, "auc": 0.5065398608897812}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9019607843137255, "accuracy_n": 59, "auc": 0.9019607843137255}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9427710843373494, "accuracy_n": 996, "auc": 0.9427710843373494}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9424242424242424, "accuracy_n": 990, "auc": 0.9424242424242424}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8079710144927537, "accuracy_n": 276, "auc": 0.8079710144927537}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.96, "accuracy_n": 100, "auc": 0.96}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8571428571428571, "accuracy_n": 994, "auc": 0.8571428571428571}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.43259557344064387, "accuracy_n": 497, "auc": 0.43259557344064387}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6893787575150301, "accuracy_n": 499, "auc": 0.6893787575150301}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7550335570469798, "accuracy_n": 298, "auc": 0.7550335570469798}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6867167919799498, "accuracy_n": 399, "auc": 0.6867167919799498}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9993821439604572, "accuracy_n": 322, "auc": 0.9993821439604572}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8062969924812031, "accuracy_n": 292, "auc": 0.8062969924812031}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5976678609341826, "accuracy_n": 1902, "auc": 0.5976678609341826}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7323873918484572, "accuracy_n": 2000, "auc": 0.7323873918484572}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8991596638655462, "accuracy_n": 59, "auc": 0.8991596638655462}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9230769230769231, "accuracy_n": 23, "auc": 0.9230769230769231}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9272727272727272, "accuracy_n": 990, "auc": 0.9272727272727272}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5653923541247485, "accuracy_n": 994, "auc": 0.5653923541247485}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7686116700201208, "accuracy_n": 497, "auc": 0.7686116700201208}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8216432865731463, "accuracy_n": 499, "auc": 0.8216432865731463}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9026845637583892, "accuracy_n": 298, "auc": 0.9026845637583892}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8245614035087719, "accuracy_n": 399, "auc": 0.8245614035087719}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9942462156317579, "accuracy_n": 322, "auc": 0.9942462156317579}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9286654135338346, "accuracy_n": 292, "auc": 0.9286654135338346}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6382043966737438, "accuracy_n": 1902, "auc": 0.6382043966737438}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6588768545444905, "accuracy_n": 2000, "auc": 0.6588768545444905}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9593837535014005, "accuracy_n": 59, "auc": 0.9593837535014005}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7384615384615385, "accuracy_n": 23, "auc": 0.7384615384615385}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9357429718875502, "accuracy_n": 996, "auc": 0.9357429718875502}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 990, "auc": 0.9}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5543478260869565, "accuracy_n": 276, "auc": 0.5543478260869565}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8138832997987927, "accuracy_n": 994, "auc": 0.8138832997987927}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7867203219315896, "accuracy_n": 497, "auc": 0.7867203219315896}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8356713426853707, "accuracy_n": 499, "auc": 0.8356713426853707}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9194630872483222, "accuracy_n": 298, "auc": 0.9194630872483222}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8421052631578947, "accuracy_n": 399, "auc": 0.8421052631578947}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8250825082508251, "accuracy_n": 303, "auc": 0.8250825082508251}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9914658634538153, "accuracy_n": 322, "auc": 0.9914658634538153}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9200657894736841, "accuracy_n": 292, "auc": 0.9200657894736841}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6454982749469216, "accuracy_n": 1902, "auc": 0.6454982749469216}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6242003363214121, "accuracy_n": 2000, "auc": 0.6242003363214121}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8823529411764706, "accuracy_n": 59, "auc": 0.8823529411764706}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6384615384615385, "accuracy_n": 23, "auc": 0.6384615384615385}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9171717171717172, "accuracy_n": 990, "auc": 0.9171717171717172}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5398550724637681, "accuracy_n": 276, "auc": 0.5398550724637681}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7223340040241448, "accuracy_n": 994, "auc": 0.7223340040241448}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7565392354124748, "accuracy_n": 497, "auc": 0.7565392354124748}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8296593186372746, "accuracy_n": 499, "auc": 0.8296593186372746}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9093959731543624, "accuracy_n": 298, "auc": 0.9093959731543624}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8320802005012531, "accuracy_n": 399, "auc": 0.8320802005012531}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9504950495049505, "accuracy_n": 303, "auc": 0.9504950495049505}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9910410874266296, "accuracy_n": 322, "auc": 0.9910410874266296}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9261748120300752, "accuracy_n": 292, "auc": 0.9261748120300752}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6352939225053078, "accuracy_n": 1902, "auc": 0.6352939225053078}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6526876202309034, "accuracy_n": 2000, "auc": 0.6526876202309034}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.938375350140056, "accuracy_n": 59, "auc": 0.938375350140056}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9367469879518072, "accuracy_n": 996, "auc": 0.9367469879518072}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9111111111111111, "accuracy_n": 990, "auc": 0.9111111111111111}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6217303822937625, "accuracy_n": 994, "auc": 0.6217303822937625}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7947686116700201, "accuracy_n": 497, "auc": 0.7947686116700201}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8476953907815631, "accuracy_n": 499, "auc": 0.8476953907815631}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9026845637583892, "accuracy_n": 298, "auc": 0.9026845637583892}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8446115288220551, "accuracy_n": 399, "auc": 0.8446115288220551}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9240924092409241, "accuracy_n": 303, "auc": 0.9240924092409241}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9952888476984862, "accuracy_n": 322, "auc": 0.9952888476984862}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9455357142857143, "accuracy_n": 292, "auc": 0.9455357142857143}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6010449840764331, "accuracy_n": 1902, "auc": 0.6010449840764331}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6933152868185415, "accuracy_n": 2000, "auc": 0.6933152868185415}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9173669467787114, "accuracy_n": 59, "auc": 0.9173669467787114}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9467871485943775, "accuracy_n": 996, "auc": 0.9467871485943775}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9161616161616162, "accuracy_n": 990, "auc": 0.9161616161616162}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8297101449275363, "accuracy_n": 276, "auc": 0.8297101449275363}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8661971830985915, "accuracy_n": 994, "auc": 0.8661971830985915}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5694164989939637, "accuracy_n": 497, "auc": 0.5694164989939637}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6452905811623246, "accuracy_n": 499, "auc": 0.6452905811623246}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8489932885906041, "accuracy_n": 298, "auc": 0.8489932885906041}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5112781954887218, "accuracy_n": 399, "auc": 0.5112781954887218}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.996177015755329, "accuracy_n": 322, "auc": 0.996177015755329}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7966635338345864, "accuracy_n": 292, "auc": 0.7966635338345864}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6374325460014154, "accuracy_n": 1902, "auc": 0.6374325460014154}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7409184715682362, "accuracy_n": 2000, "auc": 0.7409184715682362}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9901960784313726, "accuracy_n": 59, "auc": 0.9901960784313726}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9461538461538461, "accuracy_n": 23, "auc": 0.9461538461538461}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9407630522088354, "accuracy_n": 996, "auc": 0.9407630522088354}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.895959595959596, "accuracy_n": 990, "auc": 0.895959595959596}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8623188405797102, "accuracy_n": 276, "auc": 0.8623188405797102}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8018108651911469, "accuracy_n": 994, "auc": 0.8018108651911469}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6056338028169014, "accuracy_n": 497, "auc": 0.6056338028169014}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6693386773547094, "accuracy_n": 499, "auc": 0.6693386773547094}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8523489932885906, "accuracy_n": 298, "auc": 0.8523489932885906}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5213032581453634, "accuracy_n": 399, "auc": 0.5213032581453634}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9995366079703428, "accuracy_n": 322, "auc": 0.9995366079703428}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7921522556390977, "accuracy_n": 292, "auc": 0.7921522556390977}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6884686836518046, "accuracy_n": 1902, "auc": 0.6884686836518046}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7649571495309807, "accuracy_n": 2000, "auc": 0.7649571495309807}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9971988795518207, "accuracy_n": 59, "auc": 0.9971988795518207}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9384615384615385, "accuracy_n": 23, "auc": 0.9384615384615385}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9414141414141414, "accuracy_n": 990, "auc": 0.9414141414141414}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8731884057971014, "accuracy_n": 276, "auc": 0.8731884057971014}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.886317907444668, "accuracy_n": 994, "auc": 0.886317907444668}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7082494969818913, "accuracy_n": 497, "auc": 0.7082494969818913}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7915831663326653, "accuracy_n": 499, "auc": 0.7915831663326653}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8959731543624161, "accuracy_n": 298, "auc": 0.8959731543624161}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7719298245614035, "accuracy_n": 399, "auc": 0.7719298245614035}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9999227679950572, "accuracy_n": 322, "auc": 0.9999227679950572}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8704887218045113, "accuracy_n": 292, "auc": 0.8704887218045113}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6773675247699928, "accuracy_n": 1902, "auc": 0.6773675247699928}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7622071567835988, "accuracy_n": 2000, "auc": 0.7622071567835988}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9985994397759104, "accuracy_n": 59, "auc": 0.9985994397759104}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9846153846153847, "accuracy_n": 23, "auc": 0.9846153846153847}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9186746987951807, "accuracy_n": 996, "auc": 0.9186746987951807}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9252525252525252, "accuracy_n": 990, "auc": 0.9252525252525252}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7463768115942029, "accuracy_n": 276, "auc": 0.7463768115942029}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.795774647887324, "accuracy_n": 994, "auc": 0.795774647887324}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6257545271629779, "accuracy_n": 497, "auc": 0.6257545271629779}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7575150300601202, "accuracy_n": 499, "auc": 0.7575150300601202}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8926174496644296, "accuracy_n": 298, "auc": 0.8926174496644296}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6741854636591479, "accuracy_n": 399, "auc": 0.6741854636591479}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9984553599011431, "accuracy_n": 322, "auc": 0.9984553599011431}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.968843984962406, "accuracy_n": 292, "auc": 0.968843984962406}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6281377167374381, "accuracy_n": 1902, "auc": 0.6281377167374381}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7078525347650502, "accuracy_n": 2000, "auc": 0.7078525347650502}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9201680672268907, "accuracy_n": 59, "auc": 0.9201680672268907}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9146586345381527, "accuracy_n": 996, "auc": 0.9146586345381527}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9404040404040404, "accuracy_n": 990, "auc": 0.9404040404040404}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8043478260869565, "accuracy_n": 276, "auc": 0.8043478260869565}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.67, "accuracy_n": 100, "auc": 0.67}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7937625754527163, "accuracy_n": 994, "auc": 0.7937625754527163}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 497, "auc": 0.6177062374245473}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8657718120805369, "accuracy_n": 298, "auc": 0.8657718120805369}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 399, "auc": 0.6666666666666666}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9972582638245289, "accuracy_n": 322, "auc": 0.9972582638245289}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.855733082706767, "accuracy_n": 292, "auc": 0.855733082706767}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6557153441259731, "accuracy_n": 1902, "auc": 0.6557153441259731}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7666047443126968, "accuracy_n": 2000, "auc": 0.7666047443126968}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9985994397759104, "accuracy_n": 59, "auc": 0.9985994397759104}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9538461538461539, "accuracy_n": 23, "auc": 0.9538461538461539}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9387550200803213, "accuracy_n": 996, "auc": 0.9387550200803213}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7181818181818181, "accuracy_n": 990, "auc": 0.7181818181818181}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7753623188405797, "accuracy_n": 276, "auc": 0.7753623188405797}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5905432595573441, "accuracy_n": 994, "auc": 0.5905432595573441}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.41851106639839036, "accuracy_n": 497, "auc": 0.41851106639839036}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5991983967935872, "accuracy_n": 499, "auc": 0.5991983967935872}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6711409395973155, "accuracy_n": 298, "auc": 0.6711409395973155}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3333333333333333, "accuracy_n": 399, "auc": 0.3333333333333333}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9075907590759076, "accuracy_n": 303, "auc": 0.9075907590759076}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.918983626814952, "accuracy_n": 322, "auc": 0.918983626814952}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7849154135338345, "accuracy_n": 292, "auc": 0.7849154135338345}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8711960368011323, "accuracy_n": 1902, "auc": 0.8711960368011323}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5929400513585404, "accuracy_n": 2000, "auc": 0.5929400513585404}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9187675070028012, "accuracy_n": 59, "auc": 0.9187675070028012}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9447791164658634, "accuracy_n": 996, "auc": 0.9447791164658634}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9252525252525252, "accuracy_n": 990, "auc": 0.9252525252525252}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8478260869565217, "accuracy_n": 276, "auc": 0.8478260869565217}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8672032193158954, "accuracy_n": 994, "auc": 0.8672032193158954}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6498993963782697, "accuracy_n": 497, "auc": 0.6498993963782697}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7234468937875751, "accuracy_n": 499, "auc": 0.7234468937875751}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8624161073825504, "accuracy_n": 298, "auc": 0.8624161073825504}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6766917293233082, "accuracy_n": 399, "auc": 0.6766917293233082}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9993435279579859, "accuracy_n": 322, "auc": 0.9993435279579859}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8297462406015038, "accuracy_n": 292, "auc": 0.8297462406015038}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6686670647558386, "accuracy_n": 1902, "auc": 0.6686670647558386}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7829866581836042, "accuracy_n": 2000, "auc": 0.7829866581836042}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9901960784313726, "accuracy_n": 59, "auc": 0.9901960784313726}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9384615384615385, "accuracy_n": 23, "auc": 0.9384615384615385}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9447791164658634, "accuracy_n": 996, "auc": 0.9447791164658634}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9383838383838384, "accuracy_n": 990, "auc": 0.9383838383838384}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7137681159420289, "accuracy_n": 276, "auc": 0.7137681159420289}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.77, "accuracy_n": 100, "auc": 0.77}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.789738430583501, "accuracy_n": 994, "auc": 0.789738430583501}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.49295774647887325, "accuracy_n": 497, "auc": 0.49295774647887325}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5511022044088176, "accuracy_n": 499, "auc": 0.5511022044088176}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7516778523489933, "accuracy_n": 298, "auc": 0.7516778523489933}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.40100250626566414, "accuracy_n": 399, "auc": 0.40100250626566414}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9935897435897436, "accuracy_n": 322, "auc": 0.9935897435897436}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7915883458646615, "accuracy_n": 292, "auc": 0.7915883458646615}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6526528220099079, "accuracy_n": 1902, "auc": 0.6526528220099079}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7141458066361956, "accuracy_n": 2000, "auc": 0.7141458066361956}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.988795518207283, "accuracy_n": 59, "auc": 0.988795518207283}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9076923076923078, "accuracy_n": 23, "auc": 0.9076923076923078}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9236947791164659, "accuracy_n": 996, "auc": 0.9236947791164659}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9323232323232323, "accuracy_n": 990, "auc": 0.9323232323232323}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7463768115942029, "accuracy_n": 276, "auc": 0.7463768115942029}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.69, "accuracy_n": 100, "auc": 0.69}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8309859154929577, "accuracy_n": 994, "auc": 0.8309859154929577}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5331991951710262, "accuracy_n": 497, "auc": 0.5331991951710262}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5891783567134269, "accuracy_n": 499, "auc": 0.5891783567134269}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7718120805369127, "accuracy_n": 298, "auc": 0.7718120805369127}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.48872180451127817, "accuracy_n": 399, "auc": 0.48872180451127817}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9989573679332715, "accuracy_n": 322, "auc": 0.9989573679332715}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8269736842105263, "accuracy_n": 292, "auc": 0.8269736842105263}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5598504953998584, "accuracy_n": 1902, "auc": 0.5598504953998584}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7147160124805055, "accuracy_n": 2000, "auc": 0.7147160124805055}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 59, "auc": 1.0}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9538461538461538, "accuracy_n": 23, "auc": 0.9538461538461538}}
