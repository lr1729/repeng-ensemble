{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6465863453815262, "accuracy_n": 996, "auc": 0.6465863453815262}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8707070707070707, "accuracy_n": 990, "auc": 0.8707070707070707}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.613682092555332, "accuracy_n": 994, "auc": 0.613682092555332}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.352112676056338, "accuracy_n": 497, "auc": 0.352112676056338}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2965931863727455, "accuracy_n": 499, "auc": 0.2965931863727455}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2731829573934837, "accuracy_n": 399, "auc": 0.2731829573934837}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9141914191419142, "accuracy_n": 303, "auc": 0.9141914191419142}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5421300586963238, "accuracy_n": 322, "auc": 0.5421300586963238}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5553101503759398, "accuracy_n": 292, "auc": 0.5553101503759398}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5302663438256658, "accuracy_n": 413, "auc": 0.5302663438256658}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7049927016985138, "accuracy_n": 1902, "auc": 0.7049927016985138}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7055136904422497, "accuracy_n": 2000, "auc": 0.7055136904422497}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7759103641456582, "accuracy_n": 59, "auc": 0.7759103641456582}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6153846153846153, "accuracy_n": 23, "auc": 0.6153846153846153}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6375502008032129, "accuracy_n": 996, "auc": 0.6375502008032129}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8606060606060606, "accuracy_n": 990, "auc": 0.8606060606060606}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.36016096579476864, "accuracy_n": 497, "auc": 0.36016096579476864}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28256513026052105, "accuracy_n": 499, "auc": 0.28256513026052105}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 399, "auc": 0.2857142857142857}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8118811881188119, "accuracy_n": 303, "auc": 0.8118811881188119}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6161955514365153, "accuracy_n": 322, "auc": 0.6161955514365153}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5234962406015038, "accuracy_n": 292, "auc": 0.5234962406015038}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8325869161358811, "accuracy_n": 1902, "auc": 0.8325869161358811}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7075334195644627, "accuracy_n": 2000, "auc": 0.7075334195644627}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6932773109243697, "accuracy_n": 59, "auc": 0.6932773109243697}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7494949494949495, "accuracy_n": 990, "auc": 0.7494949494949495}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5434782608695652, "accuracy_n": 276, "auc": 0.5434782608695652}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5804828973843058, "accuracy_n": 994, "auc": 0.5804828973843058}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.32595573440643866, "accuracy_n": 497, "auc": 0.32595573440643866}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2725450901803607, "accuracy_n": 499, "auc": 0.2725450901803607}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.966996699669967, "accuracy_n": 303, "auc": 0.966996699669967}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7493049119555144, "accuracy_n": 322, "auc": 0.7493049119555144}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6888627819548874, "accuracy_n": 292, "auc": 0.6888627819548874}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8547215496368039, "accuracy_n": 413, "auc": 0.8547215496368039}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.597139286978061, "accuracy_n": 1902, "auc": 0.597139286978061}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6662345106583477, "accuracy_n": 2000, "auc": 0.6662345106583477}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6820728291316527, "accuracy_n": 59, "auc": 0.6820728291316527}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7307692307692307, "accuracy_n": 23, "auc": 0.7307692307692307}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5020080321285141, "accuracy_n": 996, "auc": 0.5020080321285141}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5333333333333333, "accuracy_n": 990, "auc": 0.5333333333333333}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3058350100603622, "accuracy_n": 497, "auc": 0.3058350100603622}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3106212424849699, "accuracy_n": 499, "auc": 0.3106212424849699}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2656641604010025, "accuracy_n": 399, "auc": 0.2656641604010025}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5040546802594996, "accuracy_n": 322, "auc": 0.5040546802594996}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5510338345864662, "accuracy_n": 292, "auc": 0.5510338345864662}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7651331719128329, "accuracy_n": 413, "auc": 0.7651331719128329}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6598206387119604, "accuracy_n": 1902, "auc": 0.6598206387119604}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5275074301822957, "accuracy_n": 2000, "auc": 0.5275074301822957}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5476190476190476, "accuracy_n": 59, "auc": 0.5476190476190476}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5676767676767677, "accuracy_n": 990, "auc": 0.5676767676767677}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.352112676056338, "accuracy_n": 497, "auc": 0.352112676056338}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.24649298597194388, "accuracy_n": 499, "auc": 0.24649298597194388}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2556390977443609, "accuracy_n": 399, "auc": 0.2556390977443609}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7347466790237874, "accuracy_n": 322, "auc": 0.7347466790237874}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7618421052631579, "accuracy_n": 292, "auc": 0.7618421052631579}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5030022558386411, "accuracy_n": 1902, "auc": 0.5030022558386411}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5497634646107245, "accuracy_n": 2000, "auc": 0.5497634646107245}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5728291316526611, "accuracy_n": 59, "auc": 0.5728291316526611}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6232323232323232, "accuracy_n": 990, "auc": 0.6232323232323232}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.448692152917505, "accuracy_n": 497, "auc": 0.448692152917505}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39879759519038077, "accuracy_n": 499, "auc": 0.39879759519038077}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.33221476510067116, "accuracy_n": 298, "auc": 0.33221476510067116}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.37844611528822053, "accuracy_n": 399, "auc": 0.37844611528822053}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7623185047883844, "accuracy_n": 322, "auc": 0.7623185047883844}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6970394736842105, "accuracy_n": 292, "auc": 0.6970394736842105}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7514740357395611, "accuracy_n": 1902, "auc": 0.7514740357395611}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5150529341092135, "accuracy_n": 2000, "auc": 0.5150529341092135}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.530812324929972, "accuracy_n": 59, "auc": 0.530812324929972}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6465863453815262, "accuracy_n": 996, "auc": 0.6465863453815262}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5222222222222223, "accuracy_n": 990, "auc": 0.5222222222222223}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5663983903420523, "accuracy_n": 994, "auc": 0.5663983903420523}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.45271629778672035, "accuracy_n": 497, "auc": 0.45271629778672035}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.45090180360721444, "accuracy_n": 499, "auc": 0.45090180360721444}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3691275167785235, "accuracy_n": 298, "auc": 0.3691275167785235}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.543859649122807, "accuracy_n": 399, "auc": 0.543859649122807}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8184275563793635, "accuracy_n": 322, "auc": 0.8184275563793635}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7045582706766919, "accuracy_n": 292, "auc": 0.7045582706766919}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7201344656758669, "accuracy_n": 1902, "auc": 0.7201344656758669}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5511799759713256, "accuracy_n": 2000, "auc": 0.5511799759713256}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6204481792717087, "accuracy_n": 59, "auc": 0.6204481792717087}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6054216867469879, "accuracy_n": 996, "auc": 0.6054216867469879}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5848484848484848, "accuracy_n": 990, "auc": 0.5848484848484848}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5925553319919518, "accuracy_n": 994, "auc": 0.5925553319919518}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.45472837022132795, "accuracy_n": 497, "auc": 0.45472837022132795}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.42685370741482964, "accuracy_n": 499, "auc": 0.42685370741482964}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4429530201342282, "accuracy_n": 298, "auc": 0.4429530201342282}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5112781954887218, "accuracy_n": 399, "auc": 0.5112781954887218}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5544554455445545, "accuracy_n": 303, "auc": 0.5544554455445545}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.759924312635156, "accuracy_n": 322, "auc": 0.759924312635156}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6310150375939849, "accuracy_n": 292, "auc": 0.6310150375939849}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5966129246284501, "accuracy_n": 1902, "auc": 0.5966129246284501}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5772253783615885, "accuracy_n": 2000, "auc": 0.5772253783615885}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6428571428571429, "accuracy_n": 59, "auc": 0.6428571428571429}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6898989898989899, "accuracy_n": 990, "auc": 0.6898989898989899}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5253623188405797, "accuracy_n": 276, "auc": 0.5253623188405797}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4346076458752515, "accuracy_n": 497, "auc": 0.4346076458752515}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4969939879759519, "accuracy_n": 499, "auc": 0.4969939879759519}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3825503355704698, "accuracy_n": 298, "auc": 0.3825503355704698}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5789473684210527, "accuracy_n": 399, "auc": 0.5789473684210527}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6303630363036303, "accuracy_n": 303, "auc": 0.6303630363036303}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7861059623107816, "accuracy_n": 322, "auc": 0.7861059623107816}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7905545112781955, "accuracy_n": 292, "auc": 0.7905545112781955}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5235580325548479, "accuracy_n": 1902, "auc": 0.5235580325548479}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5072010995969545, "accuracy_n": 2000, "auc": 0.5072010995969545}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6260504201680672, "accuracy_n": 59, "auc": 0.6260504201680672}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5923076923076924, "accuracy_n": 23, "auc": 0.5923076923076924}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.696969696969697, "accuracy_n": 990, "auc": 0.696969696969697}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6304347826086957, "accuracy_n": 276, "auc": 0.6304347826086957}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.24346076458752516, "accuracy_n": 497, "auc": 0.24346076458752516}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2725450901803607, "accuracy_n": 499, "auc": 0.2725450901803607}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9537953795379538, "accuracy_n": 303, "auc": 0.9537953795379538}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7495752239728144, "accuracy_n": 322, "auc": 0.7495752239728144}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5029135338345865, "accuracy_n": 292, "auc": 0.5029135338345865}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.648910411622276, "accuracy_n": 413, "auc": 0.648910411622276}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6367768046709129, "accuracy_n": 1902, "auc": 0.6367768046709129}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6744264679549317, "accuracy_n": 2000, "auc": 0.6744264679549317}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7324929971988795, "accuracy_n": 59, "auc": 0.7324929971988795}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6363636363636364, "accuracy_n": 990, "auc": 0.6363636363636364}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2716297786720322, "accuracy_n": 497, "auc": 0.2716297786720322}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2665330661322645, "accuracy_n": 499, "auc": 0.2665330661322645}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.19548872180451127, "accuracy_n": 399, "auc": 0.19548872180451127}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9240924092409241, "accuracy_n": 303, "auc": 0.9240924092409241}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6713778189681805, "accuracy_n": 322, "auc": 0.6713778189681805}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5252819548872181, "accuracy_n": 292, "auc": 0.5252819548872181}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7360774818401937, "accuracy_n": 413, "auc": 0.7360774818401937}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6312898089171974, "accuracy_n": 1902, "auc": 0.6312898089171974}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6425819720919252, "accuracy_n": 2000, "auc": 0.6425819720919252}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6946778711484594, "accuracy_n": 59, "auc": 0.6946778711484594}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5818181818181818, "accuracy_n": 990, "auc": 0.5818181818181818}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4245472837022133, "accuracy_n": 497, "auc": 0.4245472837022133}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.36072144288577157, "accuracy_n": 499, "auc": 0.36072144288577157}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3634085213032581, "accuracy_n": 399, "auc": 0.3634085213032581}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5973597359735974, "accuracy_n": 303, "auc": 0.5973597359735974}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.93817578004325, "accuracy_n": 322, "auc": 0.93817578004325}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.893890977443609, "accuracy_n": 292, "auc": 0.893890977443609}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5158107749469214, "accuracy_n": 1902, "auc": 0.5158107749469214}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7774331533683658, "accuracy_n": 2000, "auc": 0.7774331533683658}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.799719887955182, "accuracy_n": 59, "auc": 0.799719887955182}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5252525252525253, "accuracy_n": 990, "auc": 0.5252525252525253}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3440643863179074, "accuracy_n": 497, "auc": 0.3440643863179074}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.34468937875751504, "accuracy_n": 499, "auc": 0.34468937875751504}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.37593984962406013, "accuracy_n": 399, "auc": 0.37593984962406013}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5313531353135313, "accuracy_n": 303, "auc": 0.5313531353135313}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8842292245906704, "accuracy_n": 322, "auc": 0.8842292245906704}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8986842105263158, "accuracy_n": 292, "auc": 0.8986842105263158}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5373606687898089, "accuracy_n": 1902, "auc": 0.5373606687898089}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7390457955321871, "accuracy_n": 2000, "auc": 0.7390457955321871}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7492997198879552, "accuracy_n": 59, "auc": 0.7492997198879552}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5141414141414141, "accuracy_n": 990, "auc": 0.5141414141414141}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2977867203219316, "accuracy_n": 497, "auc": 0.2977867203219316}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3246492985971944, "accuracy_n": 499, "auc": 0.3246492985971944}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6963696369636964, "accuracy_n": 303, "auc": 0.6963696369636964}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.592948717948718, "accuracy_n": 322, "auc": 0.592948717948718}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5395676691729323, "accuracy_n": 292, "auc": 0.5395676691729323}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7360774818401937, "accuracy_n": 413, "auc": 0.7360774818401937}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.520853237791932, "accuracy_n": 1902, "auc": 0.520853237791932}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6077619020466388, "accuracy_n": 2000, "auc": 0.6077619020466388}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6036414565826331, "accuracy_n": 59, "auc": 0.6036414565826331}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5973895582329317, "accuracy_n": 996, "auc": 0.5973895582329317}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7656565656565657, "accuracy_n": 990, "auc": 0.7656565656565657}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5603621730382293, "accuracy_n": 994, "auc": 0.5603621730382293}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.34808853118712274, "accuracy_n": 497, "auc": 0.34808853118712274}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.34468937875751504, "accuracy_n": 499, "auc": 0.34468937875751504}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.18295739348370926, "accuracy_n": 399, "auc": 0.18295739348370926}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7821782178217822, "accuracy_n": 303, "auc": 0.7821782178217822}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5862068965517241, "accuracy_n": 58, "auc": 0.5862068965517241}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270698177324684, "accuracy_n": 322, "auc": 0.5270698177324684}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5452537593984963, "accuracy_n": 292, "auc": 0.5452537593984963}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9302547770700637, "accuracy_n": 1902, "auc": 0.9302547770700637}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5421457146029717, "accuracy_n": 2000, "auc": 0.5421457146029717}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6148459383753502, "accuracy_n": 59, "auc": 0.6148459383753502}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6384615384615384, "accuracy_n": 23, "auc": 0.6384615384615384}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5491967871485943, "accuracy_n": 996, "auc": 0.5491967871485943}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8939393939393939, "accuracy_n": 990, "auc": 0.8939393939393939}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3299798792756539, "accuracy_n": 497, "auc": 0.3299798792756539}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2725450901803607, "accuracy_n": 499, "auc": 0.2725450901803607}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.19799498746867167, "accuracy_n": 399, "auc": 0.19799498746867167}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6864686468646864, "accuracy_n": 303, "auc": 0.6864686468646864}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8462310781587891, "accuracy_n": 322, "auc": 0.8462310781587891}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7875939849624061, "accuracy_n": 292, "auc": 0.7875939849624061}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5811138014527845, "accuracy_n": 413, "auc": 0.5811138014527845}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6409080856334041, "accuracy_n": 1902, "auc": 0.6409080856334041}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8176091569056431, "accuracy_n": 2000, "auc": 0.8176091569056431}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7450980392156863, "accuracy_n": 59, "auc": 0.7450980392156863}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6818181818181818, "accuracy_n": 990, "auc": 0.6818181818181818}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2857142857142857, "accuracy_n": 497, "auc": 0.2857142857142857}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.342685370741483, "accuracy_n": 499, "auc": 0.342685370741483}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2832080200501253, "accuracy_n": 399, "auc": 0.2832080200501253}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9075907590759076, "accuracy_n": 303, "auc": 0.9075907590759076}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8420991658943466, "accuracy_n": 322, "auc": 0.8420991658943466}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7697368421052633, "accuracy_n": 292, "auc": 0.7697368421052633}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.795308076786978, "accuracy_n": 1902, "auc": 0.795308076786978}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6680981834442232, "accuracy_n": 2000, "auc": 0.6680981834442232}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7212885154061625, "accuracy_n": 59, "auc": 0.7212885154061625}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6134538152610441, "accuracy_n": 996, "auc": 0.6134538152610441}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5181818181818182, "accuracy_n": 990, "auc": 0.5181818181818182}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3319919517102616, "accuracy_n": 497, "auc": 0.3319919517102616}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3246492985971944, "accuracy_n": 499, "auc": 0.3246492985971944}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.23057644110275688, "accuracy_n": 399, "auc": 0.23057644110275688}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6204819277108433, "accuracy_n": 322, "auc": 0.6204819277108433}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5622650375939849, "accuracy_n": 292, "auc": 0.5622650375939849}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5907990314769975, "accuracy_n": 413, "auc": 0.5907990314769975}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7231510969568294, "accuracy_n": 1902, "auc": 0.7231510969568294}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5056165275664515, "accuracy_n": 2000, "auc": 0.5056165275664515}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6134453781512605, "accuracy_n": 59, "auc": 0.6134453781512605}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
