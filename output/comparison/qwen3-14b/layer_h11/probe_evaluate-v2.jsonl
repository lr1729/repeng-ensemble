{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6252525252525253, "accuracy_n": 990, "auc": 0.6252525252525253}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26961770623742454, "accuracy_n": 497, "auc": 0.26961770623742454}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2865731462925852, "accuracy_n": 499, "auc": 0.2865731462925852}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20300751879699247, "accuracy_n": 399, "auc": 0.20300751879699247}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5511551155115512, "accuracy_n": 303, "auc": 0.5511551155115512}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5361059623107816, "accuracy_n": 322, "auc": 0.5361059623107816}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5109022556390977, "accuracy_n": 292, "auc": 0.5109022556390977}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6378151539278132, "accuracy_n": 1902, "auc": 0.6378151539278132}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6004927778928193, "accuracy_n": 2000, "auc": 0.6004927778928193}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5076923076923078, "accuracy_n": 23, "auc": 0.5076923076923078}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5903614457831325, "accuracy_n": 996, "auc": 0.5903614457831325}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 990, "auc": 0.6}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28772635814889336, "accuracy_n": 497, "auc": 0.28772635814889336}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3087248322147651, "accuracy_n": 298, "auc": 0.3087248322147651}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24812030075187969, "accuracy_n": 399, "auc": 0.24812030075187969}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5016501650165016, "accuracy_n": 303, "auc": 0.5016501650165016}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5954973741118319, "accuracy_n": 322, "auc": 0.5954973741118319}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.513157894736842, "accuracy_n": 292, "auc": 0.513157894736842}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8009631546355273, "accuracy_n": 1902, "auc": 0.8009631546355273}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6478038571924465, "accuracy_n": 2000, "auc": 0.6478038571924465}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5910364145658263, "accuracy_n": 59, "auc": 0.5910364145658263}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5100401606425703, "accuracy_n": 996, "auc": 0.5100401606425703}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5747474747474748, "accuracy_n": 990, "auc": 0.5747474747474748}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.605072463768116, "accuracy_n": 276, "auc": 0.605072463768116}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3058350100603622, "accuracy_n": 497, "auc": 0.3058350100603622}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.18045112781954886, "accuracy_n": 399, "auc": 0.18045112781954886}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6707599629286377, "accuracy_n": 322, "auc": 0.6707599629286377}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7442199248120301, "accuracy_n": 292, "auc": 0.7442199248120301}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5494703202406228, "accuracy_n": 1902, "auc": 0.5494703202406228}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5649659527089279, "accuracy_n": 2000, "auc": 0.5649659527089279}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5630252100840336, "accuracy_n": 59, "auc": 0.5630252100840336}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5565656565656566, "accuracy_n": 990, "auc": 0.5565656565656566}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3118712273641851, "accuracy_n": 497, "auc": 0.3118712273641851}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27054108216432865, "accuracy_n": 499, "auc": 0.27054108216432865}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3053691275167785, "accuracy_n": 298, "auc": 0.3053691275167785}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6374729687982701, "accuracy_n": 322, "auc": 0.6374729687982701}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6498590225563909, "accuracy_n": 292, "auc": 0.6498590225563909}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6519970806794054, "accuracy_n": 1902, "auc": 0.6519970806794054}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5613166353053454, "accuracy_n": 2000, "auc": 0.5613166353053454}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5658263305322129, "accuracy_n": 59, "auc": 0.5658263305322129}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5153846153846154, "accuracy_n": 23, "auc": 0.5153846153846154}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5242424242424243, "accuracy_n": 990, "auc": 0.5242424242424243}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.317907444668008, "accuracy_n": 497, "auc": 0.317907444668008}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.33266533066132264, "accuracy_n": 499, "auc": 0.33266533066132264}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3187919463087248, "accuracy_n": 298, "auc": 0.3187919463087248}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20551378446115287, "accuracy_n": 399, "auc": 0.20551378446115287}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5998995983935744, "accuracy_n": 322, "auc": 0.5998995983935744}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6953947368421053, "accuracy_n": 292, "auc": 0.6953947368421053}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5935885527246992, "accuracy_n": 1902, "auc": 0.5935885527246992}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5302594236519383, "accuracy_n": 2000, "auc": 0.5302594236519383}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6022408963585435, "accuracy_n": 59, "auc": 0.6022408963585435}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5150602409638554, "accuracy_n": 996, "auc": 0.5150602409638554}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.604040404040404, "accuracy_n": 990, "auc": 0.604040404040404}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.4024144869215292, "accuracy_n": 497, "auc": 0.4024144869215292}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3366733466933868, "accuracy_n": 499, "auc": 0.3366733466933868}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3187919463087248, "accuracy_n": 298, "auc": 0.3187919463087248}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24561403508771928, "accuracy_n": 399, "auc": 0.24561403508771928}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6303630363036303, "accuracy_n": 303, "auc": 0.6303630363036303}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5108897126969416, "accuracy_n": 322, "auc": 0.5108897126969416}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5476503759398497, "accuracy_n": 292, "auc": 0.5476503759398497}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6680765658174097, "accuracy_n": 1902, "auc": 0.6680765658174097}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5351882029412618, "accuracy_n": 2000, "auc": 0.5351882029412618}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5266106442577031, "accuracy_n": 59, "auc": 0.5266106442577031}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5180722891566265, "accuracy_n": 996, "auc": 0.5180722891566265}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.498989898989899, "accuracy_n": 990, "auc": 0.498989898989899}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.386317907444668, "accuracy_n": 497, "auc": 0.386317907444668}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.39478957915831664, "accuracy_n": 499, "auc": 0.39478957915831664}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2807017543859649, "accuracy_n": 399, "auc": 0.2807017543859649}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6733858510966945, "accuracy_n": 322, "auc": 0.6733858510966945}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6740131578947368, "accuracy_n": 292, "auc": 0.6740131578947368}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5399515738498789, "accuracy_n": 413, "auc": 0.5399515738498789}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5620023885350318, "accuracy_n": 1902, "auc": 0.5620023885350318}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5744073610573417, "accuracy_n": 2000, "auc": 0.5744073610573417}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.542016806722689, "accuracy_n": 59, "auc": 0.542016806722689}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5676767676767677, "accuracy_n": 990, "auc": 0.5676767676767677}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3641851106639839, "accuracy_n": 497, "auc": 0.3641851106639839}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28256513026052105, "accuracy_n": 499, "auc": 0.28256513026052105}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3389261744966443, "accuracy_n": 298, "auc": 0.3389261744966443}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2581453634085213, "accuracy_n": 399, "auc": 0.2581453634085213}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5379537953795379, "accuracy_n": 303, "auc": 0.5379537953795379}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6823447636700649, "accuracy_n": 322, "auc": 0.6823447636700649}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.712406015037594, "accuracy_n": 292, "auc": 0.712406015037594}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6865412685774948, "accuracy_n": 1902, "auc": 0.6865412685774948}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5698947319982514, "accuracy_n": 2000, "auc": 0.5698947319982514}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5252100840336135, "accuracy_n": 59, "auc": 0.5252100840336135}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5140562248995983, "accuracy_n": 996, "auc": 0.5140562248995983}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6787878787878788, "accuracy_n": 990, "auc": 0.6787878787878788}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5362318840579711, "accuracy_n": 276, "auc": 0.5362318840579711}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3561368209255533, "accuracy_n": 497, "auc": 0.3561368209255533}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3527054108216433, "accuracy_n": 499, "auc": 0.3527054108216433}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.30201342281879195, "accuracy_n": 298, "auc": 0.30201342281879195}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.40100250626566414, "accuracy_n": 399, "auc": 0.40100250626566414}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6457754093296262, "accuracy_n": 322, "auc": 0.6457754093296262}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6111842105263157, "accuracy_n": 292, "auc": 0.6111842105263157}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6116242038216562, "accuracy_n": 1902, "auc": 0.6116242038216562}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.508061410169071, "accuracy_n": 2000, "auc": 0.508061410169071}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5182072829131652, "accuracy_n": 59, "auc": 0.5182072829131652}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.502020202020202, "accuracy_n": 990, "auc": 0.502020202020202}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27565392354124746, "accuracy_n": 497, "auc": 0.27565392354124746}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2665330661322645, "accuracy_n": 499, "auc": 0.2665330661322645}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.22305764411027568, "accuracy_n": 399, "auc": 0.22305764411027568}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8976897689768977, "accuracy_n": 303, "auc": 0.8976897689768977}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6164272474513439, "accuracy_n": 322, "auc": 0.6164272474513439}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6833646616541353, "accuracy_n": 292, "auc": 0.6833646616541353}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5017571213729654, "accuracy_n": 1902, "auc": 0.5017571213729654}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6428350634579083, "accuracy_n": 2000, "auc": 0.6428350634579083}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7212885154061625, "accuracy_n": 59, "auc": 0.7212885154061625}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5454545454545454, "accuracy_n": 990, "auc": 0.5454545454545454}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27364185110663986, "accuracy_n": 497, "auc": 0.27364185110663986}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3106212424849699, "accuracy_n": 499, "auc": 0.3106212424849699}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.23825503355704697, "accuracy_n": 298, "auc": 0.23825503355704697}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.528052805280528, "accuracy_n": 303, "auc": 0.528052805280528}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6528421377818967, "accuracy_n": 322, "auc": 0.6528421377818967}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6892857142857142, "accuracy_n": 292, "auc": 0.6892857142857142}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5002742392073602, "accuracy_n": 1902, "auc": 0.5002742392073602}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5695516081305351, "accuracy_n": 2000, "auc": 0.5695516081305351}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6302521008403362, "accuracy_n": 59, "auc": 0.6302521008403362}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6153846153846154, "accuracy_n": 23, "auc": 0.6153846153846154}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 990, "auc": 0.5}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.532608695652174, "accuracy_n": 276, "auc": 0.532608695652174}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2917505030181087, "accuracy_n": 497, "auc": 0.2917505030181087}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2985971943887776, "accuracy_n": 499, "auc": 0.2985971943887776}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3221476510067114, "accuracy_n": 298, "auc": 0.3221476510067114}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24310776942355888, "accuracy_n": 399, "auc": 0.24310776942355888}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8345690454124188, "accuracy_n": 322, "auc": 0.8345690454124188}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8603383458646616, "accuracy_n": 292, "auc": 0.8603383458646616}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6004842615012107, "accuracy_n": 413, "auc": 0.6004842615012107}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6003958775654636, "accuracy_n": 1902, "auc": 0.6003958775654636}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.676183102099858, "accuracy_n": 2000, "auc": 0.676183102099858}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5896358543417366, "accuracy_n": 59, "auc": 0.5896358543417366}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5050505050505051, "accuracy_n": 990, "auc": 0.5050505050505051}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5072463768115942, "accuracy_n": 276, "auc": 0.5072463768115942}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2635814889336016, "accuracy_n": 497, "auc": 0.2635814889336016}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.32550335570469796, "accuracy_n": 298, "auc": 0.32550335570469796}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24561403508771928, "accuracy_n": 399, "auc": 0.24561403508771928}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5346534653465347, "accuracy_n": 303, "auc": 0.5346534653465347}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7863376583256101, "accuracy_n": 322, "auc": 0.7863376583256101}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8322368421052632, "accuracy_n": 292, "auc": 0.8322368421052632}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6077481840193705, "accuracy_n": 413, "auc": 0.6077481840193705}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.562212491153574, "accuracy_n": 1902, "auc": 0.562212491153574}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7021814875169936, "accuracy_n": 2000, "auc": 0.7021814875169936}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5350140056022409, "accuracy_n": 59, "auc": 0.5350140056022409}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5818181818181818, "accuracy_n": 990, "auc": 0.5818181818181818}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28169014084507044, "accuracy_n": 497, "auc": 0.28169014084507044}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2565130260521042, "accuracy_n": 499, "auc": 0.2565130260521042}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20300751879699247, "accuracy_n": 399, "auc": 0.20300751879699247}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5445544554455446, "accuracy_n": 303, "auc": 0.5445544554455446}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5141334569045413, "accuracy_n": 322, "auc": 0.5141334569045413}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5207236842105263, "accuracy_n": 292, "auc": 0.5207236842105263}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5956416464891041, "accuracy_n": 413, "auc": 0.5956416464891041}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5250320682944092, "accuracy_n": 1902, "auc": 0.5250320682944092}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5149228871622655, "accuracy_n": 2000, "auc": 0.5149228871622655}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6120448179271709, "accuracy_n": 59, "auc": 0.6120448179271709}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5240963855421686, "accuracy_n": 996, "auc": 0.5240963855421686}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6232323232323232, "accuracy_n": 990, "auc": 0.6232323232323232}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3299798792756539, "accuracy_n": 497, "auc": 0.3299798792756539}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3046092184368738, "accuracy_n": 499, "auc": 0.3046092184368738}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.32550335570469796, "accuracy_n": 298, "auc": 0.32550335570469796}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.19548872180451127, "accuracy_n": 399, "auc": 0.19548872180451127}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5115511551155115, "accuracy_n": 303, "auc": 0.5115511551155115}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5682344763670065, "accuracy_n": 322, "auc": 0.5682344763670065}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5852443609022556, "accuracy_n": 292, "auc": 0.5852443609022556}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9146286712668082, "accuracy_n": 1902, "auc": 0.9146286712668082}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.507099062761657, "accuracy_n": 2000, "auc": 0.507099062761657}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5826330532212886, "accuracy_n": 59, "auc": 0.5826330532212886}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7848484848484848, "accuracy_n": 990, "auc": 0.7848484848484848}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24949698189134809, "accuracy_n": 497, "auc": 0.24949698189134809}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2665330661322645, "accuracy_n": 499, "auc": 0.2665330661322645}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31208053691275167, "accuracy_n": 298, "auc": 0.31208053691275167}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.22807017543859648, "accuracy_n": 399, "auc": 0.22807017543859648}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5181518151815182, "accuracy_n": 303, "auc": 0.5181518151815182}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7315415508186592, "accuracy_n": 322, "auc": 0.7315415508186592}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7550751879699249, "accuracy_n": 292, "auc": 0.7550751879699249}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5254237288135594, "accuracy_n": 413, "auc": 0.5254237288135594}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5300568382873319, "accuracy_n": 1902, "auc": 0.5300568382873319}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7775466943566628, "accuracy_n": 2000, "auc": 0.7775466943566628}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5952380952380952, "accuracy_n": 59, "auc": 0.5952380952380952}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5153846153846153, "accuracy_n": 23, "auc": 0.5153846153846153}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5160642570281124, "accuracy_n": 996, "auc": 0.5160642570281124}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6616161616161617, "accuracy_n": 990, "auc": 0.6616161616161617}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31388329979879276, "accuracy_n": 497, "auc": 0.31388329979879276}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28523489932885904, "accuracy_n": 298, "auc": 0.28523489932885904}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.858085808580858, "accuracy_n": 303, "auc": 0.858085808580858}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6384383688600557, "accuracy_n": 322, "auc": 0.6384383688600557}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6459586466165413, "accuracy_n": 292, "auc": 0.6459586466165413}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7593174982307148, "accuracy_n": 1902, "auc": 0.7593174982307148}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5920177183963411, "accuracy_n": 2000, "auc": 0.5920177183963411}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6442577030812325, "accuracy_n": 59, "auc": 0.6442577030812325}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5170682730923695, "accuracy_n": 996, "auc": 0.5170682730923695}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5131313131313131, "accuracy_n": 990, "auc": 0.5131313131313131}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26961770623742454, "accuracy_n": 497, "auc": 0.26961770623742454}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.21052631578947367, "accuracy_n": 399, "auc": 0.21052631578947367}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5445544554455446, "accuracy_n": 303, "auc": 0.5445544554455446}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5813639172072906, "accuracy_n": 322, "auc": 0.5813639172072906}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5401785714285714, "accuracy_n": 292, "auc": 0.5401785714285714}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5423728813559322, "accuracy_n": 413, "auc": 0.5423728813559322}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7078921178343949, "accuracy_n": 1902, "auc": 0.7078921178343949}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5167010290714948, "accuracy_n": 2000, "auc": 0.5167010290714948}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6162464985994398, "accuracy_n": 59, "auc": 0.6162464985994398}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8615384615384616, "accuracy_n": 23, "auc": 0.8615384615384616}}
