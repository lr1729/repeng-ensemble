{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9196787148594378, "accuracy_n": 996, "auc": 0.9196787148594378}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9282828282828283, "accuracy_n": 990, "auc": 0.9282828282828283}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 276, "auc": 0.782608695652174}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7997987927565392, "accuracy_n": 994, "auc": 0.7997987927565392}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36619718309859156, "accuracy_n": 497, "auc": 0.36619718309859156}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.24448897795591182, "accuracy_n": 499, "auc": 0.24448897795591182}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.33557046979865773, "accuracy_n": 298, "auc": 0.33557046979865773}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2506265664160401, "accuracy_n": 399, "auc": 0.2506265664160401}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9872567191844301, "accuracy_n": 322, "auc": 0.9872567191844301}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7124060150375942, "accuracy_n": 292, "auc": 0.7124060150375942}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8673257254069355, "accuracy_n": 1902, "auc": 0.8673257254069355}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8363679288223049, "accuracy_n": 2000, "auc": 0.8363679288223049}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9453781512605042, "accuracy_n": 59, "auc": 0.9453781512605042}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9538461538461539, "accuracy_n": 23, "auc": 0.9538461538461539}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9246987951807228, "accuracy_n": 996, "auc": 0.9246987951807228}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9474747474747475, "accuracy_n": 990, "auc": 0.9474747474747475}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7934782608695652, "accuracy_n": 276, "auc": 0.7934782608695652}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.89, "accuracy_n": 100, "auc": 0.89}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7997987927565392, "accuracy_n": 994, "auc": 0.7997987927565392}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.33601609657947684, "accuracy_n": 497, "auc": 0.33601609657947684}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.250501002004008, "accuracy_n": 499, "auc": 0.250501002004008}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.19298245614035087, "accuracy_n": 399, "auc": 0.19298245614035087}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9824683348779735, "accuracy_n": 322, "auc": 0.9824683348779735}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7638157894736841, "accuracy_n": 292, "auc": 0.7638157894736841}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8929217533616418, "accuracy_n": 1902, "auc": 0.8929217533616418}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8453751804401388, "accuracy_n": 2000, "auc": 0.8453751804401388}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9061624649859944, "accuracy_n": 59, "auc": 0.9061624649859944}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 23, "auc": 0.9}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9257028112449799, "accuracy_n": 996, "auc": 0.9257028112449799}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9414141414141414, "accuracy_n": 990, "auc": 0.9414141414141414}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.782608695652174, "accuracy_n": 276, "auc": 0.782608695652174}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7716297786720322, "accuracy_n": 994, "auc": 0.7716297786720322}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5855130784708249, "accuracy_n": 497, "auc": 0.5855130784708249}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5170340681362725, "accuracy_n": 499, "auc": 0.5170340681362725}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.540268456375839, "accuracy_n": 298, "auc": 0.540268456375839}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5639097744360902, "accuracy_n": 399, "auc": 0.5639097744360902}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9907707754093297, "accuracy_n": 322, "auc": 0.9907707754093297}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7262687969924811, "accuracy_n": 292, "auc": 0.7262687969924811}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7978702229299363, "accuracy_n": 1902, "auc": 0.7978702229299363}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8721478453721794, "accuracy_n": 2000, "auc": 0.8721478453721794}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9397759103641457, "accuracy_n": 59, "auc": 0.9397759103641457}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9615384615384616, "accuracy_n": 23, "auc": 0.9615384615384616}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.928714859437751, "accuracy_n": 996, "auc": 0.928714859437751}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9393939393939394, "accuracy_n": 990, "auc": 0.9393939393939394}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7608695652173914, "accuracy_n": 276, "auc": 0.7608695652173914}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7816901408450704, "accuracy_n": 994, "auc": 0.7816901408450704}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.44668008048289737, "accuracy_n": 497, "auc": 0.44668008048289737}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.348993288590604, "accuracy_n": 298, "auc": 0.348993288590604}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2832080200501253, "accuracy_n": 399, "auc": 0.2832080200501253}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9810395427865307, "accuracy_n": 322, "auc": 0.9810395427865307}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7440319548872181, "accuracy_n": 292, "auc": 0.7440319548872181}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7877731334041046, "accuracy_n": 1902, "auc": 0.7877731334041046}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8486503627809638, "accuracy_n": 2000, "auc": 0.8486503627809638}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9523809523809523, "accuracy_n": 59, "auc": 0.9523809523809523}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.976923076923077, "accuracy_n": 23, "auc": 0.976923076923077}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5371485943775101, "accuracy_n": 996, "auc": 0.5371485943775101}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9404040404040404, "accuracy_n": 990, "auc": 0.9404040404040404}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.39436619718309857, "accuracy_n": 497, "auc": 0.39436619718309857}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3066132264529058, "accuracy_n": 499, "auc": 0.3066132264529058}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2756892230576441, "accuracy_n": 399, "auc": 0.2756892230576441}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9372937293729373, "accuracy_n": 303, "auc": 0.9372937293729373}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9726984862527032, "accuracy_n": 322, "auc": 0.9726984862527032}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7339285714285716, "accuracy_n": 292, "auc": 0.7339285714285716}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6025118320948337, "accuracy_n": 1902, "auc": 0.6025118320948337}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8042783444823581, "accuracy_n": 2000, "auc": 0.8042783444823581}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8585434173669467, "accuracy_n": 59, "auc": 0.8585434173669467}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6307692307692307, "accuracy_n": 23, "auc": 0.6307692307692307}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6114457831325302, "accuracy_n": 996, "auc": 0.6114457831325302}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7252525252525253, "accuracy_n": 990, "auc": 0.7252525252525253}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.605072463768116, "accuracy_n": 276, "auc": 0.605072463768116}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.71, "accuracy_n": 100, "auc": 0.71}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6639839034205232, "accuracy_n": 994, "auc": 0.6639839034205232}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.579476861167002, "accuracy_n": 497, "auc": 0.579476861167002}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5791583166332666, "accuracy_n": 499, "auc": 0.5791583166332666}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5268456375838926, "accuracy_n": 298, "auc": 0.5268456375838926}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.656641604010025, "accuracy_n": 399, "auc": 0.656641604010025}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5610561056105611, "accuracy_n": 303, "auc": 0.5610561056105611}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.607429718875502, "accuracy_n": 322, "auc": 0.607429718875502}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8557800751879698, "accuracy_n": 292, "auc": 0.8557800751879698}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.639225181598063, "accuracy_n": 413, "auc": 0.639225181598063}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.77147580502477, "accuracy_n": 1902, "auc": 0.77147580502477}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5383303372517478, "accuracy_n": 2000, "auc": 0.5383303372517478}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5196078431372548, "accuracy_n": 59, "auc": 0.5196078431372548}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6153846153846153, "accuracy_n": 23, "auc": 0.6153846153846153}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6967871485943775, "accuracy_n": 996, "auc": 0.6967871485943775}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5595959595959596, "accuracy_n": 990, "auc": 0.5595959595959596}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6630434782608695, "accuracy_n": 276, "auc": 0.6630434782608695}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.51, "accuracy_n": 100, "auc": 0.51}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5563380281690141, "accuracy_n": 994, "auc": 0.5563380281690141}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6096579476861167, "accuracy_n": 497, "auc": 0.6096579476861167}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5851703406813628, "accuracy_n": 499, "auc": 0.5851703406813628}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5503355704697986, "accuracy_n": 298, "auc": 0.5503355704697986}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7042606516290727, "accuracy_n": 399, "auc": 0.7042606516290727}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5907590759075908, "accuracy_n": 303, "auc": 0.5907590759075908}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6134151992585728, "accuracy_n": 322, "auc": 0.6134151992585728}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8322838345864663, "accuracy_n": 292, "auc": 0.8322838345864663}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7481840193704601, "accuracy_n": 413, "auc": 0.7481840193704601}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5790361818825194, "accuracy_n": 1902, "auc": 0.5790361818825194}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.550634779155275, "accuracy_n": 2000, "auc": 0.550634779155275}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6274509803921569, "accuracy_n": 59, "auc": 0.6274509803921569}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8895582329317269, "accuracy_n": 996, "auc": 0.8895582329317269}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7353535353535353, "accuracy_n": 990, "auc": 0.7353535353535353}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7028985507246377, "accuracy_n": 276, "auc": 0.7028985507246377}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 100, "auc": 0.5}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5392354124748491, "accuracy_n": 994, "auc": 0.5392354124748491}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6338028169014085, "accuracy_n": 497, "auc": 0.6338028169014085}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6392785571142284, "accuracy_n": 499, "auc": 0.6392785571142284}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6409395973154363, "accuracy_n": 298, "auc": 0.6409395973154363}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7243107769423559, "accuracy_n": 399, "auc": 0.7243107769423559}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6204620462046204, "accuracy_n": 303, "auc": 0.6204620462046204}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6724137931034483, "accuracy_n": 58, "auc": 0.6724137931034483}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5740268767377201, "accuracy_n": 322, "auc": 0.5740268767377201}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7348684210526315, "accuracy_n": 292, "auc": 0.7348684210526315}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6198547215496368, "accuracy_n": 413, "auc": 0.6198547215496368}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6040240180467091, "accuracy_n": 1902, "auc": 0.6040240180467091}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5694235619058481, "accuracy_n": 2000, "auc": 0.5694235619058481}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5756302521008403, "accuracy_n": 59, "auc": 0.5756302521008403}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5301204819277109, "accuracy_n": 996, "auc": 0.5301204819277109}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5656565656565656, "accuracy_n": 990, "auc": 0.5656565656565656}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7536231884057971, "accuracy_n": 276, "auc": 0.7536231884057971}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7082494969818913, "accuracy_n": 994, "auc": 0.7082494969818913}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6036217303822937, "accuracy_n": 497, "auc": 0.6036217303822937}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6232464929859719, "accuracy_n": 499, "auc": 0.6232464929859719}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6342281879194631, "accuracy_n": 298, "auc": 0.6342281879194631}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6967418546365914, "accuracy_n": 399, "auc": 0.6967418546365914}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8184818481848185, "accuracy_n": 303, "auc": 0.8184818481848185}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8448275862068966, "accuracy_n": 58, "auc": 0.8448275862068966}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5447173308619091, "accuracy_n": 322, "auc": 0.5447173308619091}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.824812030075188, "accuracy_n": 292, "auc": 0.824812030075188}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5569007263922519, "accuracy_n": 413, "auc": 0.5569007263922519}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5496505661712667, "accuracy_n": 1902, "auc": 0.5496505661712667}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5268321864192974, "accuracy_n": 2000, "auc": 0.5268321864192974}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5140056022408963, "accuracy_n": 59, "auc": 0.5140056022408963}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9206827309236948, "accuracy_n": 996, "auc": 0.9206827309236948}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9424242424242424, "accuracy_n": 990, "auc": 0.9424242424242424}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 276, "auc": 0.75}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8138832997987927, "accuracy_n": 994, "auc": 0.8138832997987927}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.38028169014084506, "accuracy_n": 497, "auc": 0.38028169014084506}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3787575150300601, "accuracy_n": 499, "auc": 0.3787575150300601}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3187919463087248, "accuracy_n": 298, "auc": 0.3187919463087248}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.37844611528822053, "accuracy_n": 399, "auc": 0.37844611528822053}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9966996699669967, "accuracy_n": 303, "auc": 0.9966996699669967}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9917361754711151, "accuracy_n": 322, "auc": 0.9917361754711151}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7442199248120301, "accuracy_n": 292, "auc": 0.7442199248120301}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5826035031847134, "accuracy_n": 1902, "auc": 0.5826035031847134}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8263663182408849, "accuracy_n": 2000, "auc": 0.8263663182408849}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9551820728291316, "accuracy_n": 59, "auc": 0.9551820728291316}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7230769230769231, "accuracy_n": 23, "auc": 0.7230769230769231}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9246987951807228, "accuracy_n": 996, "auc": 0.9246987951807228}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9414141414141414, "accuracy_n": 990, "auc": 0.9414141414141414}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6594202898550725, "accuracy_n": 276, "auc": 0.6594202898550725}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.73, "accuracy_n": 100, "auc": 0.73}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7585513078470825, "accuracy_n": 994, "auc": 0.7585513078470825}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.40040241448692154, "accuracy_n": 497, "auc": 0.40040241448692154}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.43887775551102204, "accuracy_n": 499, "auc": 0.43887775551102204}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3724832214765101, "accuracy_n": 298, "auc": 0.3724832214765101}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.45363408521303256, "accuracy_n": 399, "auc": 0.45363408521303256}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 303, "auc": 1.0}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9906549274019153, "accuracy_n": 322, "auc": 0.9906549274019153}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.719031954887218, "accuracy_n": 292, "auc": 0.719031954887218}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7203943294409059, "accuracy_n": 1902, "auc": 0.7203943294409059}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7931803381020549, "accuracy_n": 2000, "auc": 0.7931803381020549}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9005602240896358, "accuracy_n": 59, "auc": 0.9005602240896358}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9216867469879518, "accuracy_n": 996, "auc": 0.9216867469879518}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9414141414141414, "accuracy_n": 990, "auc": 0.9414141414141414}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 276, "auc": 0.75}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8138832997987927, "accuracy_n": 994, "auc": 0.8138832997987927}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2716297786720322, "accuracy_n": 497, "auc": 0.2716297786720322}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.27054108216432865, "accuracy_n": 499, "auc": 0.27054108216432865}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.20050125313283207, "accuracy_n": 399, "auc": 0.20050125313283207}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9945165276490577, "accuracy_n": 322, "auc": 0.9945165276490577}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7912593984962406, "accuracy_n": 292, "auc": 0.7912593984962406}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5328390392781317, "accuracy_n": 1902, "auc": 0.5328390392781317}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8645360975312087, "accuracy_n": 2000, "auc": 0.8645360975312087}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9761904761904762, "accuracy_n": 59, "auc": 0.9761904761904762}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7538461538461539, "accuracy_n": 23, "auc": 0.7538461538461539}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8714859437751004, "accuracy_n": 996, "auc": 0.8714859437751004}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9404040404040404, "accuracy_n": 990, "auc": 0.9404040404040404}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7572463768115942, "accuracy_n": 276, "auc": 0.7572463768115942}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7967806841046278, "accuracy_n": 994, "auc": 0.7967806841046278}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4567404426559356, "accuracy_n": 497, "auc": 0.4567404426559356}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.39478957915831664, "accuracy_n": 499, "auc": 0.39478957915831664}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.37583892617449666, "accuracy_n": 298, "auc": 0.37583892617449666}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.43859649122807015, "accuracy_n": 399, "auc": 0.43859649122807015}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9875656472042015, "accuracy_n": 322, "auc": 0.9875656472042015}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9024906015037594, "accuracy_n": 292, "auc": 0.9024906015037594}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5795315817409767, "accuracy_n": 1902, "auc": 0.5795315817409767}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8641059422451506, "accuracy_n": 2000, "auc": 0.8641059422451506}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9341736694677871, "accuracy_n": 59, "auc": 0.9341736694677871}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9076305220883534, "accuracy_n": 996, "auc": 0.9076305220883534}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9464646464646465, "accuracy_n": 990, "auc": 0.9464646464646465}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6413043478260869, "accuracy_n": 276, "auc": 0.6413043478260869}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7746478873239436, "accuracy_n": 994, "auc": 0.7746478873239436}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.28169014084507044, "accuracy_n": 497, "auc": 0.28169014084507044}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.27181208053691275, "accuracy_n": 298, "auc": 0.27181208053691275}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.18546365914786966, "accuracy_n": 399, "auc": 0.18546365914786966}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900990099009901, "accuracy_n": 303, "auc": 0.9900990099009901}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9935897435897436, "accuracy_n": 322, "auc": 0.9935897435897436}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7761278195488722, "accuracy_n": 292, "auc": 0.7761278195488722}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6624646142958244, "accuracy_n": 1902, "auc": 0.6624646142958244}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8383286366378262, "accuracy_n": 2000, "auc": 0.8383286366378262}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9607843137254902, "accuracy_n": 59, "auc": 0.9607843137254902}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9106425702811245, "accuracy_n": 996, "auc": 0.9106425702811245}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9090909090909091, "accuracy_n": 990, "auc": 0.9090909090909091}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6666666666666666, "accuracy_n": 276, "auc": 0.6666666666666666}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.59, "accuracy_n": 100, "auc": 0.59}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5472837022132797, "accuracy_n": 994, "auc": 0.5472837022132797}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4164989939637827, "accuracy_n": 497, "auc": 0.4164989939637827}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3389261744966443, "accuracy_n": 298, "auc": 0.3389261744966443}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2606516290726817, "accuracy_n": 399, "auc": 0.2606516290726817}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5148514851485149, "accuracy_n": 303, "auc": 0.5148514851485149}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7032360210071054, "accuracy_n": 322, "auc": 0.7032360210071054}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7125939849624059, "accuracy_n": 292, "auc": 0.7125939849624059}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6198547215496368, "accuracy_n": 413, "auc": 0.6198547215496368}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9348239561217269, "accuracy_n": 1902, "auc": 0.9348239561217269}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5069069934246262, "accuracy_n": 2000, "auc": 0.5069069934246262}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9216867469879518, "accuracy_n": 996, "auc": 0.9216867469879518}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9464646464646465, "accuracy_n": 990, "auc": 0.9464646464646465}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7355072463768116, "accuracy_n": 276, "auc": 0.7355072463768116}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8199195171026157, "accuracy_n": 994, "auc": 0.8199195171026157}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2796780684104628, "accuracy_n": 497, "auc": 0.2796780684104628}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.26174496644295303, "accuracy_n": 298, "auc": 0.26174496644295303}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.21553884711779447, "accuracy_n": 399, "auc": 0.21553884711779447}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9933993399339934, "accuracy_n": 303, "auc": 0.9933993399339934}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9945551436515292, "accuracy_n": 322, "auc": 0.9945551436515292}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7923402255639098, "accuracy_n": 292, "auc": 0.7923402255639098}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6245499380750177, "accuracy_n": 1902, "auc": 0.6245499380750177}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8734343097858326, "accuracy_n": 2000, "auc": 0.8734343097858326}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9341736694677871, "accuracy_n": 59, "auc": 0.9341736694677871}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7, "accuracy_n": 23, "auc": 0.7}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9307228915662651, "accuracy_n": 996, "auc": 0.9307228915662651}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9464646464646465, "accuracy_n": 990, "auc": 0.9464646464646465}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7789855072463768, "accuracy_n": 276, "auc": 0.7789855072463768}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9, "accuracy_n": 100, "auc": 0.9}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8199195171026157, "accuracy_n": 994, "auc": 0.8199195171026157}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.29979879275653926, "accuracy_n": 497, "auc": 0.29979879275653926}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2745490981963928, "accuracy_n": 499, "auc": 0.2745490981963928}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.22055137844611528, "accuracy_n": 399, "auc": 0.22055137844611528}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9867986798679867, "accuracy_n": 303, "auc": 0.9867986798679867}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9933966635773865, "accuracy_n": 322, "auc": 0.9933966635773865}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7696428571428572, "accuracy_n": 292, "auc": 0.7696428571428572}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7675855891719745, "accuracy_n": 1902, "auc": 0.7675855891719745}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7980010783892986, "accuracy_n": 2000, "auc": 0.7980010783892986}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8865546218487395, "accuracy_n": 59, "auc": 0.8865546218487395}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7076923076923077, "accuracy_n": 23, "auc": 0.7076923076923077}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9076305220883534, "accuracy_n": 996, "auc": 0.9076305220883534}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7292929292929293, "accuracy_n": 990, "auc": 0.7292929292929293}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5289855072463768, "accuracy_n": 276, "auc": 0.5289855072463768}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2917505030181087, "accuracy_n": 497, "auc": 0.2917505030181087}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.43887775551102204, "accuracy_n": 499, "auc": 0.43887775551102204}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3724832214765101, "accuracy_n": 298, "auc": 0.3724832214765101}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3558897243107769, "accuracy_n": 399, "auc": 0.3558897243107769}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7722772277227723, "accuracy_n": 303, "auc": 0.7722772277227723}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9749768303985171, "accuracy_n": 322, "auc": 0.9749768303985171}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6388157894736841, "accuracy_n": 292, "auc": 0.6388157894736841}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5171200460014154, "accuracy_n": 1902, "auc": 0.5171200460014154}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6873501333981568, "accuracy_n": 2000, "auc": 0.6873501333981568}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7436974789915967, "accuracy_n": 59, "auc": 0.7436974789915967}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 23, "auc": 1.0}}
