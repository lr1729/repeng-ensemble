{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5125881168177241, "accuracy_n": 993, "auc": 0.5125881168177241}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8184553660982948, "accuracy_n": 997, "auc": 0.8184553660982948}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.60261569416499, "accuracy_n": 994, "auc": 0.60261569416499}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3787575150300601, "accuracy_n": 499, "auc": 0.3787575150300601}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.24623115577889448, "accuracy_n": 398, "auc": 0.24623115577889448}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5132450331125827, "accuracy_n": 302, "auc": 0.5132450331125827}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6021779425393883, "accuracy_n": 322, "auc": 0.6021779425393883}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5269736842105263, "accuracy_n": 292, "auc": 0.5269736842105263}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5447941888619855, "accuracy_n": 413, "auc": 0.5447941888619855}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7436582183297947, "accuracy_n": 1902, "auc": 0.7436582183297947}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6201778842162021, "accuracy_n": 2000, "auc": 0.6201778842162021}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7212885154061625, "accuracy_n": 59, "auc": 0.7212885154061625}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7915407854984894, "accuracy_n": 993, "auc": 0.7915407854984894}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7843530591775326, "accuracy_n": 997, "auc": 0.7843530591775326}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.35671342685370744, "accuracy_n": 499, "auc": 0.35671342685370744}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.25, "accuracy_n": 500, "auc": 0.25}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.30402010050251255, "accuracy_n": 398, "auc": 0.30402010050251255}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5066225165562914, "accuracy_n": 302, "auc": 0.5066225165562914}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6085495829471733, "accuracy_n": 322, "auc": 0.6085495829471733}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5653195488721805, "accuracy_n": 292, "auc": 0.5653195488721805}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8580181794055202, "accuracy_n": 1902, "auc": 0.8580181794055202}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.66504508127434, "accuracy_n": 2000, "auc": 0.66504508127434}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6848739495798319, "accuracy_n": 59, "auc": 0.6848739495798319}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6990972918756269, "accuracy_n": 997, "auc": 0.6990972918756269}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5451263537906137, "accuracy_n": 277, "auc": 0.5451263537906137}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.53, "accuracy_n": 100, "auc": 0.53}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5362173038229376, "accuracy_n": 994, "auc": 0.5362173038229376}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.35470941883767537, "accuracy_n": 499, "auc": 0.35470941883767537}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.25125628140703515, "accuracy_n": 398, "auc": 0.25125628140703515}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8807947019867549, "accuracy_n": 302, "auc": 0.8807947019867549}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5839125733704047, "accuracy_n": 322, "auc": 0.5839125733704047}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5282424812030075, "accuracy_n": 292, "auc": 0.5282424812030075}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6585956416464891, "accuracy_n": 413, "auc": 0.6585956416464891}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7001227441613589, "accuracy_n": 1902, "auc": 0.7001227441613589}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5871759705253596, "accuracy_n": 2000, "auc": 0.5871759705253596}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6526610644257703, "accuracy_n": 59, "auc": 0.6526610644257703}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5599194360523666, "accuracy_n": 993, "auc": 0.5599194360523666}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5315947843530592, "accuracy_n": 997, "auc": 0.5315947843530592}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3166332665330661, "accuracy_n": 499, "auc": 0.3166332665330661}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.282, "accuracy_n": 500, "auc": 0.282}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2613065326633166, "accuracy_n": 398, "auc": 0.2613065326633166}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5451421068890949, "accuracy_n": 322, "auc": 0.5451421068890949}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6070488721804512, "accuracy_n": 292, "auc": 0.6070488721804512}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6368038740920097, "accuracy_n": 413, "auc": 0.6368038740920097}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.637164941613588, "accuracy_n": 1902, "auc": 0.637164941613588}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5071320746789592, "accuracy_n": 2000, "auc": 0.5071320746789592}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5630252100840336, "accuracy_n": 59, "auc": 0.5630252100840336}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5396188565697091, "accuracy_n": 997, "auc": 0.5396188565697091}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3366733466933868, "accuracy_n": 499, "auc": 0.3366733466933868}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2483221476510067, "accuracy_n": 298, "auc": 0.2483221476510067}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.27889447236180903, "accuracy_n": 398, "auc": 0.27889447236180903}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.582483781278962, "accuracy_n": 322, "auc": 0.582483781278962}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6888627819548873, "accuracy_n": 292, "auc": 0.6888627819548873}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5810907643312102, "accuracy_n": 1902, "auc": 0.5810907643312102}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5261049238775198, "accuracy_n": 2000, "auc": 0.5261049238775198}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6064425770308123, "accuracy_n": 59, "auc": 0.6064425770308123}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5384615384615385, "accuracy_n": 23, "auc": 0.5384615384615385}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.553877139979859, "accuracy_n": 993, "auc": 0.553877139979859}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6680040120361084, "accuracy_n": 997, "auc": 0.6680040120361084}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5306859205776173, "accuracy_n": 277, "auc": 0.5306859205776173}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.613682092555332, "accuracy_n": 994, "auc": 0.613682092555332}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.47695390781563124, "accuracy_n": 499, "auc": 0.47695390781563124}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.318, "accuracy_n": 500, "auc": 0.318}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3316582914572864, "accuracy_n": 398, "auc": 0.3316582914572864}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5198675496688742, "accuracy_n": 302, "auc": 0.5198675496688742}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6869786839666358, "accuracy_n": 322, "auc": 0.6869786839666358}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6449248120300752, "accuracy_n": 292, "auc": 0.6449248120300752}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7432335898796887, "accuracy_n": 1902, "auc": 0.7432335898796887}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036678240844945, "accuracy_n": 2000, "auc": 0.5036678240844945}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.603641456582633, "accuracy_n": 59, "auc": 0.603641456582633}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.526686807653575, "accuracy_n": 993, "auc": 0.526686807653575}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5045135406218656, "accuracy_n": 997, "auc": 0.5045135406218656}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5487364620938628, "accuracy_n": 277, "auc": 0.5487364620938628}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5895372233400402, "accuracy_n": 994, "auc": 0.5895372233400402}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.39478957915831664, "accuracy_n": 499, "auc": 0.39478957915831664}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.446, "accuracy_n": 500, "auc": 0.446}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4899497487437186, "accuracy_n": 398, "auc": 0.4899497487437186}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6811862835959221, "accuracy_n": 322, "auc": 0.6811862835959221}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7345864661654136, "accuracy_n": 292, "auc": 0.7345864661654136}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5230024213075061, "accuracy_n": 413, "auc": 0.5230024213075061}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.680084483368719, "accuracy_n": 1902, "auc": 0.680084483368719}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5305915435472206, "accuracy_n": 2000, "auc": 0.5305915435472206}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7044817927170869, "accuracy_n": 59, "auc": 0.7044817927170869}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5397784491440081, "accuracy_n": 993, "auc": 0.5397784491440081}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5295887662988967, "accuracy_n": 997, "auc": 0.5295887662988967}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.58, "accuracy_n": 100, "auc": 0.58}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4028056112224449, "accuracy_n": 499, "auc": 0.4028056112224449}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.334, "accuracy_n": 500, "auc": 0.334}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3825503355704698, "accuracy_n": 298, "auc": 0.3825503355704698}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4045226130653266, "accuracy_n": 398, "auc": 0.4045226130653266}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5298013245033113, "accuracy_n": 302, "auc": 0.5298013245033113}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7234321902996602, "accuracy_n": 322, "auc": 0.7234321902996602}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6347744360902257, "accuracy_n": 292, "auc": 0.6347744360902257}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6602353149327671, "accuracy_n": 1902, "auc": 0.6602353149327671}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5570310882228485, "accuracy_n": 2000, "auc": 0.5570310882228485}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.561624649859944, "accuracy_n": 59, "auc": 0.561624649859944}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5538461538461538, "accuracy_n": 23, "auc": 0.5538461538461538}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.500503524672709, "accuracy_n": 993, "auc": 0.500503524672709}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7442326980942828, "accuracy_n": 997, "auc": 0.7442326980942828}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3847695390781563, "accuracy_n": 499, "auc": 0.3847695390781563}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.42, "accuracy_n": 500, "auc": 0.42}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5452261306532663, "accuracy_n": 398, "auc": 0.5452261306532663}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6794099474822366, "accuracy_n": 322, "auc": 0.6794099474822366}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7284774436090227, "accuracy_n": 292, "auc": 0.7284774436090227}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.52453224522293, "accuracy_n": 1902, "auc": 0.52453224522293}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5595800083830264, "accuracy_n": 2000, "auc": 0.5595800083830264}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6974789915966386, "accuracy_n": 59, "auc": 0.6974789915966386}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6461538461538462, "accuracy_n": 23, "auc": 0.6461538461538462}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5176233635448136, "accuracy_n": 993, "auc": 0.5176233635448136}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5045135406218656, "accuracy_n": 997, "auc": 0.5045135406218656}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5301810865191147, "accuracy_n": 994, "auc": 0.5301810865191147}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.312625250501002, "accuracy_n": 499, "auc": 0.312625250501002}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.244, "accuracy_n": 500, "auc": 0.244}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.23825503355704697, "accuracy_n": 298, "auc": 0.23825503355704697}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.17587939698492464, "accuracy_n": 398, "auc": 0.17587939698492464}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8807947019867549, "accuracy_n": 302, "auc": 0.8807947019867549}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.693388940376892, "accuracy_n": 322, "auc": 0.693388940376892}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5411654135338346, "accuracy_n": 292, "auc": 0.5411654135338346}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5012106537530266, "accuracy_n": 413, "auc": 0.5012106537530266}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6167949398443029, "accuracy_n": 1902, "auc": 0.6167949398443029}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6371480104317659, "accuracy_n": 2000, "auc": 0.6371480104317659}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6974789915966386, "accuracy_n": 59, "auc": 0.6974789915966386}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5166163141993958, "accuracy_n": 993, "auc": 0.5166163141993958}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6178535606820461, "accuracy_n": 997, "auc": 0.6178535606820461}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3246492985971944, "accuracy_n": 499, "auc": 0.3246492985971944}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.288, "accuracy_n": 500, "auc": 0.288}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3087248322147651, "accuracy_n": 298, "auc": 0.3087248322147651}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.1934673366834171, "accuracy_n": 398, "auc": 0.1934673366834171}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9834437086092715, "accuracy_n": 302, "auc": 0.9834437086092715}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6121022551745443, "accuracy_n": 322, "auc": 0.6121022551745443}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5602913533834586, "accuracy_n": 292, "auc": 0.5602913533834586}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5423728813559322, "accuracy_n": 413, "auc": 0.5423728813559322}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5035479697452229, "accuracy_n": 1902, "auc": 0.5035479697452229}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5997665157121721, "accuracy_n": 2000, "auc": 0.5997665157121721}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6400560224089636, "accuracy_n": 59, "auc": 0.6400560224089636}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5166163141993958, "accuracy_n": 993, "auc": 0.5166163141993958}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5346038114343029, "accuracy_n": 997, "auc": 0.5346038114343029}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5415162454873647, "accuracy_n": 277, "auc": 0.5415162454873647}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.4168336673346693, "accuracy_n": 499, "auc": 0.4168336673346693}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.324, "accuracy_n": 500, "auc": 0.324}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3743718592964824, "accuracy_n": 398, "auc": 0.3743718592964824}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8103448275862069, "accuracy_n": 58, "auc": 0.8103448275862069}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9029579857893112, "accuracy_n": 322, "auc": 0.9029579857893112}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9147556390977445, "accuracy_n": 292, "auc": 0.9147556390977445}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5804615622788393, "accuracy_n": 1902, "auc": 0.5804615622788393}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7185463952486848, "accuracy_n": 2000, "auc": 0.7185463952486848}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8053221288515406, "accuracy_n": 59, "auc": 0.8053221288515406}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5667001003009027, "accuracy_n": 997, "auc": 0.5667001003009027}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6207243460764588, "accuracy_n": 994, "auc": 0.6207243460764588}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.30861723446893785, "accuracy_n": 499, "auc": 0.30861723446893785}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.332, "accuracy_n": 500, "auc": 0.332}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3053691275167785, "accuracy_n": 298, "auc": 0.3053691275167785}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.3869346733668342, "accuracy_n": 398, "auc": 0.3869346733668342}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8446092060549892, "accuracy_n": 322, "auc": 0.8446092060549892}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9140977443609023, "accuracy_n": 292, "auc": 0.9140977443609023}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5157384987893463, "accuracy_n": 413, "auc": 0.5157384987893463}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6511500353857042, "accuracy_n": 1902, "auc": 0.6511500353857042}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.690832390492968, "accuracy_n": 2000, "auc": 0.690832390492968}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7282913165266107, "accuracy_n": 59, "auc": 0.7282913165266107}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5384615384615384, "accuracy_n": 23, "auc": 0.5384615384615384}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.518555667001003, "accuracy_n": 997, "auc": 0.518555667001003}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.31462925851703405, "accuracy_n": 499, "auc": 0.31462925851703405}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.18592964824120603, "accuracy_n": 398, "auc": 0.18592964824120603}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5165562913907285, "accuracy_n": 302, "auc": 0.5165562913907285}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5058310163731851, "accuracy_n": 322, "auc": 0.5058310163731851}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5352913533834587, "accuracy_n": 292, "auc": 0.5352913533834587}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6876513317191283, "accuracy_n": 413, "auc": 0.6876513317191283}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5886843152866242, "accuracy_n": 1902, "auc": 0.5886843152866242}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.538317332557053, "accuracy_n": 2000, "auc": 0.538317332557053}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5364145658263305, "accuracy_n": 59, "auc": 0.5364145658263305}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6615384615384615, "accuracy_n": 23, "auc": 0.6615384615384615}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6555891238670695, "accuracy_n": 993, "auc": 0.6555891238670695}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6479438314944834, "accuracy_n": 997, "auc": 0.6479438314944834}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.36072144288577157, "accuracy_n": 499, "auc": 0.36072144288577157}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.1884422110552764, "accuracy_n": 398, "auc": 0.1884422110552764}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6158940397350994, "accuracy_n": 302, "auc": 0.6158940397350994}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5767299969107198, "accuracy_n": 322, "auc": 0.5767299969107198}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5664003759398496, "accuracy_n": 292, "auc": 0.5664003759398496}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5181598062953995, "accuracy_n": 413, "auc": 0.5181598062953995}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9309492215145081, "accuracy_n": 1902, "auc": 0.9309492215145081}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5398919009762524, "accuracy_n": 2000, "auc": 0.5398919009762524}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6176470588235294, "accuracy_n": 59, "auc": 0.6176470588235294}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9117352056168505, "accuracy_n": 997, "auc": 0.9117352056168505}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2625250501002004, "accuracy_n": 499, "auc": 0.2625250501002004}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2989949748743719, "accuracy_n": 398, "auc": 0.2989949748743719}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5860927152317881, "accuracy_n": 302, "auc": 0.5860927152317881}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8970883534136547, "accuracy_n": 322, "auc": 0.8970883534136547}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8369830827067668, "accuracy_n": 292, "auc": 0.8369830827067668}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7365456917905167, "accuracy_n": 1902, "auc": 0.7365456917905167}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7799325556525905, "accuracy_n": 2000, "auc": 0.7799325556525905}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7212885154061625, "accuracy_n": 59, "auc": 0.7212885154061625}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6769230769230768, "accuracy_n": 23, "auc": 0.6769230769230768}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.513595166163142, "accuracy_n": 993, "auc": 0.513595166163142}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7031093279839519, "accuracy_n": 997, "auc": 0.7031093279839519}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.61, "accuracy_n": 100, "auc": 0.61}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.2725450901803607, "accuracy_n": 499, "auc": 0.2725450901803607}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.33, "accuracy_n": 500, "auc": 0.33}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.32550335570469796, "accuracy_n": 298, "auc": 0.32550335570469796}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.30402010050251255, "accuracy_n": 398, "auc": 0.30402010050251255}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7384105960264901, "accuracy_n": 302, "auc": 0.7384105960264901}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.763476984862527, "accuracy_n": 322, "auc": 0.763476984862527}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6783834586466166, "accuracy_n": 292, "auc": 0.6783834586466166}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5302663438256658, "accuracy_n": 413, "auc": 0.5302663438256658}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.8156460102618541, "accuracy_n": 1902, "auc": 0.8156460102618541}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6115597730780812, "accuracy_n": 2000, "auc": 0.6115597730780812}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6568627450980392, "accuracy_n": 59, "auc": 0.6568627450980392}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.513595166163142, "accuracy_n": 993, "auc": 0.513595166163142}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5205616850551655, "accuracy_n": 997, "auc": 0.5205616850551655}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.35470941883767537, "accuracy_n": 499, "auc": 0.35470941883767537}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.302, "accuracy_n": 500, "auc": 0.302}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.28187919463087246, "accuracy_n": 298, "auc": 0.28187919463087246}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.1934673366834171, "accuracy_n": 398, "auc": 0.1934673366834171}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5463576158940397, "accuracy_n": 302, "auc": 0.5463576158940397}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5278035217794255, "accuracy_n": 322, "auc": 0.5278035217794255}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5899436090225564, "accuracy_n": 292, "auc": 0.5899436090225564}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5617433414043583, "accuracy_n": 413, "auc": 0.5617433414043583}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6598847753007786, "accuracy_n": 1902, "auc": 0.6598847753007786}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.5153680478652793, "accuracy_n": 2000, "auc": 0.5153680478652793}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.6330532212885154, "accuracy_n": 59, "auc": 0.6330532212885154}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h15", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
