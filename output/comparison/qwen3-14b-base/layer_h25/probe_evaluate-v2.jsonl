{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9446122860020141, "accuracy_n": 993, "auc": 0.9446122860020141}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9488465396188566, "accuracy_n": 997, "auc": 0.9488465396188566}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8651911468812877, "accuracy_n": 994, "auc": 0.8651911468812877}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.717434869739479, "accuracy_n": 499, "auc": 0.717434869739479}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.704, "accuracy_n": 500, "auc": 0.704}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.802013422818792, "accuracy_n": 298, "auc": 0.802013422818792}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7336683417085427, "accuracy_n": 398, "auc": 0.7336683417085427}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9922381835032438, "accuracy_n": 322, "auc": 0.9922381835032438}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8452067669172934, "accuracy_n": 292, "auc": 0.8452067669172934}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8252808740268931, "accuracy_n": 1902, "auc": 0.8252808740268931}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.653539927913977, "accuracy_n": 2000, "auc": 0.653539927913977}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9411764705882353, "accuracy_n": 59, "auc": 0.9411764705882353}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9415911379657603, "accuracy_n": 993, "auc": 0.9415911379657603}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9518555667001003, "accuracy_n": 997, "auc": 0.9518555667001003}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8628158844765343, "accuracy_n": 277, "auc": 0.8628158844765343}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8380281690140845, "accuracy_n": 994, "auc": 0.8380281690140845}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5851703406813628, "accuracy_n": 499, "auc": 0.5851703406813628}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.614, "accuracy_n": 500, "auc": 0.614}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.761744966442953, "accuracy_n": 298, "auc": 0.761744966442953}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5954773869346733, "accuracy_n": 398, "auc": 0.5954773869346733}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9735099337748344, "accuracy_n": 302, "auc": 0.9735099337748344}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.971231078158789, "accuracy_n": 322, "auc": 0.971231078158789}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8012687969924812, "accuracy_n": 292, "auc": 0.8012687969924812}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.808389729299363, "accuracy_n": 1902, "auc": 0.808389729299363}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.617054756767193, "accuracy_n": 2000, "auc": 0.617054756767193}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8599439775910365, "accuracy_n": 59, "auc": 0.8599439775910365}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9476334340382678, "accuracy_n": 993, "auc": 0.9476334340382678}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8628158844765343, "accuracy_n": 277, "auc": 0.8628158844765343}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8621730382293763, "accuracy_n": 994, "auc": 0.8621730382293763}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7434869739478958, "accuracy_n": 499, "auc": 0.7434869739478958}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.808, "accuracy_n": 500, "auc": 0.808}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8691275167785235, "accuracy_n": 298, "auc": 0.8691275167785235}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7412060301507538, "accuracy_n": 398, "auc": 0.7412060301507538}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9954819277108433, "accuracy_n": 322, "auc": 0.9954819277108433}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8272086466165413, "accuracy_n": 292, "auc": 0.8272086466165413}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6934359518754424, "accuracy_n": 1902, "auc": 0.6934359518754424}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7561599737505239, "accuracy_n": 2000, "auc": 0.7561599737505239}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8697478991596639, "accuracy_n": 59, "auc": 0.8697478991596639}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8538461538461539, "accuracy_n": 23, "auc": 0.8538461538461539}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9395770392749244, "accuracy_n": 993, "auc": 0.9395770392749244}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7581227436823105, "accuracy_n": 277, "auc": 0.7581227436823105}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8108651911468813, "accuracy_n": 994, "auc": 0.8108651911468813}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6913827655310621, "accuracy_n": 499, "auc": 0.6913827655310621}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.672, "accuracy_n": 500, "auc": 0.672}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8120805369127517, "accuracy_n": 298, "auc": 0.8120805369127517}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7060301507537688, "accuracy_n": 398, "auc": 0.7060301507537688}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9370860927152318, "accuracy_n": 302, "auc": 0.9370860927152318}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9585650293481618, "accuracy_n": 322, "auc": 0.9585650293481618}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7867481203007519, "accuracy_n": 292, "auc": 0.7867481203007519}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8244636854210898, "accuracy_n": 1902, "auc": 0.8244636854210898}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5948627454511078, "accuracy_n": 2000, "auc": 0.5948627454511078}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7422969187675069, "accuracy_n": 59, "auc": 0.7422969187675069}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8461538461538463, "accuracy_n": 23, "auc": 0.8461538461538463}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9154078549848943, "accuracy_n": 993, "auc": 0.9154078549848943}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.950852557673019, "accuracy_n": 997, "auc": 0.950852557673019}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5379061371841155, "accuracy_n": 277, "auc": 0.5379061371841155}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6247484909456741, "accuracy_n": 994, "auc": 0.6247484909456741}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.687374749498998, "accuracy_n": 499, "auc": 0.687374749498998}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.584, "accuracy_n": 500, "auc": 0.584}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8322147651006712, "accuracy_n": 298, "auc": 0.8322147651006712}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6683417085427136, "accuracy_n": 398, "auc": 0.6683417085427136}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9572520852641335, "accuracy_n": 322, "auc": 0.9572520852641335}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7770206766917293, "accuracy_n": 292, "auc": 0.7770206766917293}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7444234341825903, "accuracy_n": 1902, "auc": 0.7444234341825903}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6674319429313982, "accuracy_n": 2000, "auc": 0.6674319429313982}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9397759103641457, "accuracy_n": 59, "auc": 0.9397759103641457}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 23, "auc": 0.8461538461538461}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9365558912386707, "accuracy_n": 993, "auc": 0.9365558912386707}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9187562688064193, "accuracy_n": 997, "auc": 0.9187562688064193}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6287726358148893, "accuracy_n": 994, "auc": 0.6287726358148893}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8216432865731463, "accuracy_n": 499, "auc": 0.8216432865731463}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.798, "accuracy_n": 500, "auc": 0.798}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.87248322147651, "accuracy_n": 298, "auc": 0.87248322147651}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8442211055276382, "accuracy_n": 398, "auc": 0.8442211055276382}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7781456953642384, "accuracy_n": 302, "auc": 0.7781456953642384}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9952116156935433, "accuracy_n": 322, "auc": 0.9952116156935433}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.861419172932331, "accuracy_n": 292, "auc": 0.861419172932331}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5430013711960368, "accuracy_n": 1902, "auc": 0.5430013711960368}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6600432756224998, "accuracy_n": 2000, "auc": 0.6600432756224998}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9285714285714286, "accuracy_n": 59, "auc": 0.9285714285714286}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9395770392749244, "accuracy_n": 993, "auc": 0.9395770392749244}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9338014042126379, "accuracy_n": 997, "auc": 0.9338014042126379}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6101083032490975, "accuracy_n": 277, "auc": 0.6101083032490975}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6720321931589537, "accuracy_n": 994, "auc": 0.6720321931589537}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7755511022044088, "accuracy_n": 499, "auc": 0.7755511022044088}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.768, "accuracy_n": 500, "auc": 0.768}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8557046979865772, "accuracy_n": 298, "auc": 0.8557046979865772}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8241206030150754, "accuracy_n": 398, "auc": 0.8241206030150754}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8973509933774835, "accuracy_n": 302, "auc": 0.8973509933774835}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9884538152610441, "accuracy_n": 322, "auc": 0.9884538152610441}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8851973684210527, "accuracy_n": 292, "auc": 0.8851973684210527}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6508912774239207, "accuracy_n": 1902, "auc": 0.6508912774239207}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.579327637277057, "accuracy_n": 2000, "auc": 0.579327637277057}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8557422969187676, "accuracy_n": 59, "auc": 0.8557422969187676}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9446122860020141, "accuracy_n": 993, "auc": 0.9446122860020141}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9207622868605817, "accuracy_n": 997, "auc": 0.9207622868605817}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5956678700361011, "accuracy_n": 277, "auc": 0.5956678700361011}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6378269617706237, "accuracy_n": 994, "auc": 0.6378269617706237}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7695390781563126, "accuracy_n": 499, "auc": 0.7695390781563126}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 500, "auc": 0.76}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8489932885906041, "accuracy_n": 298, "auc": 0.8489932885906041}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8190954773869347, "accuracy_n": 398, "auc": 0.8190954773869347}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7880794701986755, "accuracy_n": 302, "auc": 0.7880794701986755}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9759036144578314, "accuracy_n": 322, "auc": 0.9759036144578314}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.862406015037594, "accuracy_n": 292, "auc": 0.862406015037594}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6815751061571125, "accuracy_n": 1902, "auc": 0.6815751061571125}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5216883294869448, "accuracy_n": 2000, "auc": 0.5216883294869448}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7787114845938375, "accuracy_n": 59, "auc": 0.7787114845938375}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9274924471299094, "accuracy_n": 993, "auc": 0.9274924471299094}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6529175050301811, "accuracy_n": 994, "auc": 0.6529175050301811}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7675350701402806, "accuracy_n": 499, "auc": 0.7675350701402806}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.758, "accuracy_n": 500, "auc": 0.758}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8355704697986577, "accuracy_n": 298, "auc": 0.8355704697986577}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8090452261306532, "accuracy_n": 398, "auc": 0.8090452261306532}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8145695364238411, "accuracy_n": 302, "auc": 0.8145695364238411}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9721964782205746, "accuracy_n": 322, "auc": 0.9721964782205746}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8892857142857142, "accuracy_n": 292, "auc": 0.8892857142857142}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6566005396319886, "accuracy_n": 1902, "auc": 0.6566005396319886}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5486570652005374, "accuracy_n": 2000, "auc": 0.5486570652005374}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7745098039215685, "accuracy_n": 59, "auc": 0.7745098039215685}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9446122860020141, "accuracy_n": 993, "auc": 0.9446122860020141}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8592057761732852, "accuracy_n": 277, "auc": 0.8592057761732852}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7766599597585513, "accuracy_n": 994, "auc": 0.7766599597585513}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6012024048096193, "accuracy_n": 499, "auc": 0.6012024048096193}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.624, "accuracy_n": 500, "auc": 0.624}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7718120805369127, "accuracy_n": 298, "auc": 0.7718120805369127}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7010050251256281, "accuracy_n": 398, "auc": 0.7010050251256281}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9829317269076305, "accuracy_n": 322, "auc": 0.9829317269076305}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.774483082706767, "accuracy_n": 292, "auc": 0.774483082706767}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7673566878980891, "accuracy_n": 1902, "auc": 0.7673566878980891}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7204250734515161, "accuracy_n": 2000, "auc": 0.7204250734515161}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9943977591036415, "accuracy_n": 59, "auc": 0.9943977591036415}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8307692307692307, "accuracy_n": 23, "auc": 0.8307692307692307}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94662638469285, "accuracy_n": 993, "auc": 0.94662638469285}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.855595667870036, "accuracy_n": 277, "auc": 0.855595667870036}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7857142857142857, "accuracy_n": 994, "auc": 0.7857142857142857}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.37675350701402804, "accuracy_n": 499, "auc": 0.37675350701402804}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.506, "accuracy_n": 500, "auc": 0.506}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4161073825503356, "accuracy_n": 298, "auc": 0.4161073825503356}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.3869346733668342, "accuracy_n": 398, "auc": 0.3869346733668342}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9877201112140872, "accuracy_n": 322, "auc": 0.9877201112140872}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7672462406015037, "accuracy_n": 292, "auc": 0.7672462406015037}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7738831387119605, "accuracy_n": 1902, "auc": 0.7738831387119605}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7311699523527992, "accuracy_n": 2000, "auc": 0.7311699523527992}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9971988795518207, "accuracy_n": 59, "auc": 0.9971988795518207}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8461538461538461, "accuracy_n": 23, "auc": 0.8461538461538461}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94662638469285, "accuracy_n": 993, "auc": 0.94662638469285}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8303249097472925, "accuracy_n": 277, "auc": 0.8303249097472925}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7625754527162978, "accuracy_n": 994, "auc": 0.7625754527162978}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6593186372745491, "accuracy_n": 499, "auc": 0.6593186372745491}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.674, "accuracy_n": 500, "auc": 0.674}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.802013422818792, "accuracy_n": 298, "auc": 0.802013422818792}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7412060301507538, "accuracy_n": 398, "auc": 0.7412060301507538}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9935125115848007, "accuracy_n": 322, "auc": 0.9935125115848007}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8391917293233082, "accuracy_n": 292, "auc": 0.8391917293233082}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7366031935598019, "accuracy_n": 1902, "auc": 0.7366031935598019}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6874911843175386, "accuracy_n": 2000, "auc": 0.6874911843175386}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.980392156862745, "accuracy_n": 59, "auc": 0.980392156862745}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.919436052366566, "accuracy_n": 993, "auc": 0.919436052366566}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9097291875626881, "accuracy_n": 997, "auc": 0.9097291875626881}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7184115523465704, "accuracy_n": 277, "auc": 0.7184115523465704}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6428571428571429, "accuracy_n": 994, "auc": 0.6428571428571429}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7274549098196392, "accuracy_n": 499, "auc": 0.7274549098196392}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.702, "accuracy_n": 500, "auc": 0.702}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8120805369127517, "accuracy_n": 298, "auc": 0.8120805369127517}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7738693467336684, "accuracy_n": 398, "auc": 0.7738693467336684}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9337748344370861, "accuracy_n": 302, "auc": 0.9337748344370861}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9910024714241582, "accuracy_n": 322, "auc": 0.9910024714241582}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9759868421052632, "accuracy_n": 292, "auc": 0.9759868421052632}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6805301220806794, "accuracy_n": 1902, "auc": 0.6805301220806794}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6678510942450225, "accuracy_n": 2000, "auc": 0.6678510942450225}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8571428571428572, "accuracy_n": 59, "auc": 0.8571428571428572}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94662638469285, "accuracy_n": 993, "auc": 0.94662638469285}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8231046931407943, "accuracy_n": 277, "auc": 0.8231046931407943}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.95, "accuracy_n": 100, "auc": 0.95}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.823943661971831, "accuracy_n": 994, "auc": 0.823943661971831}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7294589178356713, "accuracy_n": 499, "auc": 0.7294589178356713}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.748, "accuracy_n": 500, "auc": 0.748}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7989949748743719, "accuracy_n": 398, "auc": 0.7989949748743719}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9310344827586207, "accuracy_n": 58, "auc": 0.9310344827586207}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9958680877355577, "accuracy_n": 322, "auc": 0.9958680877355577}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8339285714285715, "accuracy_n": 292, "auc": 0.8339285714285715}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7331951079263977, "accuracy_n": 1902, "auc": 0.7331951079263977}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7804997604135093, "accuracy_n": 2000, "auc": 0.7804997604135093}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 59, "auc": 1.0}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9254783484390735, "accuracy_n": 993, "auc": 0.9254783484390735}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9197592778335005, "accuracy_n": 997, "auc": 0.9197592778335005}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5812274368231047, "accuracy_n": 277, "auc": 0.5812274368231047}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7223340040241448, "accuracy_n": 994, "auc": 0.7223340040241448}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.342685370741483, "accuracy_n": 499, "auc": 0.342685370741483}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.444, "accuracy_n": 500, "auc": 0.444}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6442953020134228, "accuracy_n": 298, "auc": 0.6442953020134228}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.44472361809045224, "accuracy_n": 398, "auc": 0.44472361809045224}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8543046357615894, "accuracy_n": 302, "auc": 0.8543046357615894}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8976289774482547, "accuracy_n": 322, "auc": 0.8976289774482547}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7097744360902255, "accuracy_n": 292, "auc": 0.7097744360902255}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8910411622276029, "accuracy_n": 413, "auc": 0.8910411622276029}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8866584837225762, "accuracy_n": 1902, "auc": 0.8866584837225762}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5317889758202712, "accuracy_n": 2000, "auc": 0.5317889758202712}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7310924369747899, "accuracy_n": 59, "auc": 0.7310924369747899}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9395770392749244, "accuracy_n": 993, "auc": 0.9395770392749244}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9478435305917753, "accuracy_n": 997, "auc": 0.9478435305917753}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8375451263537906, "accuracy_n": 277, "auc": 0.8375451263537906}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8189134808853119, "accuracy_n": 994, "auc": 0.8189134808853119}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6012024048096193, "accuracy_n": 499, "auc": 0.6012024048096193}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 500, "auc": 0.75}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5704697986577181, "accuracy_n": 298, "auc": 0.5704697986577181}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.47738693467336685, "accuracy_n": 398, "auc": 0.47738693467336685}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9854031510658017, "accuracy_n": 322, "auc": 0.9854031510658017}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7931390977443609, "accuracy_n": 292, "auc": 0.7931390977443609}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6397613676574664, "accuracy_n": 1902, "auc": 0.6397613676574664}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8044834185140837, "accuracy_n": 2000, "auc": 0.8044834185140837}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9887955182072828, "accuracy_n": 59, "auc": 0.9887955182072828}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8384615384615385, "accuracy_n": 23, "auc": 0.8384615384615385}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9436052366565961, "accuracy_n": 993, "auc": 0.9436052366565961}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8122743682310469, "accuracy_n": 277, "auc": 0.8122743682310469}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6951710261569416, "accuracy_n": 994, "auc": 0.6951710261569416}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5290581162324649, "accuracy_n": 499, "auc": 0.5290581162324649}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.602, "accuracy_n": 500, "auc": 0.602}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7214765100671141, "accuracy_n": 298, "auc": 0.7214765100671141}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6407035175879398, "accuracy_n": 398, "auc": 0.6407035175879398}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.896551724137931, "accuracy_n": 58, "auc": 0.896551724137931}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9666357738646895, "accuracy_n": 322, "auc": 0.9666357738646895}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8020206766917294, "accuracy_n": 292, "auc": 0.8020206766917294}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.7503494338287331, "accuracy_n": 1902, "auc": 0.7503494338287331}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6652411520558922, "accuracy_n": 2000, "auc": 0.6652411520558922}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9551820728291316, "accuracy_n": 59, "auc": 0.9551820728291316}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9365558912386707, "accuracy_n": 993, "auc": 0.9365558912386707}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8696088264794383, "accuracy_n": 997, "auc": 0.8696088264794383}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.6106639839034205, "accuracy_n": 994, "auc": 0.6106639839034205}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4148296593186373, "accuracy_n": 499, "auc": 0.4148296593186373}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.384, "accuracy_n": 500, "auc": 0.384}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.4228187919463087, "accuracy_n": 298, "auc": 0.4228187919463087}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.20100502512562815, "accuracy_n": 398, "auc": 0.20100502512562815}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8793103448275862, "accuracy_n": 58, "auc": 0.8793103448275862}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9371331479765214, "accuracy_n": 322, "auc": 0.9371331479765214}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5819078947368421, "accuracy_n": 292, "auc": 0.5819078947368421}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.5543690286624203, "accuracy_n": 1902, "auc": 0.5543690286624203}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.666683672805883, "accuracy_n": 2000, "auc": 0.666683672805883}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.9593837535014005, "accuracy_n": 59, "auc": 0.9593837535014005}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h25", "token_idx": -1, "accuracy": 0.8846153846153846, "accuracy_n": 23, "auc": 0.8846153846153846}}
