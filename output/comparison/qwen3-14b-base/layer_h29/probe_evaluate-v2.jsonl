{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9405840886203424, "accuracy_n": 993, "auc": 0.9405840886203424}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9458375125376128, "accuracy_n": 997, "auc": 0.9458375125376128}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8339350180505415, "accuracy_n": 277, "auc": 0.8339350180505415}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8370221327967807, "accuracy_n": 994, "auc": 0.8370221327967807}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5571142284569138, "accuracy_n": 499, "auc": 0.5571142284569138}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.652, "accuracy_n": 500, "auc": 0.652}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7281879194630873, "accuracy_n": 298, "auc": 0.7281879194630873}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6457286432160804, "accuracy_n": 398, "auc": 0.6457286432160804}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9671763978992896, "accuracy_n": 322, "auc": 0.9671763978992896}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7032894736842106, "accuracy_n": 292, "auc": 0.7032894736842106}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7119128184713375, "accuracy_n": 1902, "auc": 0.7119128184713375}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.603663922676086, "accuracy_n": 2000, "auc": 0.603663922676086}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9243697478991596, "accuracy_n": 59, "auc": 0.9243697478991596}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.676923076923077, "accuracy_n": 23, "auc": 0.676923076923077}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9405840886203424, "accuracy_n": 993, "auc": 0.9405840886203424}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7617328519855595, "accuracy_n": 277, "auc": 0.7617328519855595}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6871227364185111, "accuracy_n": 994, "auc": 0.6871227364185111}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.40080160320641284, "accuracy_n": 499, "auc": 0.40080160320641284}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.544, "accuracy_n": 500, "auc": 0.544}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6040268456375839, "accuracy_n": 298, "auc": 0.6040268456375839}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.44974874371859297, "accuracy_n": 398, "auc": 0.44974874371859297}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.956953642384106, "accuracy_n": 302, "auc": 0.956953642384106}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8275862068965517, "accuracy_n": 58, "auc": 0.8275862068965517}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9148517145505097, "accuracy_n": 322, "auc": 0.9148517145505097}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6459586466165415, "accuracy_n": 292, "auc": 0.6459586466165415}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9757869249394673, "accuracy_n": 413, "auc": 0.9757869249394673}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.695978193559802, "accuracy_n": 1902, "auc": 0.695978193559802}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5754237279657957, "accuracy_n": 2000, "auc": 0.5754237279657957}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.861344537815126, "accuracy_n": 59, "auc": 0.861344537815126}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 23, "auc": 0.5}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9446122860020141, "accuracy_n": 993, "auc": 0.9446122860020141}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8700361010830325, "accuracy_n": 277, "auc": 0.8700361010830325}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8048289738430584, "accuracy_n": 994, "auc": 0.8048289738430584}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4749498997995992, "accuracy_n": 499, "auc": 0.4749498997995992}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.604, "accuracy_n": 500, "auc": 0.604}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.436241610738255, "accuracy_n": 298, "auc": 0.436241610738255}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.32914572864321606, "accuracy_n": 398, "auc": 0.32914572864321606}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6192052980132451, "accuracy_n": 302, "auc": 0.6192052980132451}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5427865307383379, "accuracy_n": 322, "auc": 0.5427865307383379}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5852913533834586, "accuracy_n": 292, "auc": 0.5852913533834586}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9878934624697336, "accuracy_n": 413, "auc": 0.9878934624697336}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5521286712668082, "accuracy_n": 1902, "auc": 0.5521286712668082}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5767322003243172, "accuracy_n": 2000, "auc": 0.5767322003243172}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5196078431372549, "accuracy_n": 59, "auc": 0.5196078431372549}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9395770392749244, "accuracy_n": 993, "auc": 0.9395770392749244}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9468405215646941, "accuracy_n": 997, "auc": 0.9468405215646941}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7906137184115524, "accuracy_n": 277, "auc": 0.7906137184115524}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7706237424547284, "accuracy_n": 994, "auc": 0.7706237424547284}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6012024048096193, "accuracy_n": 499, "auc": 0.6012024048096193}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.678, "accuracy_n": 500, "auc": 0.678}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7214765100671141, "accuracy_n": 298, "auc": 0.7214765100671141}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.628140703517588, "accuracy_n": 398, "auc": 0.628140703517588}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9039735099337748, "accuracy_n": 302, "auc": 0.9039735099337748}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8733008958912573, "accuracy_n": 322, "auc": 0.8733008958912573}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5801221804511278, "accuracy_n": 292, "auc": 0.5801221804511278}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9322033898305084, "accuracy_n": 413, "auc": 0.9322033898305084}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7676884288747345, "accuracy_n": 1902, "auc": 0.7676884288747345}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.56005217883656, "accuracy_n": 2000, "auc": 0.56005217883656}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7661064425770308, "accuracy_n": 59, "auc": 0.7661064425770308}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8086606243705942, "accuracy_n": 993, "auc": 0.8086606243705942}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6101083032490975, "accuracy_n": 277, "auc": 0.6101083032490975}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6327967806841046, "accuracy_n": 994, "auc": 0.6327967806841046}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5531062124248497, "accuracy_n": 499, "auc": 0.5531062124248497}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.62, "accuracy_n": 500, "auc": 0.62}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6610738255033557, "accuracy_n": 298, "auc": 0.6610738255033557}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5653266331658291, "accuracy_n": 398, "auc": 0.5653266331658291}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9834437086092715, "accuracy_n": 302, "auc": 0.9834437086092715}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7931034482758621, "accuracy_n": 58, "auc": 0.7931034482758621}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8904077849860983, "accuracy_n": 322, "auc": 0.8904077849860983}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6070488721804511, "accuracy_n": 292, "auc": 0.6070488721804511}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9709443099273608, "accuracy_n": 413, "auc": 0.9709443099273608}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7400886854210899, "accuracy_n": 1902, "auc": 0.7400886854210899}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6067450349576197, "accuracy_n": 2000, "auc": 0.6067450349576197}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8907563025210083, "accuracy_n": 59, "auc": 0.8907563025210083}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9224572004028198, "accuracy_n": 993, "auc": 0.9224572004028198}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9157472417251755, "accuracy_n": 997, "auc": 0.9157472417251755}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6207243460764588, "accuracy_n": 994, "auc": 0.6207243460764588}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8216432865731463, "accuracy_n": 499, "auc": 0.8216432865731463}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.78, "accuracy_n": 500, "auc": 0.78}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8758389261744967, "accuracy_n": 298, "auc": 0.8758389261744967}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8140703517587939, "accuracy_n": 398, "auc": 0.8140703517587939}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7483443708609272, "accuracy_n": 302, "auc": 0.7483443708609272}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9842446709916589, "accuracy_n": 322, "auc": 0.9842446709916589}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8965225563909776, "accuracy_n": 292, "auc": 0.8965225563909776}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9903147699757869, "accuracy_n": 413, "auc": 0.9903147699757869}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.594009863765039, "accuracy_n": 1902, "auc": 0.594009863765039}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.605677649631517, "accuracy_n": 2000, "auc": 0.605677649631517}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8571428571428571, "accuracy_n": 59, "auc": 0.8571428571428571}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8902316213494461, "accuracy_n": 993, "auc": 0.8902316213494461}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9398194583751254, "accuracy_n": 997, "auc": 0.9398194583751254}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5884476534296029, "accuracy_n": 277, "auc": 0.5884476534296029}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6388329979879276, "accuracy_n": 994, "auc": 0.6388329979879276}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7635270541082164, "accuracy_n": 499, "auc": 0.7635270541082164}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.762, "accuracy_n": 500, "auc": 0.762}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.825503355704698, "accuracy_n": 298, "auc": 0.825503355704698}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7989949748743719, "accuracy_n": 398, "auc": 0.7989949748743719}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8940397350993378, "accuracy_n": 302, "auc": 0.8940397350993378}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.603448275862069, "accuracy_n": 58, "auc": 0.603448275862069}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9426938523324065, "accuracy_n": 322, "auc": 0.9426938523324065}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7545582706766918, "accuracy_n": 292, "auc": 0.7545582706766918}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9854721549636803, "accuracy_n": 413, "auc": 0.9854721549636803}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.665230228237792, "accuracy_n": 1902, "auc": 0.665230228237792}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5591958697089648, "accuracy_n": 2000, "auc": 0.5591958697089648}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8207282913165266, "accuracy_n": 59, "auc": 0.8207282913165266}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6230769230769231, "accuracy_n": 23, "auc": 0.6230769230769231}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8841893252769386, "accuracy_n": 993, "auc": 0.8841893252769386}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9297893681043129, "accuracy_n": 997, "auc": 0.9297893681043129}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5992779783393501, "accuracy_n": 277, "auc": 0.5992779783393501}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7254509018036072, "accuracy_n": 499, "auc": 0.7254509018036072}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.756, "accuracy_n": 500, "auc": 0.756}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8288590604026845, "accuracy_n": 298, "auc": 0.8288590604026845}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7889447236180904, "accuracy_n": 398, "auc": 0.7889447236180904}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7417218543046358, "accuracy_n": 302, "auc": 0.7417218543046358}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8899057769539697, "accuracy_n": 322, "auc": 0.8899057769539697}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7189379699248121, "accuracy_n": 292, "auc": 0.7189379699248121}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8498789346246973, "accuracy_n": 413, "auc": 0.8498789346246973}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6644075106157112, "accuracy_n": 1902, "auc": 0.6644075106157112}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.516549474360244, "accuracy_n": 2000, "auc": 0.516549474360244}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7156862745098039, "accuracy_n": 59, "auc": 0.7156862745098039}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5769230769230769, "accuracy_n": 23, "auc": 0.5769230769230769}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8841893252769386, "accuracy_n": 993, "auc": 0.8841893252769386}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9378134403209629, "accuracy_n": 997, "auc": 0.9378134403209629}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6267605633802817, "accuracy_n": 994, "auc": 0.6267605633802817}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.751503006012024, "accuracy_n": 499, "auc": 0.751503006012024}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 500, "auc": 0.76}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8187919463087249, "accuracy_n": 298, "auc": 0.8187919463087249}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8040201005025126, "accuracy_n": 398, "auc": 0.8040201005025126}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8311258278145696, "accuracy_n": 302, "auc": 0.8311258278145696}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9380599320358356, "accuracy_n": 322, "auc": 0.9380599320358356}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7603383458646618, "accuracy_n": 292, "auc": 0.7603383458646618}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9757869249394673, "accuracy_n": 413, "auc": 0.9757869249394673}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6589780166312809, "accuracy_n": 1902, "auc": 0.6589780166312809}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5648489104566748, "accuracy_n": 2000, "auc": 0.5648489104566748}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7619047619047619, "accuracy_n": 59, "auc": 0.7619047619047619}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6076923076923076, "accuracy_n": 23, "auc": 0.6076923076923076}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9395770392749244, "accuracy_n": 993, "auc": 0.9395770392749244}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9478435305917753, "accuracy_n": 997, "auc": 0.9478435305917753}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6064981949458483, "accuracy_n": 277, "auc": 0.6064981949458483}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6670020120724346, "accuracy_n": 994, "auc": 0.6670020120724346}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4909819639278557, "accuracy_n": 499, "auc": 0.4909819639278557}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.554, "accuracy_n": 500, "auc": 0.554}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5771812080536913, "accuracy_n": 298, "auc": 0.5771812080536913}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.4949748743718593, "accuracy_n": 398, "auc": 0.4949748743718593}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8620689655172413, "accuracy_n": 58, "auc": 0.8620689655172413}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9742431263515601, "accuracy_n": 322, "auc": 0.9742431263515601}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6542763157894735, "accuracy_n": 292, "auc": 0.6542763157894735}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6605045780254777, "accuracy_n": 1902, "auc": 0.6605045780254777}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6458821634610095, "accuracy_n": 2000, "auc": 0.6458821634610095}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9705882352941175, "accuracy_n": 59, "auc": 0.9705882352941175}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7538461538461538, "accuracy_n": 23, "auc": 0.7538461538461538}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9446122860020141, "accuracy_n": 993, "auc": 0.9446122860020141}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8628158844765343, "accuracy_n": 277, "auc": 0.8628158844765343}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.97, "accuracy_n": 100, "auc": 0.97}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7334004024144869, "accuracy_n": 994, "auc": 0.7334004024144869}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3527054108216433, "accuracy_n": 499, "auc": 0.3527054108216433}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.484, "accuracy_n": 500, "auc": 0.484}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.412751677852349, "accuracy_n": 298, "auc": 0.412751677852349}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.43467336683417085, "accuracy_n": 398, "auc": 0.43467336683417085}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9855962310781589, "accuracy_n": 322, "auc": 0.9855962310781589}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5994360902255639, "accuracy_n": 292, "auc": 0.5994360902255639}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.659529812455768, "accuracy_n": 1902, "auc": 0.659529812455768}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6803130930265826, "accuracy_n": 2000, "auc": 0.6803130930265826}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9901960784313726, "accuracy_n": 59, "auc": 0.9901960784313726}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8769230769230769, "accuracy_n": 23, "auc": 0.8769230769230769}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9385699899295066, "accuracy_n": 993, "auc": 0.9385699899295066}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9458375125376128, "accuracy_n": 997, "auc": 0.9458375125376128}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.647887323943662, "accuracy_n": 994, "auc": 0.647887323943662}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5410821643286573, "accuracy_n": 499, "auc": 0.5410821643286573}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.578, "accuracy_n": 500, "auc": 0.578}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6409395973154363, "accuracy_n": 298, "auc": 0.6409395973154363}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5603015075376885, "accuracy_n": 398, "auc": 0.5603015075376885}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7241379310344828, "accuracy_n": 58, "auc": 0.7241379310344828}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9886468952734013, "accuracy_n": 322, "auc": 0.9886468952734013}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.737687969924812, "accuracy_n": 292, "auc": 0.737687969924812}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6821667108987969, "accuracy_n": 1902, "auc": 0.6821667108987969}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6176339658616761, "accuracy_n": 2000, "auc": 0.6176339658616761}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9607843137254901, "accuracy_n": 59, "auc": 0.9607843137254901}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6000000000000001, "accuracy_n": 23, "auc": 0.6000000000000001}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9315206445115811, "accuracy_n": 993, "auc": 0.9315206445115811}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9087261785356068, "accuracy_n": 997, "auc": 0.9087261785356068}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5342960288808665, "accuracy_n": 277, "auc": 0.5342960288808665}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.76, "accuracy_n": 100, "auc": 0.76}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.613682092555332, "accuracy_n": 994, "auc": 0.613682092555332}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6753507014028056, "accuracy_n": 499, "auc": 0.6753507014028056}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.678, "accuracy_n": 500, "auc": 0.678}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7684563758389261, "accuracy_n": 298, "auc": 0.7684563758389261}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7185929648241206, "accuracy_n": 398, "auc": 0.7185929648241206}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8278145695364238, "accuracy_n": 302, "auc": 0.8278145695364238}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5517241379310345, "accuracy_n": 58, "auc": 0.5517241379310345}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9843219029966018, "accuracy_n": 322, "auc": 0.9843219029966018}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9809210526315789, "accuracy_n": 292, "auc": 0.9809210526315789}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6524504600141543, "accuracy_n": 1902, "auc": 0.6524504600141543}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6521184147477239, "accuracy_n": 2000, "auc": 0.6521184147477239}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7380952380952381, "accuracy_n": 59, "auc": 0.7380952380952381}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5615384615384615, "accuracy_n": 23, "auc": 0.5615384615384615}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9365558912386707, "accuracy_n": 993, "auc": 0.9365558912386707}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9478435305917753, "accuracy_n": 997, "auc": 0.9478435305917753}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.776173285198556, "accuracy_n": 277, "auc": 0.776173285198556}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7505030181086519, "accuracy_n": 994, "auc": 0.7505030181086519}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6432865731462926, "accuracy_n": 499, "auc": 0.6432865731462926}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.706, "accuracy_n": 500, "auc": 0.706}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7315436241610739, "accuracy_n": 298, "auc": 0.7315436241610739}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7688442211055276, "accuracy_n": 398, "auc": 0.7688442211055276}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7586206896551724, "accuracy_n": 58, "auc": 0.7586206896551724}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9947868396663577, "accuracy_n": 322, "auc": 0.9947868396663577}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7351973684210525, "accuracy_n": 292, "auc": 0.7351973684210525}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6626459660297239, "accuracy_n": 1902, "auc": 0.6626459660297239}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6866268723009007, "accuracy_n": 2000, "auc": 0.6866268723009007}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9957983193277311, "accuracy_n": 59, "auc": 0.9957983193277311}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9133937562940584, "accuracy_n": 993, "auc": 0.9133937562940584}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5776173285198556, "accuracy_n": 277, "auc": 0.5776173285198556}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.94, "accuracy_n": 100, "auc": 0.94}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6468812877263581, "accuracy_n": 994, "auc": 0.6468812877263581}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.37675350701402804, "accuracy_n": 499, "auc": 0.37675350701402804}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.488, "accuracy_n": 500, "auc": 0.488}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6006711409395973, "accuracy_n": 298, "auc": 0.6006711409395973}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.45226130653266333, "accuracy_n": 398, "auc": 0.45226130653266333}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8278145695364238, "accuracy_n": 302, "auc": 0.8278145695364238}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8601714550509733, "accuracy_n": 322, "auc": 0.8601714550509733}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6216165413533835, "accuracy_n": 292, "auc": 0.6216165413533835}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7941888619854721, "accuracy_n": 413, "auc": 0.7941888619854721}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8507486288039632, "accuracy_n": 1902, "auc": 0.8507486288039632}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5314228436465565, "accuracy_n": 2000, "auc": 0.5314228436465565}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7296918767507004, "accuracy_n": 59, "auc": 0.7296918767507004}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6846153846153846, "accuracy_n": 23, "auc": 0.6846153846153846}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9415911379657603, "accuracy_n": 993, "auc": 0.9415911379657603}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9478435305917753, "accuracy_n": 997, "auc": 0.9478435305917753}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8122743682310469, "accuracy_n": 277, "auc": 0.8122743682310469}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.93, "accuracy_n": 100, "auc": 0.93}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7424547283702213, "accuracy_n": 994, "auc": 0.7424547283702213}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5130260521042084, "accuracy_n": 499, "auc": 0.5130260521042084}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.644, "accuracy_n": 500, "auc": 0.644}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.38926174496644295, "accuracy_n": 298, "auc": 0.38926174496644295}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6180904522613065, "accuracy_n": 398, "auc": 0.6180904522613065}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9137931034482759, "accuracy_n": 58, "auc": 0.9137931034482759}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9861368551127587, "accuracy_n": 322, "auc": 0.9861368551127587}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6741071428571428, "accuracy_n": 292, "auc": 0.6741071428571428}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.601126813517339, "accuracy_n": 1902, "auc": 0.601126813517339}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7280138129864882, "accuracy_n": 2000, "auc": 0.7280138129864882}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9789915966386554, "accuracy_n": 59, "auc": 0.9789915966386554}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7923076923076924, "accuracy_n": 23, "auc": 0.7923076923076924}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9405840886203424, "accuracy_n": 993, "auc": 0.9405840886203424}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9438314944834504, "accuracy_n": 997, "auc": 0.9438314944834504}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6227364185110664, "accuracy_n": 994, "auc": 0.6227364185110664}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.44889779559118237, "accuracy_n": 499, "auc": 0.44889779559118237}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.552, "accuracy_n": 500, "auc": 0.552}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6174496644295302, "accuracy_n": 298, "auc": 0.6174496644295302}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5276381909547738, "accuracy_n": 398, "auc": 0.5276381909547738}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.7413793103448276, "accuracy_n": 58, "auc": 0.7413793103448276}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9646277417361755, "accuracy_n": 322, "auc": 0.9646277417361755}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6353383458646616, "accuracy_n": 292, "auc": 0.6353383458646616}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6690839525831564, "accuracy_n": 1902, "auc": 0.6690839525831564}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6296983210939149, "accuracy_n": 2000, "auc": 0.6296983210939149}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9523809523809523, "accuracy_n": 59, "auc": 0.9523809523809523}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9436052366565961, "accuracy_n": 993, "auc": 0.9436052366565961}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.56, "accuracy_n": 100, "auc": 0.56}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6126760563380281, "accuracy_n": 994, "auc": 0.6126760563380281}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.39879759519038077, "accuracy_n": 499, "auc": 0.39879759519038077}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.426, "accuracy_n": 500, "auc": 0.426}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.3065326633165829, "accuracy_n": 398, "auc": 0.3065326633165829}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6896551724137931, "accuracy_n": 58, "auc": 0.6896551724137931}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9477139326536916, "accuracy_n": 322, "auc": 0.9477139326536916}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5598684210526316, "accuracy_n": 292, "auc": 0.5598684210526316}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.5927956917905166, "accuracy_n": 1902, "auc": 0.5927956917905166}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.6218364829703523, "accuracy_n": 2000, "auc": 0.6218364829703523}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.9915966386554621, "accuracy_n": 59, "auc": 0.9915966386554621}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h29", "token_idx": -1, "accuracy": 0.8615384615384616, "accuracy_n": 23, "auc": 0.8615384615384616}}
