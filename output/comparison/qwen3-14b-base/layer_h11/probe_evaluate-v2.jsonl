{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6258776328986961, "accuracy_n": 997, "auc": 0.6258776328986961}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3046092184368738, "accuracy_n": 499, "auc": 0.3046092184368738}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24, "accuracy_n": 500, "auc": 0.24}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24874371859296482, "accuracy_n": 398, "auc": 0.24874371859296482}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6533055298115539, "accuracy_n": 322, "auc": 0.6533055298115539}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5391917293233082, "accuracy_n": 292, "auc": 0.5391917293233082}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7155343241330503, "accuracy_n": 1902, "auc": 0.7155343241330503}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5289889650163708, "accuracy_n": 2000, "auc": 0.5289889650163708}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5532212885154062, "accuracy_n": 59, "auc": 0.5532212885154062}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6193353474320241, "accuracy_n": 993, "auc": 0.6193353474320241}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5566700100300903, "accuracy_n": 997, "auc": 0.5566700100300903}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3226452905811623, "accuracy_n": 499, "auc": 0.3226452905811623}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.276, "accuracy_n": 500, "auc": 0.276}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31543624161073824, "accuracy_n": 298, "auc": 0.31543624161073824}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.25879396984924624, "accuracy_n": 398, "auc": 0.25879396984924624}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6605653382761817, "accuracy_n": 322, "auc": 0.6605653382761817}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5291823308270677, "accuracy_n": 292, "auc": 0.5291823308270677}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8201344656758669, "accuracy_n": 1902, "auc": 0.8201344656758669}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6260039874394656, "accuracy_n": 2000, "auc": 0.6260039874394656}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.607843137254902, "accuracy_n": 59, "auc": 0.607843137254902}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5166163141993958, "accuracy_n": 993, "auc": 0.5166163141993958}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5526579739217653, "accuracy_n": 997, "auc": 0.5526579739217653}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.516245487364621, "accuracy_n": 277, "auc": 0.516245487364621}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2565130260521042, "accuracy_n": 499, "auc": 0.2565130260521042}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.266, "accuracy_n": 500, "auc": 0.266}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.22613065326633167, "accuracy_n": 398, "auc": 0.22613065326633167}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5397350993377483, "accuracy_n": 302, "auc": 0.5397350993377483}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5304294099474822, "accuracy_n": 322, "auc": 0.5304294099474822}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5491541353383459, "accuracy_n": 292, "auc": 0.5491541353383459}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7116977397381459, "accuracy_n": 1902, "auc": 0.7116977397381459}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5188232952095706, "accuracy_n": 2000, "auc": 0.5188232952095706}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5056022408963585, "accuracy_n": 59, "auc": 0.5056022408963585}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5307692307692308, "accuracy_n": 23, "auc": 0.5307692307692308}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5897693079237714, "accuracy_n": 997, "auc": 0.5897693079237714}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3226452905811623, "accuracy_n": 499, "auc": 0.3226452905811623}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.262, "accuracy_n": 500, "auc": 0.262}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31543624161073824, "accuracy_n": 298, "auc": 0.31543624161073824}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.22110552763819097, "accuracy_n": 398, "auc": 0.22110552763819097}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6315647204201422, "accuracy_n": 322, "auc": 0.6315647204201422}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6415883458646616, "accuracy_n": 292, "auc": 0.6415883458646616}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6388535031847135, "accuracy_n": 1902, "auc": 0.6388535031847135}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5296792141963249, "accuracy_n": 2000, "auc": 0.5296792141963249}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5098039215686275, "accuracy_n": 59, "auc": 0.5098039215686275}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5230769230769231, "accuracy_n": 23, "auc": 0.5230769230769231}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.522567703109328, "accuracy_n": 997, "auc": 0.522567703109328}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.272, "accuracy_n": 500, "auc": 0.272}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.23366834170854273, "accuracy_n": 398, "auc": 0.23366834170854273}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5331125827814569, "accuracy_n": 302, "auc": 0.5331125827814569}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6491736175471114, "accuracy_n": 322, "auc": 0.6491736175471114}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6401315789473684, "accuracy_n": 292, "auc": 0.6401315789473684}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6200482130219391, "accuracy_n": 1902, "auc": 0.6200482130219391}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5391566355454319, "accuracy_n": 2000, "auc": 0.5391566355454319}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6260504201680672, "accuracy_n": 59, "auc": 0.6260504201680672}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5538461538461539, "accuracy_n": 23, "auc": 0.5538461538461539}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5196374622356495, "accuracy_n": 993, "auc": 0.5196374622356495}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6158475426278837, "accuracy_n": 997, "auc": 0.6158475426278837}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.44288577154308617, "accuracy_n": 499, "auc": 0.44288577154308617}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3053691275167785, "accuracy_n": 298, "auc": 0.3053691275167785}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27638190954773867, "accuracy_n": 398, "auc": 0.27638190954773867}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5298013245033113, "accuracy_n": 302, "auc": 0.5298013245033113}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.516759345072598, "accuracy_n": 322, "auc": 0.516759345072598}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5132988721804511, "accuracy_n": 292, "auc": 0.5132988721804511}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5205811138014528, "accuracy_n": 413, "auc": 0.5205811138014528}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7114516985138005, "accuracy_n": 1902, "auc": 0.7114516985138005}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5266191094985291, "accuracy_n": 2000, "auc": 0.5266191094985291}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5364145658263305, "accuracy_n": 59, "auc": 0.5364145658263305}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6846153846153847, "accuracy_n": 23, "auc": 0.6846153846153847}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.513595166163142, "accuracy_n": 993, "auc": 0.513595166163142}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5255767301905717, "accuracy_n": 997, "auc": 0.5255767301905717}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5975855130784709, "accuracy_n": 994, "auc": 0.5975855130784709}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3366733466933868, "accuracy_n": 499, "auc": 0.3366733466933868}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.406, "accuracy_n": 500, "auc": 0.406}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2953020134228188, "accuracy_n": 298, "auc": 0.2953020134228188}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.29396984924623115, "accuracy_n": 398, "auc": 0.29396984924623115}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5894039735099338, "accuracy_n": 302, "auc": 0.5894039735099338}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5459916589434662, "accuracy_n": 322, "auc": 0.5459916589434662}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6052161654135338, "accuracy_n": 292, "auc": 0.6052161654135338}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5956416464891041, "accuracy_n": 413, "auc": 0.5956416464891041}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6076908616418968, "accuracy_n": 1902, "auc": 0.6076908616418968}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5443064946445667, "accuracy_n": 2000, "auc": 0.5443064946445667}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5140056022408963, "accuracy_n": 59, "auc": 0.5140056022408963}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5125881168177241, "accuracy_n": 993, "auc": 0.5125881168177241}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6469408224674023, "accuracy_n": 997, "auc": 0.6469408224674023}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.51985559566787, "accuracy_n": 277, "auc": 0.51985559566787}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27054108216432865, "accuracy_n": 499, "auc": 0.27054108216432865}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.258, "accuracy_n": 500, "auc": 0.258}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3288590604026846, "accuracy_n": 298, "auc": 0.3288590604026846}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.23869346733668342, "accuracy_n": 398, "auc": 0.23869346733668342}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7947019867549668, "accuracy_n": 302, "auc": 0.7947019867549668}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5344827586206896, "accuracy_n": 58, "auc": 0.5344827586206896}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5666512202656782, "accuracy_n": 322, "auc": 0.5666512202656782}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6445488721804512, "accuracy_n": 292, "auc": 0.6445488721804512}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5108958837772397, "accuracy_n": 413, "auc": 0.5108958837772397}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6276395523708422, "accuracy_n": 1902, "auc": 0.6276395523708422}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5355773434209751, "accuracy_n": 2000, "auc": 0.5355773434209751}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5476190476190476, "accuracy_n": 59, "auc": 0.5476190476190476}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5461538461538462, "accuracy_n": 23, "auc": 0.5461538461538462}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5226586102719033, "accuracy_n": 993, "auc": 0.5226586102719033}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6449348044132397, "accuracy_n": 997, "auc": 0.6449348044132397}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6146881287726358, "accuracy_n": 994, "auc": 0.6146881287726358}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3306613226452906, "accuracy_n": 499, "auc": 0.3306613226452906}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.348, "accuracy_n": 500, "auc": 0.348}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3542713567839196, "accuracy_n": 398, "auc": 0.3542713567839196}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5993377483443708, "accuracy_n": 302, "auc": 0.5993377483443708}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6483240654927402, "accuracy_n": 322, "auc": 0.6483240654927402}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.662265037593985, "accuracy_n": 292, "auc": 0.662265037593985}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5351089588377724, "accuracy_n": 413, "auc": 0.5351089588377724}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5393632784854919, "accuracy_n": 1902, "auc": 0.5393632784854919}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5463792429066894, "accuracy_n": 2000, "auc": 0.5463792429066894}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6162464985994398, "accuracy_n": 59, "auc": 0.6162464985994398}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7153846153846154, "accuracy_n": 23, "auc": 0.7153846153846154}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5807422266800402, "accuracy_n": 997, "auc": 0.5807422266800402}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.30060120240480964, "accuracy_n": 499, "auc": 0.30060120240480964}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 500, "auc": 0.26}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.228643216080402, "accuracy_n": 398, "auc": 0.228643216080402}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8874172185430463, "accuracy_n": 302, "auc": 0.8874172185430463}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6860132839048501, "accuracy_n": 322, "auc": 0.6860132839048501}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7005169172932332, "accuracy_n": 292, "auc": 0.7005169172932332}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5768057767162067, "accuracy_n": 1902, "auc": 0.5768057767162067}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6238652153427388, "accuracy_n": 2000, "auc": 0.6238652153427388}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7016806722689075, "accuracy_n": 59, "auc": 0.7016806722689075}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6538461538461539, "accuracy_n": 23, "auc": 0.6538461538461539}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5195586760280843, "accuracy_n": 997, "auc": 0.5195586760280843}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6207243460764588, "accuracy_n": 994, "auc": 0.6207243460764588}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2845691382765531, "accuracy_n": 499, "auc": 0.2845691382765531}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.18090452261306533, "accuracy_n": 398, "auc": 0.18090452261306533}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9205298013245033, "accuracy_n": 302, "auc": 0.9205298013245033}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7491504479456287, "accuracy_n": 322, "auc": 0.7491504479456287}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.669360902255639, "accuracy_n": 292, "auc": 0.669360902255639}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.513317191283293, "accuracy_n": 413, "auc": 0.513317191283293}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6295890835102618, "accuracy_n": 1902, "auc": 0.6295890835102618}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5690434246763082, "accuracy_n": 2000, "auc": 0.5690434246763082}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6372549019607844, "accuracy_n": 59, "auc": 0.6372549019607844}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5846153846153845, "accuracy_n": 23, "auc": 0.5846153846153845}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5166163141993958, "accuracy_n": 993, "auc": 0.5166163141993958}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5005015045135406, "accuracy_n": 997, "auc": 0.5005015045135406}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.28857715430861725, "accuracy_n": 499, "auc": 0.28857715430861725}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.248, "accuracy_n": 500, "auc": 0.248}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31543624161073824, "accuracy_n": 298, "auc": 0.31543624161073824}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.31155778894472363, "accuracy_n": 398, "auc": 0.31155778894472363}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8415971578622181, "accuracy_n": 322, "auc": 0.8415971578622181}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8400375939849624, "accuracy_n": 292, "auc": 0.8400375939849624}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5617433414043583, "accuracy_n": 413, "auc": 0.5617433414043583}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5179482926397736, "accuracy_n": 1902, "auc": 0.5179482926397736}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6753287936945237, "accuracy_n": 2000, "auc": 0.6753287936945237}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6246498599439776, "accuracy_n": 59, "auc": 0.6246498599439776}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6076923076923078, "accuracy_n": 23, "auc": 0.6076923076923078}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5105315947843531, "accuracy_n": 997, "auc": 0.5105315947843531}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5234657039711191, "accuracy_n": 277, "auc": 0.5234657039711191}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.25851703406813625, "accuracy_n": 499, "auc": 0.25851703406813625}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26, "accuracy_n": 500, "auc": 0.26}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.34563758389261745, "accuracy_n": 298, "auc": 0.34563758389261745}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.26884422110552764, "accuracy_n": 398, "auc": 0.26884422110552764}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8128668520234785, "accuracy_n": 322, "auc": 0.8128668520234785}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7976033834586466, "accuracy_n": 292, "auc": 0.7976033834586466}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5520581113801453, "accuracy_n": 413, "auc": 0.5520581113801453}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5590797505307856, "accuracy_n": 1902, "auc": 0.5590797505307856}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6484090756763192, "accuracy_n": 2000, "auc": 0.6484090756763192}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5015105740181269, "accuracy_n": 993, "auc": 0.5015105740181269}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5315947843530592, "accuracy_n": 997, "auc": 0.5315947843530592}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6187122736418511, "accuracy_n": 994, "auc": 0.6187122736418511}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27054108216432865, "accuracy_n": 499, "auc": 0.27054108216432865}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.324, "accuracy_n": 500, "auc": 0.324}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20100502512562815, "accuracy_n": 398, "auc": 0.20100502512562815}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5636777880753785, "accuracy_n": 322, "auc": 0.5636777880753785}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5050751879699248, "accuracy_n": 292, "auc": 0.5050751879699248}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5907990314769975, "accuracy_n": 413, "auc": 0.5907990314769975}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6356886942675158, "accuracy_n": 1902, "auc": 0.6356886942675158}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5083515148968778, "accuracy_n": 2000, "auc": 0.5083515148968778}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5434173669467788, "accuracy_n": 59, "auc": 0.5434173669467788}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5846153846153846, "accuracy_n": 23, "auc": 0.5846153846153846}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5576730190571715, "accuracy_n": 997, "auc": 0.5576730190571715}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3527054108216433, "accuracy_n": 499, "auc": 0.3527054108216433}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.292, "accuracy_n": 500, "auc": 0.292}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.29194630872483224, "accuracy_n": 298, "auc": 0.29194630872483224}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.20351758793969849, "accuracy_n": 398, "auc": 0.20351758793969849}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5827814569536424, "accuracy_n": 302, "auc": 0.5827814569536424}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5100787766450418, "accuracy_n": 322, "auc": 0.5100787766450418}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5012687969924812, "accuracy_n": 292, "auc": 0.5012687969924812}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5060532687651331, "accuracy_n": 413, "auc": 0.5060532687651331}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.9140326433121019, "accuracy_n": 1902, "auc": 0.9140326433121019}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5203158340160798, "accuracy_n": 2000, "auc": 0.5203158340160798}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5700280112044818, "accuracy_n": 59, "auc": 0.5700280112044818}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7692307692307693, "accuracy_n": 23, "auc": 0.7692307692307693}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5146022155085599, "accuracy_n": 993, "auc": 0.5146022155085599}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8525576730190572, "accuracy_n": 997, "auc": 0.8525576730190572}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5054151624548736, "accuracy_n": 277, "auc": 0.5054151624548736}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2685370741482966, "accuracy_n": 499, "auc": 0.2685370741482966}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.276, "accuracy_n": 500, "auc": 0.276}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3053691275167785, "accuracy_n": 298, "auc": 0.3053691275167785}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.27638190954773867, "accuracy_n": 398, "auc": 0.27638190954773867}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5364238410596026, "accuracy_n": 302, "auc": 0.5364238410596026}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5, "accuracy_n": 58, "auc": 0.5}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7920142106889094, "accuracy_n": 322, "auc": 0.7920142106889094}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.800704887218045, "accuracy_n": 292, "auc": 0.800704887218045}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5302663438256658, "accuracy_n": 413, "auc": 0.5302663438256658}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6171510084925691, "accuracy_n": 1902, "auc": 0.6171510084925691}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7569932745721206, "accuracy_n": 2000, "auc": 0.7569932745721206}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6064425770308124, "accuracy_n": 59, "auc": 0.6064425770308124}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.576923076923077, "accuracy_n": 23, "auc": 0.576923076923077}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.513595166163142, "accuracy_n": 993, "auc": 0.513595166163142}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5997993981945837, "accuracy_n": 997, "auc": 0.5997993981945837}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6177062374245473, "accuracy_n": 994, "auc": 0.6177062374245473}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2785571142284569, "accuracy_n": 499, "auc": 0.2785571142284569}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2684563758389262, "accuracy_n": 298, "auc": 0.2684563758389262}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.23618090452261306, "accuracy_n": 398, "auc": 0.23618090452261306}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.609271523178808, "accuracy_n": 302, "auc": 0.609271523178808}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6206896551724138, "accuracy_n": 58, "auc": 0.6206896551724138}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.671068890948409, "accuracy_n": 322, "auc": 0.671068890948409}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6441259398496241, "accuracy_n": 292, "auc": 0.6441259398496241}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5036319612590799, "accuracy_n": 413, "auc": 0.5036319612590799}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.783132077140835, "accuracy_n": 1902, "auc": 0.783132077140835}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5666395568800338, "accuracy_n": 2000, "auc": 0.5666395568800338}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6092436974789917, "accuracy_n": 59, "auc": 0.6092436974789917}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.513595166163142, "accuracy_n": 993, "auc": 0.513595166163142}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.555667001003009, "accuracy_n": 997, "auc": 0.555667001003009}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.54, "accuracy_n": 100, "auc": 0.54}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6167002012072434, "accuracy_n": 994, "auc": 0.6167002012072434}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.3346693386773547, "accuracy_n": 499, "auc": 0.3346693386773547}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.254, "accuracy_n": 500, "auc": 0.254}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.2651006711409396, "accuracy_n": 298, "auc": 0.2651006711409396}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.24874371859296482, "accuracy_n": 398, "auc": 0.24874371859296482}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5463576158940397, "accuracy_n": 302, "auc": 0.5463576158940397}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6551724137931034, "accuracy_n": 58, "auc": 0.6551724137931034}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5022397281433426, "accuracy_n": 322, "auc": 0.5022397281433426}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5032894736842105, "accuracy_n": 292, "auc": 0.5032894736842105}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5084745762711864, "accuracy_n": 413, "auc": 0.5084745762711864}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.7128527512384998, "accuracy_n": 1902, "auc": 0.7128527512384998}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.5099600955945096, "accuracy_n": 2000, "auc": 0.5099600955945096}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.6064425770308123, "accuracy_n": 59, "auc": 0.6064425770308123}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h11", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 23, "auc": 0.8}}
