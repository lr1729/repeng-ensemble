{"key": "result_0", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8882175226586103, "accuracy_n": 993, "auc": 0.8882175226586103}}
{"key": "result_1", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.921765295887663, "accuracy_n": 997, "auc": 0.921765295887663}}
{"key": "result_2", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7509025270758123, "accuracy_n": 277, "auc": 0.7509025270758123}}
{"key": "result_3", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.92, "accuracy_n": 100, "auc": 0.92}}
{"key": "result_4", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7374245472837022, "accuracy_n": 994, "auc": 0.7374245472837022}}
{"key": "result_5", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3587174348697395, "accuracy_n": 499, "auc": 0.3587174348697395}}
{"key": "result_6", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.368, "accuracy_n": 500, "auc": 0.368}}
{"key": "result_7", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3825503355704698, "accuracy_n": 298, "auc": 0.3825503355704698}}
{"key": "result_8", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.24371859296482412, "accuracy_n": 398, "auc": 0.24371859296482412}}
{"key": "result_9", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 302, "auc": 1.0}}
{"key": "result_10", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_11", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9438523324065493, "accuracy_n": 322, "auc": 0.9438523324065493}}
{"key": "result_12", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7515977443609022, "accuracy_n": 292, "auc": 0.7515977443609022}}
{"key": "result_13", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_14", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8584792993630573, "accuracy_n": 1902, "auc": 0.8584792993630573}}
{"key": "result_15", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.815856524205238, "accuracy_n": 2000, "auc": 0.815856524205238}}
{"key": "result_16", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9355742296918768, "accuracy_n": 59, "auc": 0.9355742296918768}}
{"key": "result_17", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "imdb", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7615384615384615, "accuracy_n": 23, "auc": 0.7615384615384615}}
{"key": "result_18", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9174219536757301, "accuracy_n": 993, "auc": 0.9174219536757301}}
{"key": "result_19", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9458375125376128, "accuracy_n": 997, "auc": 0.9458375125376128}}
{"key": "result_20", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7725631768953068, "accuracy_n": 277, "auc": 0.7725631768953068}}
{"key": "result_21", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_22", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7625754527162978, "accuracy_n": 994, "auc": 0.7625754527162978}}
{"key": "result_23", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.28256513026052105, "accuracy_n": 499, "auc": 0.28256513026052105}}
{"key": "result_24", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.35, "accuracy_n": 500, "auc": 0.35}}
{"key": "result_25", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31208053691275167, "accuracy_n": 298, "auc": 0.31208053691275167}}
{"key": "result_26", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2185929648241206, "accuracy_n": 398, "auc": 0.2185929648241206}}
{"key": "result_27", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_28", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_29", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9509962928637627, "accuracy_n": 322, "auc": 0.9509962928637627}}
{"key": "result_30", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8115601503759399, "accuracy_n": 292, "auc": 0.8115601503759399}}
{"key": "result_31", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_32", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9088386854210899, "accuracy_n": 1902, "auc": 0.9088386854210899}}
{"key": "result_33", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8240654876410385, "accuracy_n": 2000, "auc": 0.8240654876410385}}
{"key": "result_34", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9061624649859944, "accuracy_n": 59, "auc": 0.9061624649859944}}
{"key": "result_35", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "ag_news", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_36", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9295065458207452, "accuracy_n": 993, "auc": 0.9295065458207452}}
{"key": "result_37", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9348044132397192, "accuracy_n": 997, "auc": 0.9348044132397192}}
{"key": "result_38", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7906137184115524, "accuracy_n": 277, "auc": 0.7906137184115524}}
{"key": "result_39", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.91, "accuracy_n": 100, "auc": 0.91}}
{"key": "result_40", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7354124748490946, "accuracy_n": 994, "auc": 0.7354124748490946}}
{"key": "result_41", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36072144288577157, "accuracy_n": 499, "auc": 0.36072144288577157}}
{"key": "result_42", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_43", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2516778523489933, "accuracy_n": 298, "auc": 0.2516778523489933}}
{"key": "result_44", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3743718592964824, "accuracy_n": 398, "auc": 0.3743718592964824}}
{"key": "result_45", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_46", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_47", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9403768921841211, "accuracy_n": 322, "auc": 0.9403768921841211}}
{"key": "result_48", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7896616541353384, "accuracy_n": 292, "auc": 0.7896616541353384}}
{"key": "result_49", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_50", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8537320859872611, "accuracy_n": 1902, "auc": 0.8537320859872611}}
{"key": "result_51", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8490034902599838, "accuracy_n": 2000, "auc": 0.8490034902599838}}
{"key": "result_52", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9313725490196079, "accuracy_n": 59, "auc": 0.9313725490196079}}
{"key": "result_53", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "rte", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7307692307692308, "accuracy_n": 23, "auc": 0.7307692307692308}}
{"key": "result_54", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.919436052366566, "accuracy_n": 993, "auc": 0.919436052366566}}
{"key": "result_55", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_56", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7689530685920578, "accuracy_n": 277, "auc": 0.7689530685920578}}
{"key": "result_57", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.84, "accuracy_n": 100, "auc": 0.84}}
{"key": "result_58", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7354124748490946, "accuracy_n": 994, "auc": 0.7354124748490946}}
{"key": "result_59", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3727454909819639, "accuracy_n": 499, "auc": 0.3727454909819639}}
{"key": "result_60", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.364, "accuracy_n": 500, "auc": 0.364}}
{"key": "result_61", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4798657718120805, "accuracy_n": 298, "auc": 0.4798657718120805}}
{"key": "result_62", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3417085427135678, "accuracy_n": 398, "auc": 0.3417085427135678}}
{"key": "result_63", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_64", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_65", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9316882916280507, "accuracy_n": 322, "auc": 0.9316882916280507}}
{"key": "result_66", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7982142857142857, "accuracy_n": 292, "auc": 0.7982142857142857}}
{"key": "result_67", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_68", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8370134465675867, "accuracy_n": 1902, "auc": 0.8370134465675867}}
{"key": "result_69", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8361678565962312, "accuracy_n": 2000, "auc": 0.8361678565962312}}
{"key": "result_70", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9313725490196078, "accuracy_n": 59, "auc": 0.9313725490196078}}
{"key": "result_71", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "copa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8076923076923077, "accuracy_n": 23, "auc": 0.8076923076923077}}
{"key": "result_72", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5176233635448136, "accuracy_n": 993, "auc": 0.5176233635448136}}
{"key": "result_73", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9277833500501504, "accuracy_n": 997, "auc": 0.9277833500501504}}
{"key": "result_74", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_75", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_76", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6207243460764588, "accuracy_n": 994, "auc": 0.6207243460764588}}
{"key": "result_77", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.33266533066132264, "accuracy_n": 499, "auc": 0.33266533066132264}}
{"key": "result_78", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.284, "accuracy_n": 500, "auc": 0.284}}
{"key": "result_79", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.28859060402684567, "accuracy_n": 298, "auc": 0.28859060402684567}}
{"key": "result_80", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.30904522613065327, "accuracy_n": 398, "auc": 0.30904522613065327}}
{"key": "result_81", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.652317880794702, "accuracy_n": 302, "auc": 0.652317880794702}}
{"key": "result_82", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_83", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8450339820821748, "accuracy_n": 322, "auc": 0.8450339820821748}}
{"key": "result_84", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7648026315789476, "accuracy_n": 292, "auc": 0.7648026315789476}}
{"key": "result_85", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9443099273607748, "accuracy_n": 413, "auc": 0.9443099273607748}}
{"key": "result_86", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.643204838995046, "accuracy_n": 1902, "auc": 0.643204838995046}}
{"key": "result_87", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.70219849365621, "accuracy_n": 2000, "auc": 0.70219849365621}}
{"key": "result_88", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7661064425770309, "accuracy_n": 59, "auc": 0.7661064425770309}}
{"key": "result_89", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "boolq", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5692307692307692, "accuracy_n": 23, "auc": 0.5692307692307692}}
{"key": "result_90", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8338368580060423, "accuracy_n": 993, "auc": 0.8338368580060423}}
{"key": "result_91", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5025075225677031, "accuracy_n": 997, "auc": 0.5025075225677031}}
{"key": "result_92", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5595667870036101, "accuracy_n": 277, "auc": 0.5595667870036101}}
{"key": "result_93", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.79, "accuracy_n": 100, "auc": 0.79}}
{"key": "result_94", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6509054325955734, "accuracy_n": 994, "auc": 0.6509054325955734}}
{"key": "result_95", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6012024048096193, "accuracy_n": 499, "auc": 0.6012024048096193}}
{"key": "result_96", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.518, "accuracy_n": 500, "auc": 0.518}}
{"key": "result_97", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4966442953020134, "accuracy_n": 298, "auc": 0.4966442953020134}}
{"key": "result_98", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6457286432160804, "accuracy_n": 398, "auc": 0.6457286432160804}}
{"key": "result_99", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6059602649006622, "accuracy_n": 302, "auc": 0.6059602649006622}}
{"key": "result_100", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5172413793103449, "accuracy_n": 58, "auc": 0.5172413793103449}}
{"key": "result_101", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7037380290392339, "accuracy_n": 322, "auc": 0.7037380290392339}}
{"key": "result_102", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6942199248120301, "accuracy_n": 292, "auc": 0.6942199248120301}}
{"key": "result_103", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.738498789346247, "accuracy_n": 413, "auc": 0.738498789346247}}
{"key": "result_104", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7280111022646851, "accuracy_n": 1902, "auc": 0.7280111022646851}}
{"key": "result_105", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5276729899493717, "accuracy_n": 2000, "auc": 0.5276729899493717}}
{"key": "result_106", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5742296918767508, "accuracy_n": 59, "auc": 0.5742296918767508}}
{"key": "result_107", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "open_book_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6923076923076924, "accuracy_n": 23, "auc": 0.6923076923076924}}
{"key": "result_108", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9073514602215509, "accuracy_n": 993, "auc": 0.9073514602215509}}
{"key": "result_109", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7301905717151455, "accuracy_n": 997, "auc": 0.7301905717151455}}
{"key": "result_110", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5126353790613718, "accuracy_n": 277, "auc": 0.5126353790613718}}
{"key": "result_111", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8, "accuracy_n": 100, "auc": 0.8}}
{"key": "result_112", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6197183098591549, "accuracy_n": 994, "auc": 0.6197183098591549}}
{"key": "result_113", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5851703406813628, "accuracy_n": 499, "auc": 0.5851703406813628}}
{"key": "result_114", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.574, "accuracy_n": 500, "auc": 0.574}}
{"key": "result_115", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6442953020134228, "accuracy_n": 298, "auc": 0.6442953020134228}}
{"key": "result_116", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7060301507537688, "accuracy_n": 398, "auc": 0.7060301507537688}}
{"key": "result_117", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5264900662251656, "accuracy_n": 302, "auc": 0.5264900662251656}}
{"key": "result_118", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_119", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8963160333642262, "accuracy_n": 322, "auc": 0.8963160333642262}}
{"key": "result_120", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.712406015037594, "accuracy_n": 292, "auc": 0.712406015037594}}
{"key": "result_121", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8983050847457628, "accuracy_n": 413, "auc": 0.8983050847457628}}
{"key": "result_122", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5649537774239207, "accuracy_n": 1902, "auc": 0.5649537774239207}}
{"key": "result_123", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5836987152362003, "accuracy_n": 2000, "auc": 0.5836987152362003}}
{"key": "result_124", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6764705882352942, "accuracy_n": 59, "auc": 0.6764705882352942}}
{"key": "result_125", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "race", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9384615384615385, "accuracy_n": 23, "auc": 0.9384615384615385}}
{"key": "result_126", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8710976837865055, "accuracy_n": 993, "auc": 0.8710976837865055}}
{"key": "result_127", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8445336008024072, "accuracy_n": 997, "auc": 0.8445336008024072}}
{"key": "result_128", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5523465703971119, "accuracy_n": 277, "auc": 0.5523465703971119}}
{"key": "result_129", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.87, "accuracy_n": 100, "auc": 0.87}}
{"key": "result_130", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_131", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5871743486973948, "accuracy_n": 499, "auc": 0.5871743486973948}}
{"key": "result_132", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 500, "auc": 0.55}}
{"key": "result_133", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6577181208053692, "accuracy_n": 298, "auc": 0.6577181208053692}}
{"key": "result_134", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7211055276381909, "accuracy_n": 398, "auc": 0.7211055276381909}}
{"key": "result_135", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7880794701986755, "accuracy_n": 302, "auc": 0.7880794701986755}}
{"key": "result_136", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5689655172413793, "accuracy_n": 58, "auc": 0.5689655172413793}}
{"key": "result_137", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9265523632993512, "accuracy_n": 322, "auc": 0.9265523632993512}}
{"key": "result_138", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7659774436090225, "accuracy_n": 292, "auc": 0.7659774436090225}}
{"key": "result_139", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6997578692493946, "accuracy_n": 413, "auc": 0.6997578692493946}}
{"key": "result_140", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5923511588818118, "accuracy_n": 1902, "auc": 0.5923511588818118}}
{"key": "result_141", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5871569636638826, "accuracy_n": 2000, "auc": 0.5871569636638826}}
{"key": "result_142", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5980392156862745, "accuracy_n": 59, "auc": 0.5980392156862745}}
{"key": "result_143", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "arc_challenge", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8307692307692308, "accuracy_n": 23, "auc": 0.8307692307692308}}
{"key": "result_144", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5156092648539778, "accuracy_n": 993, "auc": 0.5156092648539778}}
{"key": "result_145", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5135406218655968, "accuracy_n": 997, "auc": 0.5135406218655968}}
{"key": "result_146", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6823104693140795, "accuracy_n": 277, "auc": 0.6823104693140795}}
{"key": "result_147", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.75, "accuracy_n": 100, "auc": 0.75}}
{"key": "result_148", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7072434607645876, "accuracy_n": 994, "auc": 0.7072434607645876}}
{"key": "result_149", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5511022044088176, "accuracy_n": 499, "auc": 0.5511022044088176}}
{"key": "result_150", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.508, "accuracy_n": 500, "auc": 0.508}}
{"key": "result_151", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6208053691275168, "accuracy_n": 298, "auc": 0.6208053691275168}}
{"key": "result_152", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6934673366834171, "accuracy_n": 398, "auc": 0.6934673366834171}}
{"key": "result_153", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8807947019867549, "accuracy_n": 302, "auc": 0.8807947019867549}}
{"key": "result_154", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6379310344827587, "accuracy_n": 58, "auc": 0.6379310344827587}}
{"key": "result_155", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6862449799196788, "accuracy_n": 322, "auc": 0.6862449799196788}}
{"key": "result_156", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5561560150375939, "accuracy_n": 292, "auc": 0.5561560150375939}}
{"key": "result_157", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7191283292978208, "accuracy_n": 413, "auc": 0.7191283292978208}}
{"key": "result_158", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5049562101910828, "accuracy_n": 1902, "auc": 0.5049562101910828}}
{"key": "result_159", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.504680189548427, "accuracy_n": 2000, "auc": 0.504680189548427}}
{"key": "result_160", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5742296918767508, "accuracy_n": 59, "auc": 0.5742296918767508}}
{"key": "result_161", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "common_sense_qa", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7461538461538462, "accuracy_n": 23, "auc": 0.7461538461538462}}
{"key": "result_162", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9244712990936556, "accuracy_n": 993, "auc": 0.9244712990936556}}
{"key": "result_163", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9388164493480441, "accuracy_n": 997, "auc": 0.9388164493480441}}
{"key": "result_164", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7725631768953068, "accuracy_n": 277, "auc": 0.7725631768953068}}
{"key": "result_165", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_166", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7595573440643864, "accuracy_n": 994, "auc": 0.7595573440643864}}
{"key": "result_167", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3026052104208417, "accuracy_n": 499, "auc": 0.3026052104208417}}
{"key": "result_168", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.262, "accuracy_n": 500, "auc": 0.262}}
{"key": "result_169", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3523489932885906, "accuracy_n": 298, "auc": 0.3523489932885906}}
{"key": "result_170", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.507537688442211, "accuracy_n": 398, "auc": 0.507537688442211}}
{"key": "result_171", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 302, "auc": 1.0}}
{"key": "result_172", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_173", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9820049428483164, "accuracy_n": 322, "auc": 0.9820049428483164}}
{"key": "result_174", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7984492481203008, "accuracy_n": 292, "auc": 0.7984492481203008}}
{"key": "result_175", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_176", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.795132254069356, "accuracy_n": 1902, "auc": 0.795132254069356}}
{"key": "result_177", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8293533965761639, "accuracy_n": 2000, "auc": 0.8293533965761639}}
{"key": "result_178", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9369747899159664, "accuracy_n": 59, "auc": 0.9369747899159664}}
{"key": "result_179", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_180", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9234642497482377, "accuracy_n": 993, "auc": 0.9234642497482377}}
{"key": "result_181", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9398194583751254, "accuracy_n": 997, "auc": 0.9398194583751254}}
{"key": "result_182", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6714801444043321, "accuracy_n": 277, "auc": 0.6714801444043321}}
{"key": "result_183", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_184", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7515090543259557, "accuracy_n": 994, "auc": 0.7515090543259557}}
{"key": "result_185", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3066132264529058, "accuracy_n": 499, "auc": 0.3066132264529058}}
{"key": "result_186", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.264, "accuracy_n": 500, "auc": 0.264}}
{"key": "result_187", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3087248322147651, "accuracy_n": 298, "auc": 0.3087248322147651}}
{"key": "result_188", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.24371859296482412, "accuracy_n": 398, "auc": 0.24371859296482412}}
{"key": "result_189", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 302, "auc": 1.0}}
{"key": "result_190", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 58, "auc": 1.0}}
{"key": "result_191", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9430027803521779, "accuracy_n": 322, "auc": 0.9430027803521779}}
{"key": "result_192", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7844924812030075, "accuracy_n": 292, "auc": 0.7844924812030075}}
{"key": "result_193", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_194", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7043845099079973, "accuracy_n": 1902, "auc": 0.7043845099079973}}
{"key": "result_195", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7671009234333594, "accuracy_n": 2000, "auc": 0.7671009234333594}}
{"key": "result_196", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8515406162464987, "accuracy_n": 59, "auc": 0.8515406162464987}}
{"key": "result_197", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_sp_en_trans", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5923076923076923, "accuracy_n": 23, "auc": 0.5923076923076923}}
{"key": "result_198", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9284994964753273, "accuracy_n": 993, "auc": 0.9284994964753273}}
{"key": "result_199", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_200", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7617328519855595, "accuracy_n": 277, "auc": 0.7617328519855595}}
{"key": "result_201", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_202", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.721327967806841, "accuracy_n": 994, "auc": 0.721327967806841}}
{"key": "result_203", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3787575150300601, "accuracy_n": 499, "auc": 0.3787575150300601}}
{"key": "result_204", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.354, "accuracy_n": 500, "auc": 0.354}}
{"key": "result_205", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.4463087248322148, "accuracy_n": 298, "auc": 0.4463087248322148}}
{"key": "result_206", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3391959798994975, "accuracy_n": 398, "auc": 0.3391959798994975}}
{"key": "result_207", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_208", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_209", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9857506950880446, "accuracy_n": 322, "auc": 0.9857506950880446}}
{"key": "result_210", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9028665413533834, "accuracy_n": 292, "auc": 0.9028665413533834}}
{"key": "result_211", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9975786924939467, "accuracy_n": 413, "auc": 0.9975786924939467}}
{"key": "result_212", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6026605626326964, "accuracy_n": 1902, "auc": 0.6026605626326964}}
{"key": "result_213", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8381485716343601, "accuracy_n": 2000, "auc": 0.8381485716343601}}
{"key": "result_214", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9089635854341737, "accuracy_n": 59, "auc": 0.9089635854341737}}
{"key": "result_215", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_conj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7769230769230768, "accuracy_n": 23, "auc": 0.7769230769230768}}
{"key": "result_216", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9023162134944612, "accuracy_n": 993, "auc": 0.9023162134944612}}
{"key": "result_217", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9418254764292878, "accuracy_n": 997, "auc": 0.9418254764292878}}
{"key": "result_218", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7472924187725631, "accuracy_n": 277, "auc": 0.7472924187725631}}
{"key": "result_219", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_220", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7575452716297787, "accuracy_n": 994, "auc": 0.7575452716297787}}
{"key": "result_221", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3787575150300601, "accuracy_n": 499, "auc": 0.3787575150300601}}
{"key": "result_222", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.326, "accuracy_n": 500, "auc": 0.326}}
{"key": "result_223", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.35570469798657717, "accuracy_n": 298, "auc": 0.35570469798657717}}
{"key": "result_224", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.20100502512562815, "accuracy_n": 398, "auc": 0.20100502512562815}}
{"key": "result_225", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9966887417218543, "accuracy_n": 302, "auc": 0.9966887417218543}}
{"key": "result_226", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_227", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9763670064874884, "accuracy_n": 322, "auc": 0.9763670064874884}}
{"key": "result_228", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9387687969924813, "accuracy_n": 292, "auc": 0.9387687969924813}}
{"key": "result_229", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9951573849878934, "accuracy_n": 413, "auc": 0.9951573849878934}}
{"key": "result_230", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5611863057324841, "accuracy_n": 1902, "auc": 0.5611863057324841}}
{"key": "result_231", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8396000956345241, "accuracy_n": 2000, "auc": 0.8396000956345241}}
{"key": "result_232", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.903361344537815, "accuracy_n": 59, "auc": 0.903361344537815}}
{"key": "result_233", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_cities_cities_disj", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7615384615384616, "accuracy_n": 23, "auc": 0.7615384615384616}}
{"key": "result_234", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8217522658610272, "accuracy_n": 993, "auc": 0.8217522658610272}}
{"key": "result_235", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9448345035105316, "accuracy_n": 997, "auc": 0.9448345035105316}}
{"key": "result_236", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6895306859205776, "accuracy_n": 277, "auc": 0.6895306859205776}}
{"key": "result_237", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.85, "accuracy_n": 100, "auc": 0.85}}
{"key": "result_238", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7484909456740443, "accuracy_n": 994, "auc": 0.7484909456740443}}
{"key": "result_239", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.3587174348697395, "accuracy_n": 499, "auc": 0.3587174348697395}}
{"key": "result_240", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.334, "accuracy_n": 500, "auc": 0.334}}
{"key": "result_241", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2751677852348993, "accuracy_n": 298, "auc": 0.2751677852348993}}
{"key": "result_242", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2135678391959799, "accuracy_n": 398, "auc": 0.2135678391959799}}
{"key": "result_243", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9933774834437086, "accuracy_n": 302, "auc": 0.9933774834437086}}
{"key": "result_244", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9655172413793104, "accuracy_n": 58, "auc": 0.9655172413793104}}
{"key": "result_245", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.982429718875502, "accuracy_n": 322, "auc": 0.982429718875502}}
{"key": "result_246", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8315789473684211, "accuracy_n": 292, "auc": 0.8315789473684211}}
{"key": "result_247", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_248", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7217223991507431, "accuracy_n": 1902, "auc": 0.7217223991507431}}
{"key": "result_249", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8194828333028223, "accuracy_n": 2000, "auc": 0.8194828333028223}}
{"key": "result_250", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9285714285714286, "accuracy_n": 59, "auc": 0.9285714285714286}}
{"key": "result_251", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "got_larger_than", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8230769230769232, "accuracy_n": 23, "auc": 0.8230769230769232}}
{"key": "result_252", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8700906344410876, "accuracy_n": 993, "auc": 0.8700906344410876}}
{"key": "result_253", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9237713139418254, "accuracy_n": 997, "auc": 0.9237713139418254}}
{"key": "result_254", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7472924187725631, "accuracy_n": 277, "auc": 0.7472924187725631}}
{"key": "result_255", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6, "accuracy_n": 100, "auc": 0.6}}
{"key": "result_256", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6951710261569416, "accuracy_n": 994, "auc": 0.6951710261569416}}
{"key": "result_257", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.36472945891783565, "accuracy_n": 499, "auc": 0.36472945891783565}}
{"key": "result_258", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.298, "accuracy_n": 500, "auc": 0.298}}
{"key": "result_259", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2550335570469799, "accuracy_n": 298, "auc": 0.2550335570469799}}
{"key": "result_260", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.21105527638190955, "accuracy_n": 398, "auc": 0.21105527638190955}}
{"key": "result_261", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.695364238410596, "accuracy_n": 302, "auc": 0.695364238410596}}
{"key": "result_262", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7758620689655172, "accuracy_n": 58, "auc": 0.7758620689655172}}
{"key": "result_263", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5269925857275255, "accuracy_n": 322, "auc": 0.5269925857275255}}
{"key": "result_264", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5115131578947368, "accuracy_n": 292, "auc": 0.5115131578947368}}
{"key": "result_265", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5278450363196125, "accuracy_n": 413, "auc": 0.5278450363196125}}
{"key": "result_266", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9308109961075725, "accuracy_n": 1902, "auc": 0.9308109961075725}}
{"key": "result_267", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5365687013011697, "accuracy_n": 2000, "auc": 0.5365687013011697}}
{"key": "result_268", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5770308123249299, "accuracy_n": 59, "auc": 0.5770308123249299}}
{"key": "result_269", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "likely", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6923076923076923, "accuracy_n": 23, "auc": 0.6923076923076923}}
{"key": "result_270", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9254783484390735, "accuracy_n": 993, "auc": 0.9254783484390735}}
{"key": "result_271", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9428284854563691, "accuracy_n": 997, "auc": 0.9428284854563691}}
{"key": "result_272", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7689530685920578, "accuracy_n": 277, "auc": 0.7689530685920578}}
{"key": "result_273", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.86, "accuracy_n": 100, "auc": 0.86}}
{"key": "result_274", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7394366197183099, "accuracy_n": 994, "auc": 0.7394366197183099}}
{"key": "result_275", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.312625250501002, "accuracy_n": 499, "auc": 0.312625250501002}}
{"key": "result_276", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31, "accuracy_n": 500, "auc": 0.31}}
{"key": "result_277", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.31543624161073824, "accuracy_n": 298, "auc": 0.31543624161073824}}
{"key": "result_278", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.20100502512562815, "accuracy_n": 398, "auc": 0.20100502512562815}}
{"key": "result_279", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9900662251655629, "accuracy_n": 302, "auc": 0.9900662251655629}}
{"key": "result_280", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9827586206896551, "accuracy_n": 58, "auc": 0.9827586206896551}}
{"key": "result_281", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9822752548656163, "accuracy_n": 322, "auc": 0.9822752548656163}}
{"key": "result_282", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8539943609022557, "accuracy_n": 292, "auc": 0.8539943609022557}}
{"key": "result_283", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 1.0, "accuracy_n": 413, "auc": 1.0}}
{"key": "result_284", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7330977972399151, "accuracy_n": 1902, "auc": 0.7330977972399151}}
{"key": "result_285", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8648382065925799, "accuracy_n": 2000, "auc": 0.8648382065925799}}
{"key": "result_286", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9145658263305322, "accuracy_n": 59, "auc": 0.9145658263305322}}
{"key": "result_287", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "counterfact_true_false", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7000000000000001, "accuracy_n": 23, "auc": 0.7000000000000001}}
{"key": "result_288", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9144008056394763, "accuracy_n": 993, "auc": 0.9144008056394763}}
{"key": "result_289", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9498495486459378, "accuracy_n": 997, "auc": 0.9498495486459378}}
{"key": "result_290", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7725631768953068, "accuracy_n": 277, "auc": 0.7725631768953068}}
{"key": "result_291", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.88, "accuracy_n": 100, "auc": 0.88}}
{"key": "result_292", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7776659959758552, "accuracy_n": 994, "auc": 0.7776659959758552}}
{"key": "result_293", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.26452905811623245, "accuracy_n": 499, "auc": 0.26452905811623245}}
{"key": "result_294", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.306, "accuracy_n": 500, "auc": 0.306}}
{"key": "result_295", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.30201342281879195, "accuracy_n": 298, "auc": 0.30201342281879195}}
{"key": "result_296", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2236180904522613, "accuracy_n": 398, "auc": 0.2236180904522613}}
{"key": "result_297", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9768211920529801, "accuracy_n": 302, "auc": 0.9768211920529801}}
{"key": "result_298", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9482758620689655, "accuracy_n": 58, "auc": 0.9482758620689655}}
{"key": "result_299", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9702270620945319, "accuracy_n": 322, "auc": 0.9702270620945319}}
{"key": "result_300", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7984492481203007, "accuracy_n": 292, "auc": 0.7984492481203007}}
{"key": "result_301", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9612590799031477, "accuracy_n": 413, "auc": 0.9612590799031477}}
{"key": "result_302", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.8429825725406936, "accuracy_n": 1902, "auc": 0.8429825725406936}}
{"key": "result_303", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7188004869757982, "accuracy_n": 2000, "auc": 0.7188004869757982}}
{"key": "result_304", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7997198879551821, "accuracy_n": 59, "auc": 0.7997198879551821}}
{"key": "result_305", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "diverse_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6692307692307693, "accuracy_n": 23, "auc": 0.6692307692307693}}
{"key": "result_306", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "imdb", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.9123867069486404, "accuracy_n": 993, "auc": 0.9123867069486404}}
{"key": "result_307", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "ag_news", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5997993981945837, "accuracy_n": 997, "auc": 0.5997993981945837}}
{"key": "result_308", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "rte", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5270758122743683, "accuracy_n": 277, "auc": 0.5270758122743683}}
{"key": "result_309", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "copa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.55, "accuracy_n": 100, "auc": 0.55}}
{"key": "result_310", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "boolq", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6156941649899397, "accuracy_n": 994, "auc": 0.6156941649899397}}
{"key": "result_311", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "open_book_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.32064128256513025, "accuracy_n": 499, "auc": 0.32064128256513025}}
{"key": "result_312", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "race", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.294, "accuracy_n": 500, "auc": 0.294}}
{"key": "result_313", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "arc_challenge", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2785234899328859, "accuracy_n": 298, "auc": 0.2785234899328859}}
{"key": "result_314", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "common_sense_qa", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.2562814070351759, "accuracy_n": 398, "auc": 0.2562814070351759}}
{"key": "result_315", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6192052980132451, "accuracy_n": 302, "auc": 0.6192052980132451}}
{"key": "result_316", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_sp_en_trans", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7068965517241379, "accuracy_n": 58, "auc": 0.7068965517241379}}
{"key": "result_317", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_conj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7247065183812171, "accuracy_n": 322, "auc": 0.7247065183812171}}
{"key": "result_318", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_cities_cities_disj", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5226033834586465, "accuracy_n": 292, "auc": 0.5226033834586465}}
{"key": "result_319", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "got_larger_than", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5811138014527845, "accuracy_n": 413, "auc": 0.5811138014527845}}
{"key": "result_320", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "likely", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.5034622699929228, "accuracy_n": 1902, "auc": 0.5034622699929228}}
{"key": "result_321", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "counterfact_true_false", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.6024384802913851, "accuracy_n": 2000, "auc": 0.6024384802913851}}
{"key": "result_322", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "diverse_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.7563025210084033, "accuracy_n": 59, "auc": 0.7563025210084033}}
{"key": "result_323", "value": {"llm_id": "Qwen/Qwen3-14B-Base", "train_dataset": "complex_truth", "eval_dataset": "complex_truth", "probe_method": "dim", "point_name": "h19", "token_idx": -1, "accuracy": 0.823076923076923, "accuracy_n": 23, "auc": 0.823076923076923}}
